{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DRL-robot-navigation-IR-SIM","text":"<p>Deep Reinforcement Learning algorithm implementation for simulated robot navigation in IR-SIM. Using 2D laser sensor data and information about the goal point a robot learns to navigate to a specified point in the environment.</p>"},{"location":"api/robot_nav/","title":"Robot Navigation","text":"<p>Will be updated</p>"},{"location":"api/robot_nav/#robot_nav","title":"<code>robot_nav</code>","text":""},{"location":"api/IR-SIM/ir-sim/","title":"IR-SIM","text":""},{"location":"api/IR-SIM/ir-sim/#robot_nav.sim","title":"<code>robot_nav.sim</code>","text":""},{"location":"api/IR-SIM/ir-sim/#robot_nav.sim.SIM_ENV","title":"<code>SIM_ENV</code>","text":"<p>A simulation environment interface for robot navigation using IRSim.</p> <p>This class wraps around the IRSim environment and provides methods for stepping, resetting, and interacting with a mobile robot, including reward computation.</p> <p>Attributes:</p> Name Type Description <code>env</code> <code>object</code> <p>The simulation environment instance from IRSim.</p> <code>robot_goal</code> <code>ndarray</code> <p>The goal position of the robot.</p> Source code in <code>robot_nav/sim.py</code> <pre><code>class SIM_ENV:\n    \"\"\"\n    A simulation environment interface for robot navigation using IRSim.\n\n    This class wraps around the IRSim environment and provides methods for stepping,\n    resetting, and interacting with a mobile robot, including reward computation.\n\n    Attributes:\n        env (object): The simulation environment instance from IRSim.\n        robot_goal (np.ndarray): The goal position of the robot.\n    \"\"\"\n\n    def __init__(self, world_file=\"robot_world.yaml\", disable_plotting=False):\n        \"\"\"\n        Initialize the simulation environment.\n\n        Args:\n            world_file (str): Path to the world configuration YAML file.\n            disable_plotting (bool): If True, disables rendering and plotting.\n        \"\"\"\n        self.env = irsim.make(world_file, disable_all_plot=disable_plotting)\n        robot_info = self.env.get_robot_info(0)\n        self.robot_goal = robot_info.goal\n\n    def step(self, lin_velocity=0.0, ang_velocity=0.1):\n        \"\"\"\n        Perform one step in the simulation using the given control commands.\n\n        Args:\n            lin_velocity (float): Linear velocity to apply to the robot.\n            ang_velocity (float): Angular velocity to apply to the robot.\n\n        Returns:\n            tuple: Contains the latest LIDAR scan, distance to goal, cosine and sine of angle to goal,\n                   collision flag, goal reached flag, applied action, and computed reward.\n        \"\"\"\n        self.env.step(action_id=0, action=np.array([[lin_velocity], [ang_velocity]]))\n        self.env.render()\n\n        scan = self.env.get_lidar_scan()\n        latest_scan = scan[\"ranges\"]\n\n        robot_state = self.env.get_robot_state()\n        goal_vector = [\n            self.robot_goal[0].item() - robot_state[0].item(),\n            self.robot_goal[1].item() - robot_state[1].item(),\n        ]\n        distance = np.linalg.norm(goal_vector)\n        goal = self.env.robot.arrive\n        pose_vector = [np.cos(robot_state[2]).item(), np.sin(robot_state[2]).item()]\n        cos, sin = self.cossin(pose_vector, goal_vector)\n        collision = self.env.robot.collision\n        action = [lin_velocity, ang_velocity]\n        reward = self.get_reward(goal, collision, action, latest_scan)\n\n        return latest_scan, distance, cos, sin, collision, goal, action, reward\n\n    def reset(\n        self,\n        robot_state=None,\n        robot_goal=None,\n        random_obstacles=True,\n        random_obstacle_ids=None,\n    ):\n        \"\"\"\n        Reset the simulation environment, optionally setting robot and obstacle states.\n\n        Args:\n            robot_state (list or None): Initial state of the robot as a list of [x, y, theta, speed].\n            robot_goal (list or None): Goal state for the robot.\n            random_obstacles (bool): Whether to randomly reposition obstacles.\n            random_obstacle_ids (list or None): Specific obstacle IDs to randomize.\n\n        Returns:\n            tuple: Initial observation after reset, including LIDAR scan, distance, cos/sin,\n                   and reward-related flags and values.\n        \"\"\"\n        if robot_state is None:\n            robot_state = [[random.uniform(1, 9)], [random.uniform(1, 9)], [0], [0]]\n\n        self.env.robot.set_state(\n            state=np.array(robot_state),\n            init=True,\n        )\n\n        if random_obstacles:\n            if random_obstacle_ids is None:\n                random_obstacle_ids = [i + 1 for i in range(7)]\n            self.env.random_obstacle_position(\n                range_low=[0, 0, -3.14],\n                range_high=[10, 10, 3.14],\n                ids=random_obstacle_ids,\n                non_overlapping=True,\n            )\n\n        if robot_goal is None:\n            unique = True\n            while unique:\n                robot_goal = [[random.uniform(1, 9)], [random.uniform(1, 9)], [0]]\n                shape = {\"name\": \"circle\", \"radius\": 0.4}\n                state = [robot_goal[0], robot_goal[1], robot_goal[2]]\n                gf = GeometryFactory.create_geometry(**shape)\n                geometry = gf.step(np.c_[state])\n                unique = any(\n                    [\n                        shapely.intersects(geometry, obj._geometry)\n                        for obj in self.env.obstacle_list\n                    ]\n                )\n        self.env.robot.set_goal(np.array(robot_goal), init=True)\n        self.env.reset()\n        self.robot_goal = self.env.robot.goal\n\n        action = [0.0, 0.0]\n        latest_scan, distance, cos, sin, _, _, action, reward = self.step(\n            lin_velocity=action[0], ang_velocity=action[1]\n        )\n        return latest_scan, distance, cos, sin, False, False, action, reward\n\n    @staticmethod\n    def cossin(vec1, vec2):\n        \"\"\"\n        Compute the cosine and sine of the angle between two 2D vectors.\n\n        Args:\n            vec1 (list): First 2D vector.\n            vec2 (list): Second 2D vector.\n\n        Returns:\n            tuple: (cosine, sine) of the angle between the vectors.\n        \"\"\"\n        vec1 = vec1 / np.linalg.norm(vec1)\n        vec2 = vec2 / np.linalg.norm(vec2)\n        cos = np.dot(vec1, vec2)\n        sin = vec1[0] * vec2[1] - vec1[1] * vec2[0]\n        return cos, sin\n\n    @staticmethod\n    def get_reward(goal, collision, action, laser_scan):\n        \"\"\"\n        Calculate the reward for the current step.\n\n        Args:\n            goal (bool): Whether the goal has been reached.\n            collision (bool): Whether a collision occurred.\n            action (list): The action taken [linear velocity, angular velocity].\n            laser_scan (list): The LIDAR scan readings.\n\n        Returns:\n            float: Computed reward for the current state.\n        \"\"\"\n        if goal:\n            return 100.0\n        elif collision:\n            return -100.0\n        else:\n            r3 = lambda x: 1.35 - x if x &lt; 1.35 else 0.0\n            return action[0] - abs(action[1]) / 2 - r3(min(laser_scan)) / 2\n</code></pre>"},{"location":"api/IR-SIM/ir-sim/#robot_nav.sim.SIM_ENV.__init__","title":"<code>__init__(world_file='robot_world.yaml', disable_plotting=False)</code>","text":"<p>Initialize the simulation environment.</p> <p>Parameters:</p> Name Type Description Default <code>world_file</code> <code>str</code> <p>Path to the world configuration YAML file.</p> <code>'robot_world.yaml'</code> <code>disable_plotting</code> <code>bool</code> <p>If True, disables rendering and plotting.</p> <code>False</code> Source code in <code>robot_nav/sim.py</code> <pre><code>def __init__(self, world_file=\"robot_world.yaml\", disable_plotting=False):\n    \"\"\"\n    Initialize the simulation environment.\n\n    Args:\n        world_file (str): Path to the world configuration YAML file.\n        disable_plotting (bool): If True, disables rendering and plotting.\n    \"\"\"\n    self.env = irsim.make(world_file, disable_all_plot=disable_plotting)\n    robot_info = self.env.get_robot_info(0)\n    self.robot_goal = robot_info.goal\n</code></pre>"},{"location":"api/IR-SIM/ir-sim/#robot_nav.sim.SIM_ENV.cossin","title":"<code>cossin(vec1, vec2)</code>  <code>staticmethod</code>","text":"<p>Compute the cosine and sine of the angle between two 2D vectors.</p> <p>Parameters:</p> Name Type Description Default <code>vec1</code> <code>list</code> <p>First 2D vector.</p> required <code>vec2</code> <code>list</code> <p>Second 2D vector.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(cosine, sine) of the angle between the vectors.</p> Source code in <code>robot_nav/sim.py</code> <pre><code>@staticmethod\ndef cossin(vec1, vec2):\n    \"\"\"\n    Compute the cosine and sine of the angle between two 2D vectors.\n\n    Args:\n        vec1 (list): First 2D vector.\n        vec2 (list): Second 2D vector.\n\n    Returns:\n        tuple: (cosine, sine) of the angle between the vectors.\n    \"\"\"\n    vec1 = vec1 / np.linalg.norm(vec1)\n    vec2 = vec2 / np.linalg.norm(vec2)\n    cos = np.dot(vec1, vec2)\n    sin = vec1[0] * vec2[1] - vec1[1] * vec2[0]\n    return cos, sin\n</code></pre>"},{"location":"api/IR-SIM/ir-sim/#robot_nav.sim.SIM_ENV.get_reward","title":"<code>get_reward(goal, collision, action, laser_scan)</code>  <code>staticmethod</code>","text":"<p>Calculate the reward for the current step.</p> <p>Parameters:</p> Name Type Description Default <code>goal</code> <code>bool</code> <p>Whether the goal has been reached.</p> required <code>collision</code> <code>bool</code> <p>Whether a collision occurred.</p> required <code>action</code> <code>list</code> <p>The action taken [linear velocity, angular velocity].</p> required <code>laser_scan</code> <code>list</code> <p>The LIDAR scan readings.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Computed reward for the current state.</p> Source code in <code>robot_nav/sim.py</code> <pre><code>@staticmethod\ndef get_reward(goal, collision, action, laser_scan):\n    \"\"\"\n    Calculate the reward for the current step.\n\n    Args:\n        goal (bool): Whether the goal has been reached.\n        collision (bool): Whether a collision occurred.\n        action (list): The action taken [linear velocity, angular velocity].\n        laser_scan (list): The LIDAR scan readings.\n\n    Returns:\n        float: Computed reward for the current state.\n    \"\"\"\n    if goal:\n        return 100.0\n    elif collision:\n        return -100.0\n    else:\n        r3 = lambda x: 1.35 - x if x &lt; 1.35 else 0.0\n        return action[0] - abs(action[1]) / 2 - r3(min(laser_scan)) / 2\n</code></pre>"},{"location":"api/IR-SIM/ir-sim/#robot_nav.sim.SIM_ENV.reset","title":"<code>reset(robot_state=None, robot_goal=None, random_obstacles=True, random_obstacle_ids=None)</code>","text":"<p>Reset the simulation environment, optionally setting robot and obstacle states.</p> <p>Parameters:</p> Name Type Description Default <code>robot_state</code> <code>list or None</code> <p>Initial state of the robot as a list of [x, y, theta, speed].</p> <code>None</code> <code>robot_goal</code> <code>list or None</code> <p>Goal state for the robot.</p> <code>None</code> <code>random_obstacles</code> <code>bool</code> <p>Whether to randomly reposition obstacles.</p> <code>True</code> <code>random_obstacle_ids</code> <code>list or None</code> <p>Specific obstacle IDs to randomize.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Initial observation after reset, including LIDAR scan, distance, cos/sin,    and reward-related flags and values.</p> Source code in <code>robot_nav/sim.py</code> <pre><code>def reset(\n    self,\n    robot_state=None,\n    robot_goal=None,\n    random_obstacles=True,\n    random_obstacle_ids=None,\n):\n    \"\"\"\n    Reset the simulation environment, optionally setting robot and obstacle states.\n\n    Args:\n        robot_state (list or None): Initial state of the robot as a list of [x, y, theta, speed].\n        robot_goal (list or None): Goal state for the robot.\n        random_obstacles (bool): Whether to randomly reposition obstacles.\n        random_obstacle_ids (list or None): Specific obstacle IDs to randomize.\n\n    Returns:\n        tuple: Initial observation after reset, including LIDAR scan, distance, cos/sin,\n               and reward-related flags and values.\n    \"\"\"\n    if robot_state is None:\n        robot_state = [[random.uniform(1, 9)], [random.uniform(1, 9)], [0], [0]]\n\n    self.env.robot.set_state(\n        state=np.array(robot_state),\n        init=True,\n    )\n\n    if random_obstacles:\n        if random_obstacle_ids is None:\n            random_obstacle_ids = [i + 1 for i in range(7)]\n        self.env.random_obstacle_position(\n            range_low=[0, 0, -3.14],\n            range_high=[10, 10, 3.14],\n            ids=random_obstacle_ids,\n            non_overlapping=True,\n        )\n\n    if robot_goal is None:\n        unique = True\n        while unique:\n            robot_goal = [[random.uniform(1, 9)], [random.uniform(1, 9)], [0]]\n            shape = {\"name\": \"circle\", \"radius\": 0.4}\n            state = [robot_goal[0], robot_goal[1], robot_goal[2]]\n            gf = GeometryFactory.create_geometry(**shape)\n            geometry = gf.step(np.c_[state])\n            unique = any(\n                [\n                    shapely.intersects(geometry, obj._geometry)\n                    for obj in self.env.obstacle_list\n                ]\n            )\n    self.env.robot.set_goal(np.array(robot_goal), init=True)\n    self.env.reset()\n    self.robot_goal = self.env.robot.goal\n\n    action = [0.0, 0.0]\n    latest_scan, distance, cos, sin, _, _, action, reward = self.step(\n        lin_velocity=action[0], ang_velocity=action[1]\n    )\n    return latest_scan, distance, cos, sin, False, False, action, reward\n</code></pre>"},{"location":"api/IR-SIM/ir-sim/#robot_nav.sim.SIM_ENV.step","title":"<code>step(lin_velocity=0.0, ang_velocity=0.1)</code>","text":"<p>Perform one step in the simulation using the given control commands.</p> <p>Parameters:</p> Name Type Description Default <code>lin_velocity</code> <code>float</code> <p>Linear velocity to apply to the robot.</p> <code>0.0</code> <code>ang_velocity</code> <code>float</code> <p>Angular velocity to apply to the robot.</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Contains the latest LIDAR scan, distance to goal, cosine and sine of angle to goal,    collision flag, goal reached flag, applied action, and computed reward.</p> Source code in <code>robot_nav/sim.py</code> <pre><code>def step(self, lin_velocity=0.0, ang_velocity=0.1):\n    \"\"\"\n    Perform one step in the simulation using the given control commands.\n\n    Args:\n        lin_velocity (float): Linear velocity to apply to the robot.\n        ang_velocity (float): Angular velocity to apply to the robot.\n\n    Returns:\n        tuple: Contains the latest LIDAR scan, distance to goal, cosine and sine of angle to goal,\n               collision flag, goal reached flag, applied action, and computed reward.\n    \"\"\"\n    self.env.step(action_id=0, action=np.array([[lin_velocity], [ang_velocity]]))\n    self.env.render()\n\n    scan = self.env.get_lidar_scan()\n    latest_scan = scan[\"ranges\"]\n\n    robot_state = self.env.get_robot_state()\n    goal_vector = [\n        self.robot_goal[0].item() - robot_state[0].item(),\n        self.robot_goal[1].item() - robot_state[1].item(),\n    ]\n    distance = np.linalg.norm(goal_vector)\n    goal = self.env.robot.arrive\n    pose_vector = [np.cos(robot_state[2]).item(), np.sin(robot_state[2]).item()]\n    cos, sin = self.cossin(pose_vector, goal_vector)\n    collision = self.env.robot.collision\n    action = [lin_velocity, ang_velocity]\n    reward = self.get_reward(goal, collision, action, latest_scan)\n\n    return latest_scan, distance, cos, sin, collision, goal, action, reward\n</code></pre>"},{"location":"api/Testing/test/","title":"Testing","text":""},{"location":"api/Testing/test/#robot_nav.test","title":"<code>robot_nav.test</code>","text":""},{"location":"api/Testing/test/#robot_nav.test.main","title":"<code>main(args=None)</code>","text":"<p>Main testing function</p> Source code in <code>robot_nav/test.py</code> <pre><code>def main(args=None):\n    \"\"\"Main testing function\"\"\"\n    action_dim = 2  # number of actions produced by the model\n    max_action = 1  # maximum absolute value of output actions\n    state_dim = 25  # number of input values in the neural network (vector length of state input)\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )  # using cuda if it is available, cpu otherwise\n    epoch = 0  # epoch number\n    max_steps = 300  # maximum number of steps in single episode\n\n    model = TD3(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        max_action=max_action,\n        device=device,\n        load_model=True,\n        model_name=\"TD3\",\n    )  # instantiate a model\n\n    sim = SIM_ENV(world_file=\"eval_world.yaml\")  # instantiate environment\n    with open(\"robot_nav/eval_points.yaml\") as file:\n        points = yaml.safe_load(file)\n    robot_poses = points[\"robot\"][\"poses\"]\n    robot_goals = points[\"robot\"][\"goals\"]\n\n    assert len(robot_poses) == len(\n        robot_goals\n    ), \"Number of robot poses do not equal the robot goals\"\n\n    print(\"..............................................\")\n    print(f\"Testing {len(robot_poses)} scenarios\")\n    total_reward = 0.0\n    total_steps = 0\n    col = 0\n    goals = 0\n    for idx in range(len(robot_poses)):\n        count = 0\n        latest_scan, distance, cos, sin, collision, goal, a, reward = sim.reset(\n            robot_state=robot_poses[idx],\n            robot_goal=robot_goals[idx],\n            random_obstacles=False,\n        )\n        done = False\n        while not done and count &lt; max_steps:\n            state, terminal = model.prepare_state(\n                latest_scan, distance, cos, sin, collision, goal, a\n            )\n            action = model.get_action(np.array(state), False)\n            a_in = [(action[0] + 1) / 4, action[1]]\n            latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(\n                lin_velocity=a_in[0], ang_velocity=a_in[1]\n            )\n            total_reward += reward\n            total_steps += 1\n            count += 1\n            if collision:\n                col += 1\n            if goal:\n                goals += 1\n            done = collision or goal\n    avg_step_reward = total_reward / total_steps\n    avg_reward = total_reward / len(robot_poses)\n    avg_col = col / len(robot_poses)\n    avg_goal = goals / len(robot_poses)\n    print(f\"Total Reward: {total_reward}\")\n    print(f\"Average Reward: {avg_reward}\")\n    print(f\"Average Step Reward: {avg_step_reward}\")\n    print(f\"Average Collision rate: {avg_col}\")\n    print(f\"Average Goal rate: {avg_goal}\")\n    print(\"..............................................\")\n    model.writer.add_scalar(\"test/total_reward\", total_reward, epoch)\n    model.writer.add_scalar(\"test/avg_reward\", avg_reward, epoch)\n    model.writer.add_scalar(\"test/avg_step_reward\", avg_step_reward, epoch)\n    model.writer.add_scalar(\"test/avg_col\", avg_col, epoch)\n    model.writer.add_scalar(\"test/avg_goal\", avg_goal, epoch)\n</code></pre>"},{"location":"api/Testing/test/#robot_nav.test_random","title":"<code>robot_nav.test_random</code>","text":""},{"location":"api/Testing/test/#robot_nav.test_random.main","title":"<code>main(args=None)</code>","text":"<p>Main testing function</p> Source code in <code>robot_nav/test_random.py</code> <pre><code>def main(args=None):\n    \"\"\"Main testing function\"\"\"\n    action_dim = 2  # number of actions produced by the model\n    max_action = 1  # maximum absolute value of output actions\n    state_dim = 25  # number of input values in the neural network (vector length of state input)\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )  # using cuda if it is available, cpu otherwise\n    epoch = 0  # epoch number\n    max_steps = 300  # maximum number of steps in single episode\n    test_scenarios = 1000\n\n    model = SAC(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        max_action=max_action,\n        device=device,\n        load_model=True,\n        model_name=\"SAC\",\n    )  # instantiate a model\n\n    sim = SIM_ENV(\n        world_file=\"eval_world.yaml\", disable_plotting=False\n    )  # instantiate environment\n\n    print(\"..............................................\")\n    print(f\"Testing {test_scenarios} scenarios\")\n    total_reward = []\n    reward_per_ep = []\n    lin_actions = []\n    ang_actions = []\n    total_steps = 0\n    col = 0\n    goals = 0\n    inter_rew = []\n    steps_to_goal = []\n    for _ in tqdm.tqdm(range(test_scenarios)):\n        count = 0\n        ep_reward = 0\n        latest_scan, distance, cos, sin, collision, goal, a, reward = sim.reset(\n            robot_state=None,\n            robot_goal=None,\n            random_obstacles=True,\n            random_obstacle_ids=[i + 1 for i in range(6)],\n        )\n        done = False\n        while not done and count &lt; max_steps:\n            state, terminal = model.prepare_state(\n                latest_scan, distance, cos, sin, collision, goal, a\n            )\n            action = model.get_action(np.array(state), False)\n            a_in = [(action[0] + 1) / 4, action[1]]\n            lin_actions.append(a_in[0])\n            ang_actions.append(a_in[1])\n            latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(\n                lin_velocity=a_in[0], ang_velocity=a_in[1]\n            )\n            ep_reward += reward\n            total_reward.append(reward)\n            total_steps += 1\n            count += 1\n            if collision:\n                col += 1\n            if goal:\n                goals += 1\n                steps_to_goal.append(count)\n            done = collision or goal\n            if done:\n                reward_per_ep.append(ep_reward)\n            if not done:\n                inter_rew.append(reward)\n\n    total_reward = np.array(total_reward)\n    reward_per_ep = np.array(reward_per_ep)\n    inter_rew = np.array(inter_rew)\n    steps_to_goal = np.array(steps_to_goal)\n    lin_actions = np.array(lin_actions)\n    ang_actions = np.array(ang_actions)\n    avg_step_reward = statistics.mean(total_reward)\n    avg_step_reward_std = statistics.stdev(total_reward)\n    avg_ep_reward = statistics.mean(reward_per_ep)\n    avg_ep_reward_std = statistics.stdev(reward_per_ep)\n    avg_col = col / test_scenarios\n    avg_goal = goals / test_scenarios\n    avg_inter_step_rew = statistics.mean(inter_rew)\n    avg_inter_step_rew_std = statistics.stdev(inter_rew)\n    avg_steps_to_goal = statistics.mean(steps_to_goal)\n    avg_steps_to_goal_std = statistics.stdev(steps_to_goal)\n    mean_lin_action = statistics.mean(lin_actions)\n    lin_actions_std = statistics.stdev(lin_actions)\n    mean_ang_action = statistics.mean(ang_actions)\n    ang_actions_std = statistics.stdev(ang_actions)\n    print(f\"avg_step_reward {avg_step_reward}\")\n    print(f\"avg_step_reward_std: {avg_step_reward_std}\")\n    print(f\"avg_ep_reward: {avg_ep_reward}\")\n    print(f\"avg_ep_reward_std: {avg_ep_reward_std}\")\n    print(f\"avg_col: {avg_col}\")\n    print(f\"avg_goal: {avg_goal}\")\n    print(f\"avg_inter_step_rew: {avg_inter_step_rew}\")\n    print(f\"avg_inter_step_rew_std: {avg_inter_step_rew_std}\")\n    print(f\"avg_steps_to_goal: {avg_steps_to_goal}\")\n    print(f\"avg_steps_to_goal_std: {avg_steps_to_goal_std}\")\n    print(f\"mean_lin_action: {mean_lin_action}\")\n    print(f\"lin_actions_std: {lin_actions_std}\")\n    print(f\"mean_ang_action: {mean_ang_action}\")\n    print(f\"ang_actions_std: {ang_actions_std}\")\n    print(\"..............................................\")\n    model.writer.add_scalar(\"test/avg_step_reward\", avg_step_reward, epoch)\n    model.writer.add_scalar(\"test/avg_step_reward_std\", avg_step_reward_std, epoch)\n    model.writer.add_scalar(\"test/avg_ep_reward\", avg_ep_reward, epoch)\n    model.writer.add_scalar(\"test/avg_ep_reward_std\", avg_ep_reward_std, epoch)\n    model.writer.add_scalar(\"test/avg_col\", avg_col, epoch)\n    model.writer.add_scalar(\"test/avg_goal\", avg_goal, epoch)\n    model.writer.add_scalar(\"test/avg_inter_step_rew\", avg_inter_step_rew, epoch)\n    model.writer.add_scalar(\n        \"test/avg_inter_step_rew_std\", avg_inter_step_rew_std, epoch\n    )\n    model.writer.add_scalar(\"test/avg_steps_to_goal\", avg_steps_to_goal, epoch)\n    model.writer.add_scalar(\"test/avg_steps_to_goal_std\", avg_steps_to_goal_std, epoch)\n    model.writer.add_scalar(\"test/mean_lin_action\", mean_lin_action, epoch)\n    model.writer.add_scalar(\"test/lin_actions_std\", lin_actions_std, epoch)\n    model.writer.add_scalar(\"test/mean_ang_action\", mean_ang_action, epoch)\n    model.writer.add_scalar(\"test/ang_actions_std\", ang_actions_std, epoch)\n    bins = 100\n    model.writer.add_histogram(\"test/lin_actions\", lin_actions, epoch, max_bins=bins)\n    model.writer.add_histogram(\"test/ang_actions\", ang_actions, epoch, max_bins=bins)\n\n    counts, bin_edges = np.histogram(lin_actions, bins=bins)\n    fig, ax = plt.subplots()\n    ax.bar(\n        bin_edges[:-1], counts, width=np.diff(bin_edges), align=\"edge\", log=True\n    )  # Log scale on y-axis\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency (Log Scale)\")\n    ax.set_title(\"Histogram with Log Scale\")\n    model.writer.add_figure(\"test/lin_actions_hist\", fig)\n\n    counts, bin_edges = np.histogram(ang_actions, bins=bins)\n    fig, ax = plt.subplots()\n    ax.bar(\n        bin_edges[:-1], counts, width=np.diff(bin_edges), align=\"edge\", log=True\n    )  # Log scale on y-axis\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency (Log Scale)\")\n    ax.set_title(\"Histogram with Log Scale\")\n    model.writer.add_figure(\"test/ang_actions_hist\", fig)\n</code></pre>"},{"location":"api/Testing/testrnn/","title":"Testing RNN","text":""},{"location":"api/Testing/testrnn/#robot_nav.test_rnn","title":"<code>robot_nav.test_rnn</code>","text":""},{"location":"api/Testing/testrnn/#robot_nav.test_rnn.main","title":"<code>main(args=None)</code>","text":"<p>Main testing function</p> Source code in <code>robot_nav/test_rnn.py</code> <pre><code>def main(args=None):\n    \"\"\"Main testing function\"\"\"\n    action_dim = 2  # number of actions produced by the model\n    max_action = 1  # maximum absolute value of output actions\n    state_dim = 185  # number of input values in the neural network (vector length of state input)\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )  # using cuda if it is available, cpu otherwise\n    epoch = 0  # epoch number\n    max_steps = 300  # maximum number of steps in single episode\n\n    model = RCPG(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        max_action=max_action,\n        device=device,\n        load_model=True,\n    )  # instantiate a model\n\n    sim = SIM_ENV(world_file=\"eval_world.yaml\")  # instantiate environment\n    with open(\"robot_nav/eval_points.yaml\") as file:\n        points = yaml.safe_load(file)\n    robot_poses = points[\"robot\"][\"poses\"]\n    robot_goals = points[\"robot\"][\"goals\"]\n\n    assert len(robot_poses) == len(\n        robot_goals\n    ), \"Number of robot poses do not equal the robot goals\"\n\n    print(\"..............................................\")\n    print(f\"Testing {len(robot_poses)} scenarios\")\n    total_reward = 0.0\n    total_steps = 0\n    col = 0\n    goals = 0\n    state_queue = deque(maxlen=5)\n    for idx in range(len(robot_poses)):\n        fill_state = True\n        count = 0\n        latest_scan, distance, cos, sin, collision, goal, a, reward = sim.reset(\n            robot_state=robot_poses[idx],\n            robot_goal=robot_goals[idx],\n            random_obstacles=False,\n        )\n        done = False\n        while not done and count &lt; max_steps:\n            state, terminal = model.prepare_state(\n                latest_scan, distance, cos, sin, collision, goal, a\n            )\n            if fill_state:\n                state_queue.clear()\n                for _ in range(5):\n                    state_queue.append(state)\n                fill_state = False\n            state_queue.append(state)\n            action = model.get_action(np.array(state_queue), False)\n            a_in = [(action[0] + 1) / 4, action[1]]\n            latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(\n                lin_velocity=a_in[0], ang_velocity=a_in[1]\n            )\n            total_reward += reward\n            total_steps += 1\n            count += 1\n            if collision:\n                col += 1\n            if goal:\n                goals += 1\n            done = collision or goal\n    avg_step_reward = total_reward / total_steps\n    avg_reward = total_reward / len(robot_poses)\n    avg_col = col / len(robot_poses)\n    avg_goal = goals / len(robot_poses)\n    print(f\"Total Reward: {total_reward}\")\n    print(f\"Average Reward: {avg_reward}\")\n    print(f\"Average Step Reward: {avg_step_reward}\")\n    print(f\"Average Collision rate: {avg_col}\")\n    print(f\"Average Goal rate: {avg_goal}\")\n    print(\"..............................................\")\n    model.writer.add_scalar(\"test/total_reward\", total_reward, epoch)\n    model.writer.add_scalar(\"test/avg_reward\", avg_reward, epoch)\n    model.writer.add_scalar(\"test/avg_step_reward\", avg_step_reward, epoch)\n    model.writer.add_scalar(\"test/avg_col\", avg_col, epoch)\n    model.writer.add_scalar(\"test/avg_goal\", avg_goal, epoch)\n</code></pre>"},{"location":"api/Training/train/","title":"Training","text":""},{"location":"api/Training/train/#robot_nav.train","title":"<code>robot_nav.train</code>","text":""},{"location":"api/Training/train/#robot_nav.train.main","title":"<code>main(args=None)</code>","text":"<p>Main training function</p> Source code in <code>robot_nav/train.py</code> <pre><code>def main(args=None):\n    \"\"\"Main training function\"\"\"\n    action_dim = 2  # number of actions produced by the model\n    max_action = 1  # maximum absolute value of output actions\n    state_dim = 25  # number of input values in the neural network (vector length of state input)\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )  # using cuda if it is available, cpu otherwise\n    nr_eval_episodes = 10  # how many episodes to use to run evaluation\n    max_epochs = 60  # max number of epochs\n    epoch = 0  # starting epoch number\n    episodes_per_epoch = 70  # how many episodes to run in single epoch\n    episode = 0  # starting episode number\n    train_every_n = 2  # train and update network parameters every n episodes\n    training_iterations = 80  # how many batches to use for single training cycle\n    batch_size = 64  # batch size for each training iteration\n    max_steps = 300  # maximum number of steps in single episode\n    steps = 0  # starting step number\n    load_saved_buffer = False  # whether to load experiences from assets/data.yml\n    pretrain = False  # whether to use the loaded experiences to pre-train the model (load_saved_buffer must be True)\n    pretraining_iterations = (\n        10  # number of training iterations to run during pre-training\n    )\n    save_every = 5  # save the model every n training cycles\n\n    model = TD3(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        max_action=max_action,\n        device=device,\n        save_every=save_every,\n        load_model=False,\n        model_name=\"TD3\",\n    )  # instantiate a model\n\n    sim = SIM_ENV(disable_plotting=False)  # instantiate environment\n    replay_buffer = get_buffer(\n        model,\n        sim,\n        load_saved_buffer,\n        pretrain,\n        pretraining_iterations,\n        training_iterations,\n        batch_size,\n    )\n\n    latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(\n        lin_velocity=0.0, ang_velocity=0.0\n    )  # get the initial step state\n\n    while epoch &lt; max_epochs:  # train until max_epochs is reached\n        state, terminal = model.prepare_state(\n            latest_scan, distance, cos, sin, collision, goal, a\n        )  # get state a state representation from returned data from the environment\n\n        action = model.get_action(np.array(state), True)  # get an action from the model\n        a_in = [\n            (action[0] + 1) / 4,\n            action[1],\n        ]  # clip linear velocity to [0, 0.5] m/s range\n\n        latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(\n            lin_velocity=a_in[0], ang_velocity=a_in[1]\n        )  # get data from the environment\n        next_state, terminal = model.prepare_state(\n            latest_scan, distance, cos, sin, collision, goal, a\n        )  # get a next state representation\n        replay_buffer.add(\n            state, action, reward, terminal, next_state\n        )  # add experience to the replay buffer\n\n        if (\n            terminal or steps == max_steps\n        ):  # reset environment of terminal stat ereached, or max_steps were taken\n            latest_scan, distance, cos, sin, collision, goal, a, reward = sim.reset()\n            episode += 1\n            if episode % train_every_n == 0:\n                model.train(\n                    replay_buffer=replay_buffer,\n                    iterations=training_iterations,\n                    batch_size=batch_size,\n                )  # train the model and update its parameters\n\n            steps = 0\n        else:\n            steps += 1\n\n        if (\n            episode + 1\n        ) % episodes_per_epoch == 0:  # if epoch is concluded, run evaluation\n            episode = 0\n            epoch += 1\n            evaluate(model, epoch, sim, eval_episodes=nr_eval_episodes)\n</code></pre>"},{"location":"api/Training/trainrnn/","title":"Training RNN","text":""},{"location":"api/Training/trainrnn/#robot_nav.train_rnn","title":"<code>robot_nav.train_rnn</code>","text":""},{"location":"api/Training/trainrnn/#robot_nav.train_rnn.main","title":"<code>main(args=None)</code>","text":"<p>Main training function</p> Source code in <code>robot_nav/train_rnn.py</code> <pre><code>def main(args=None):\n    \"\"\"Main training function\"\"\"\n    action_dim = 2  # number of actions produced by the model\n    max_action = 1  # maximum absolute value of output actions\n    state_dim = 185  # number of input values in the neural network (vector length of state input)\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )  # using cuda if it is available, cpu otherwise\n    nr_eval_episodes = 10  # how many episodes to use to run evaluation\n    max_epochs = 60  # max number of epochs\n    epoch = 0  # starting epoch number\n    episodes_per_epoch = 70  # how many episodes to run in single epoch\n    episode = 0  # starting episode number\n    train_every_n = 2  # train and update network parameters every n episodes\n    training_iterations = 80  # how many batches to use for single training cycle\n    batch_size = 64  # batch size for each training iteration\n    max_steps = 300  # maximum number of steps in single episode\n    steps = 0  # starting step number\n    load_saved_buffer = False  # whether to load experiences from assets/data.yml\n    pretrain = False  # whether to use the loaded experiences to pre-train the model (load_saved_buffer must be True)\n    history_len = 10\n    pretraining_iterations = (\n        10  # number of training iterations to run during pre-training\n    )\n    save_every = 10  # save the model every n training cycles\n\n    state_queue = deque(maxlen=history_len)\n\n    model = RCPG(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        max_action=max_action,\n        device=device,\n        save_every=save_every,\n        load_model=False,\n        rnn=\"gru\",\n    )  # instantiate a model\n\n    sim = SIM_ENV()  # instantiate environment\n    replay_buffer = get_buffer(\n        model,\n        sim,\n        load_saved_buffer,\n        pretrain,\n        pretraining_iterations,\n        training_iterations,\n        batch_size,\n    )\n\n    latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(\n        lin_velocity=0.0, ang_velocity=0.0\n    )  # get the initial step state\n    fill_state = True\n    while epoch &lt; max_epochs:  # train until max_epochs is reached\n        state, terminal = model.prepare_state(\n            latest_scan, distance, cos, sin, collision, goal, a\n        )  # get state a state representation from returned data from the environment\n        if fill_state:\n            state_queue.clear()\n            for _ in range(history_len):\n                state_queue.append(state)\n            fill_state = False\n        state_queue.append(state)\n\n        action = model.get_action(\n            np.array(state_queue), True\n        )  # get an action from the model\n        a_in = [\n            (action[0] + 1) / 4,\n            action[1],\n        ]  # clip linear velocity to [0, 0.5] m/s range\n\n        latest_scan, distance, cos, sin, collision, goal, a, reward = sim.step(\n            lin_velocity=a_in[0], ang_velocity=a_in[1]\n        )  # get data from the environment\n        next_state, terminal = model.prepare_state(\n            latest_scan, distance, cos, sin, collision, goal, a\n        )  # get a next state representation\n        replay_buffer.add(\n            state, action, reward, terminal, next_state\n        )  # add experience to the replay buffer\n\n        if (\n            terminal or steps == max_steps\n        ):  # reset environment of terminal stat ereached, or max_steps were taken\n            fill_state = True\n            latest_scan, distance, cos, sin, collision, goal, a, reward = sim.reset()\n            episode += 1\n            if episode % train_every_n == 0:\n                model.train(\n                    replay_buffer=replay_buffer,\n                    iterations=training_iterations,\n                    batch_size=batch_size,\n                )  # train the model and update its parameters\n\n            steps = 0\n        else:\n            steps += 1\n\n        if (\n            episode + 1\n        ) % episodes_per_epoch == 0:  # if epoch is concluded, run evaluation\n            episode = 0\n            epoch += 1\n            evaluate(\n                model,\n                epoch,\n                sim,\n                history_len,\n                max_steps,\n                eval_episodes=nr_eval_episodes,\n            )\n</code></pre>"},{"location":"api/Utils/replay_buffer/","title":"Replay/Rollout Buffer","text":""},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer","title":"<code>robot_nav.replay_buffer</code>","text":""},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.ReplayBuffer","title":"<code>ReplayBuffer</code>","text":"<p>               Bases: <code>object</code></p> <p>Standard experience replay buffer for off-policy reinforcement learning algorithms.</p> <p>Stores tuples of (state, action, reward, done, next_state) up to a fixed capacity, enabling sampling of uncorrelated mini-batches for training.</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>class ReplayBuffer(object):\n    \"\"\"\n    Standard experience replay buffer for off-policy reinforcement learning algorithms.\n\n    Stores tuples of (state, action, reward, done, next_state) up to a fixed capacity,\n    enabling sampling of uncorrelated mini-batches for training.\n    \"\"\"\n\n    def __init__(self, buffer_size, random_seed=123):\n        \"\"\"\n        Initialize the replay buffer.\n\n        Args:\n            buffer_size (int): Maximum number of transitions to store in the buffer.\n            random_seed (int): Seed for random number generation.\n        \"\"\"\n        self.buffer_size = buffer_size\n        self.count = 0\n        self.buffer = deque()\n        random.seed(random_seed)\n\n    def add(self, s, a, r, t, s2):\n        \"\"\"\n        Add a transition to the buffer.\n\n        Args:\n            s (np.ndarray): State.\n            a (np.ndarray): Action.\n            r (float): Reward.\n            t (bool): Done flag (True if episode ended).\n            s2 (np.ndarray): Next state.\n        \"\"\"\n        experience = (s, a, r, t, s2)\n        if self.count &lt; self.buffer_size:\n            self.buffer.append(experience)\n            self.count += 1\n        else:\n            self.buffer.popleft()\n            self.buffer.append(experience)\n\n    def size(self):\n        \"\"\"\n        Get the number of elements currently in the buffer.\n\n        Returns:\n            int: Current buffer size.\n        \"\"\"\n        return self.count\n\n    def sample_batch(self, batch_size):\n        \"\"\"\n        Sample a batch of experiences from the buffer.\n\n        Args:\n            batch_size (int): Number of experiences to sample.\n\n        Returns:\n            Tuple of np.ndarrays: Batches of states, actions, rewards, done flags, and next states.\n        \"\"\"\n        if self.count &lt; batch_size:\n            batch = random.sample(self.buffer, self.count)\n        else:\n            batch = random.sample(self.buffer, batch_size)\n\n        s_batch = np.array([_[0] for _ in batch])\n        a_batch = np.array([_[1] for _ in batch])\n        r_batch = np.array([_[2] for _ in batch]).reshape(-1, 1)\n        t_batch = np.array([_[3] for _ in batch]).reshape(-1, 1)\n        s2_batch = np.array([_[4] for _ in batch])\n\n        return s_batch, a_batch, r_batch, t_batch, s2_batch\n\n    def return_buffer(self):\n        \"\"\"\n        Return the entire buffer contents as separate arrays.\n\n        Returns:\n            Tuple of np.ndarrays: Full arrays of states, actions, rewards, done flags, and next states.\n        \"\"\"\n        s = np.array([_[0] for _ in self.buffer])\n        a = np.array([_[1] for _ in self.buffer])\n        r = np.array([_[2] for _ in self.buffer]).reshape(-1, 1)\n        t = np.array([_[3] for _ in self.buffer]).reshape(-1, 1)\n        s2 = np.array([_[4] for _ in self.buffer])\n\n        return s, a, r, t, s2\n\n    def clear(self):\n        \"\"\"\n        Clear all contents of the buffer.\n        \"\"\"\n        self.buffer.clear()\n        self.count = 0\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.ReplayBuffer.__init__","title":"<code>__init__(buffer_size, random_seed=123)</code>","text":"<p>Initialize the replay buffer.</p> <p>Parameters:</p> Name Type Description Default <code>buffer_size</code> <code>int</code> <p>Maximum number of transitions to store in the buffer.</p> required <code>random_seed</code> <code>int</code> <p>Seed for random number generation.</p> <code>123</code> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def __init__(self, buffer_size, random_seed=123):\n    \"\"\"\n    Initialize the replay buffer.\n\n    Args:\n        buffer_size (int): Maximum number of transitions to store in the buffer.\n        random_seed (int): Seed for random number generation.\n    \"\"\"\n    self.buffer_size = buffer_size\n    self.count = 0\n    self.buffer = deque()\n    random.seed(random_seed)\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.ReplayBuffer.add","title":"<code>add(s, a, r, t, s2)</code>","text":"<p>Add a transition to the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>ndarray</code> <p>State.</p> required <code>a</code> <code>ndarray</code> <p>Action.</p> required <code>r</code> <code>float</code> <p>Reward.</p> required <code>t</code> <code>bool</code> <p>Done flag (True if episode ended).</p> required <code>s2</code> <code>ndarray</code> <p>Next state.</p> required Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def add(self, s, a, r, t, s2):\n    \"\"\"\n    Add a transition to the buffer.\n\n    Args:\n        s (np.ndarray): State.\n        a (np.ndarray): Action.\n        r (float): Reward.\n        t (bool): Done flag (True if episode ended).\n        s2 (np.ndarray): Next state.\n    \"\"\"\n    experience = (s, a, r, t, s2)\n    if self.count &lt; self.buffer_size:\n        self.buffer.append(experience)\n        self.count += 1\n    else:\n        self.buffer.popleft()\n        self.buffer.append(experience)\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.ReplayBuffer.clear","title":"<code>clear()</code>","text":"<p>Clear all contents of the buffer.</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clear all contents of the buffer.\n    \"\"\"\n    self.buffer.clear()\n    self.count = 0\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.ReplayBuffer.return_buffer","title":"<code>return_buffer()</code>","text":"<p>Return the entire buffer contents as separate arrays.</p> <p>Returns:</p> Type Description <p>Tuple of np.ndarrays: Full arrays of states, actions, rewards, done flags, and next states.</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def return_buffer(self):\n    \"\"\"\n    Return the entire buffer contents as separate arrays.\n\n    Returns:\n        Tuple of np.ndarrays: Full arrays of states, actions, rewards, done flags, and next states.\n    \"\"\"\n    s = np.array([_[0] for _ in self.buffer])\n    a = np.array([_[1] for _ in self.buffer])\n    r = np.array([_[2] for _ in self.buffer]).reshape(-1, 1)\n    t = np.array([_[3] for _ in self.buffer]).reshape(-1, 1)\n    s2 = np.array([_[4] for _ in self.buffer])\n\n    return s, a, r, t, s2\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.ReplayBuffer.sample_batch","title":"<code>sample_batch(batch_size)</code>","text":"<p>Sample a batch of experiences from the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of experiences to sample.</p> required <p>Returns:</p> Type Description <p>Tuple of np.ndarrays: Batches of states, actions, rewards, done flags, and next states.</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def sample_batch(self, batch_size):\n    \"\"\"\n    Sample a batch of experiences from the buffer.\n\n    Args:\n        batch_size (int): Number of experiences to sample.\n\n    Returns:\n        Tuple of np.ndarrays: Batches of states, actions, rewards, done flags, and next states.\n    \"\"\"\n    if self.count &lt; batch_size:\n        batch = random.sample(self.buffer, self.count)\n    else:\n        batch = random.sample(self.buffer, batch_size)\n\n    s_batch = np.array([_[0] for _ in batch])\n    a_batch = np.array([_[1] for _ in batch])\n    r_batch = np.array([_[2] for _ in batch]).reshape(-1, 1)\n    t_batch = np.array([_[3] for _ in batch]).reshape(-1, 1)\n    s2_batch = np.array([_[4] for _ in batch])\n\n    return s_batch, a_batch, r_batch, t_batch, s2_batch\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.ReplayBuffer.size","title":"<code>size()</code>","text":"<p>Get the number of elements currently in the buffer.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Current buffer size.</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def size(self):\n    \"\"\"\n    Get the number of elements currently in the buffer.\n\n    Returns:\n        int: Current buffer size.\n    \"\"\"\n    return self.count\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.RolloutReplayBuffer","title":"<code>RolloutReplayBuffer</code>","text":"<p>               Bases: <code>object</code></p> <p>Replay buffer that stores full episode rollouts, allowing access to historical trajectories.</p> <p>Useful for algorithms that condition on sequences of past states (e.g., RNN-based policies).</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>class RolloutReplayBuffer(object):\n    \"\"\"\n    Replay buffer that stores full episode rollouts, allowing access to historical trajectories.\n\n    Useful for algorithms that condition on sequences of past states (e.g., RNN-based policies).\n    \"\"\"\n\n    def __init__(self, buffer_size, random_seed=123, history_len=10):\n        \"\"\"\n        Initialize the rollout replay buffer.\n\n        Args:\n            buffer_size (int): Maximum number of episodes (rollouts) to store.\n            random_seed (int): Seed for random number generation.\n            history_len (int): Number of past steps to return for each sampled state.\n        \"\"\"\n        self.buffer_size = buffer_size\n        self.count = 0\n        self.buffer = deque(maxlen=buffer_size)\n        random.seed(random_seed)\n        self.buffer.append([])\n        self.history_len = history_len\n\n    def add(self, s, a, r, t, s2):\n        \"\"\"\n        Add a transition to the current episode.\n\n        If the transition ends the episode (t=True), a new episode is started.\n\n        Args:\n            s (np.ndarray): State.\n            a (np.ndarray): Action.\n            r (float): Reward.\n            t (bool): Done flag.\n            s2 (np.ndarray): Next state.\n        \"\"\"\n        experience = (s, a, r, t, s2)\n        if t:\n            self.count += 1\n            self.buffer[-1].append(experience)\n            self.buffer.append([])\n        else:\n            self.buffer[-1].append(experience)\n\n    def size(self):\n        \"\"\"\n        Get the number of complete episodes in the buffer.\n\n        Returns:\n            int: Number of episodes.\n        \"\"\"\n        return self.count\n\n    def sample_batch(self, batch_size):\n        \"\"\"\n        Sample a batch of state sequences and corresponding transitions from full episodes.\n\n        Returns past `history_len` steps for each sampled transition, padded with the earliest step if necessary.\n\n        Args:\n            batch_size (int): Number of sequences to sample.\n\n        Returns:\n            Tuple of np.ndarrays: Sequences of past states, actions, rewards, done flags, and next states.\n        \"\"\"\n        if self.count &lt; batch_size:\n            batch = random.sample(\n                list(itertools.islice(self.buffer, 0, len(self.buffer) - 1)), self.count\n            )\n        else:\n            batch = random.sample(\n                list(itertools.islice(self.buffer, 0, len(self.buffer) - 1)), batch_size\n            )\n\n        idx = [random.randint(0, len(b) - 1) for b in batch]\n\n        s_batch = []\n        s2_batch = []\n        for i in range(len(batch)):\n            if idx[i] == len(batch[i]):\n                s = batch[i]\n                s2 = batch[i]\n            else:\n                s = batch[i][: idx[i] + 1]\n                s2 = batch[i][: idx[i] + 1]\n            s = [v[0] for v in s]\n            s = s[::-1]\n\n            s2 = [v[4] for v in s2]\n            s2 = s2[::-1]\n\n            if len(s) &lt; self.history_len:\n                missing = self.history_len - len(s)\n                s += [s[-1]] * missing\n                s2 += [s2[-1]] * missing\n            else:\n                s = s[: self.history_len]\n                s2 = s2[: self.history_len]\n            s = s[::-1]\n            s_batch.append(s)\n            s2 = s2[::-1]\n            s2_batch.append(s2)\n\n        a_batch = np.array([batch[i][idx[i]][1] for i in range(len(batch))])\n        r_batch = np.array([batch[i][idx[i]][2] for i in range(len(batch))]).reshape(\n            -1, 1\n        )\n        t_batch = np.array([batch[i][idx[i]][3] for i in range(len(batch))]).reshape(\n            -1, 1\n        )\n\n        return np.array(s_batch), a_batch, r_batch, t_batch, np.array(s2_batch)\n\n    def clear(self):\n        \"\"\"\n        Clear all stored episodes from the buffer.\n        \"\"\"\n        self.buffer.clear()\n        self.count = 0\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.RolloutReplayBuffer.__init__","title":"<code>__init__(buffer_size, random_seed=123, history_len=10)</code>","text":"<p>Initialize the rollout replay buffer.</p> <p>Parameters:</p> Name Type Description Default <code>buffer_size</code> <code>int</code> <p>Maximum number of episodes (rollouts) to store.</p> required <code>random_seed</code> <code>int</code> <p>Seed for random number generation.</p> <code>123</code> <code>history_len</code> <code>int</code> <p>Number of past steps to return for each sampled state.</p> <code>10</code> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def __init__(self, buffer_size, random_seed=123, history_len=10):\n    \"\"\"\n    Initialize the rollout replay buffer.\n\n    Args:\n        buffer_size (int): Maximum number of episodes (rollouts) to store.\n        random_seed (int): Seed for random number generation.\n        history_len (int): Number of past steps to return for each sampled state.\n    \"\"\"\n    self.buffer_size = buffer_size\n    self.count = 0\n    self.buffer = deque(maxlen=buffer_size)\n    random.seed(random_seed)\n    self.buffer.append([])\n    self.history_len = history_len\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.RolloutReplayBuffer.add","title":"<code>add(s, a, r, t, s2)</code>","text":"<p>Add a transition to the current episode.</p> <p>If the transition ends the episode (t=True), a new episode is started.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>ndarray</code> <p>State.</p> required <code>a</code> <code>ndarray</code> <p>Action.</p> required <code>r</code> <code>float</code> <p>Reward.</p> required <code>t</code> <code>bool</code> <p>Done flag.</p> required <code>s2</code> <code>ndarray</code> <p>Next state.</p> required Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def add(self, s, a, r, t, s2):\n    \"\"\"\n    Add a transition to the current episode.\n\n    If the transition ends the episode (t=True), a new episode is started.\n\n    Args:\n        s (np.ndarray): State.\n        a (np.ndarray): Action.\n        r (float): Reward.\n        t (bool): Done flag.\n        s2 (np.ndarray): Next state.\n    \"\"\"\n    experience = (s, a, r, t, s2)\n    if t:\n        self.count += 1\n        self.buffer[-1].append(experience)\n        self.buffer.append([])\n    else:\n        self.buffer[-1].append(experience)\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.RolloutReplayBuffer.clear","title":"<code>clear()</code>","text":"<p>Clear all stored episodes from the buffer.</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clear all stored episodes from the buffer.\n    \"\"\"\n    self.buffer.clear()\n    self.count = 0\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.RolloutReplayBuffer.sample_batch","title":"<code>sample_batch(batch_size)</code>","text":"<p>Sample a batch of state sequences and corresponding transitions from full episodes.</p> <p>Returns past <code>history_len</code> steps for each sampled transition, padded with the earliest step if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of sequences to sample.</p> required <p>Returns:</p> Type Description <p>Tuple of np.ndarrays: Sequences of past states, actions, rewards, done flags, and next states.</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def sample_batch(self, batch_size):\n    \"\"\"\n    Sample a batch of state sequences and corresponding transitions from full episodes.\n\n    Returns past `history_len` steps for each sampled transition, padded with the earliest step if necessary.\n\n    Args:\n        batch_size (int): Number of sequences to sample.\n\n    Returns:\n        Tuple of np.ndarrays: Sequences of past states, actions, rewards, done flags, and next states.\n    \"\"\"\n    if self.count &lt; batch_size:\n        batch = random.sample(\n            list(itertools.islice(self.buffer, 0, len(self.buffer) - 1)), self.count\n        )\n    else:\n        batch = random.sample(\n            list(itertools.islice(self.buffer, 0, len(self.buffer) - 1)), batch_size\n        )\n\n    idx = [random.randint(0, len(b) - 1) for b in batch]\n\n    s_batch = []\n    s2_batch = []\n    for i in range(len(batch)):\n        if idx[i] == len(batch[i]):\n            s = batch[i]\n            s2 = batch[i]\n        else:\n            s = batch[i][: idx[i] + 1]\n            s2 = batch[i][: idx[i] + 1]\n        s = [v[0] for v in s]\n        s = s[::-1]\n\n        s2 = [v[4] for v in s2]\n        s2 = s2[::-1]\n\n        if len(s) &lt; self.history_len:\n            missing = self.history_len - len(s)\n            s += [s[-1]] * missing\n            s2 += [s2[-1]] * missing\n        else:\n            s = s[: self.history_len]\n            s2 = s2[: self.history_len]\n        s = s[::-1]\n        s_batch.append(s)\n        s2 = s2[::-1]\n        s2_batch.append(s2)\n\n    a_batch = np.array([batch[i][idx[i]][1] for i in range(len(batch))])\n    r_batch = np.array([batch[i][idx[i]][2] for i in range(len(batch))]).reshape(\n        -1, 1\n    )\n    t_batch = np.array([batch[i][idx[i]][3] for i in range(len(batch))]).reshape(\n        -1, 1\n    )\n\n    return np.array(s_batch), a_batch, r_batch, t_batch, np.array(s2_batch)\n</code></pre>"},{"location":"api/Utils/replay_buffer/#robot_nav.replay_buffer.RolloutReplayBuffer.size","title":"<code>size()</code>","text":"<p>Get the number of complete episodes in the buffer.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of episodes.</p> Source code in <code>robot_nav/replay_buffer.py</code> <pre><code>def size(self):\n    \"\"\"\n    Get the number of complete episodes in the buffer.\n\n    Returns:\n        int: Number of episodes.\n    \"\"\"\n    return self.count\n</code></pre>"},{"location":"api/Utils/utils/","title":"Utils","text":""},{"location":"api/Utils/utils/#robot_nav.utils","title":"<code>robot_nav.utils</code>","text":""},{"location":"api/Utils/utils/#robot_nav.utils.Pretraining","title":"<code>Pretraining</code>","text":"<p>Handles loading of offline experience data and pretraining of a reinforcement learning model.</p> <p>Attributes:</p> Name Type Description <code>file_names</code> <code>List[str]</code> <p>List of YAML files containing pre-recorded environment samples.</p> <code>model</code> <code>object</code> <p>The model with <code>prepare_state</code> and <code>train</code> methods.</p> <code>replay_buffer</code> <code>object</code> <p>The buffer used to store experiences for training.</p> <code>reward_function</code> <code>callable</code> <p>Function to compute the reward from the environment state.</p> Source code in <code>robot_nav/utils.py</code> <pre><code>class Pretraining:\n    \"\"\"\n    Handles loading of offline experience data and pretraining of a reinforcement learning model.\n\n    Attributes:\n        file_names (List[str]): List of YAML files containing pre-recorded environment samples.\n        model (object): The model with `prepare_state` and `train` methods.\n        replay_buffer (object): The buffer used to store experiences for training.\n        reward_function (callable): Function to compute the reward from the environment state.\n    \"\"\"\n\n    def __init__(\n        self,\n        file_names: List[str],\n        model: object,\n        replay_buffer: object,\n        reward_function,\n    ):\n        self.file_names = file_names\n        self.model = model\n        self.replay_buffer = replay_buffer\n        self.reward_function = reward_function\n\n    def load_buffer(self):\n        \"\"\"\n        Load samples from the specified files and populate the replay buffer.\n\n        Returns:\n            object: The populated replay buffer.\n        \"\"\"\n        for file_name in self.file_names:\n            print(\"Loading file: \", file_name)\n            with open(file_name, \"r\") as file:\n                samples = yaml.full_load(file)\n                for i in tqdm(range(1, len(samples) - 1)):\n                    sample = samples[i]\n                    latest_scan = sample[\"latest_scan\"]\n                    distance = sample[\"distance\"]\n                    cos = sample[\"cos\"]\n                    sin = sample[\"sin\"]\n                    collision = sample[\"collision\"]\n                    goal = sample[\"goal\"]\n                    action = sample[\"action\"]\n\n                    state, terminal = self.model.prepare_state(\n                        latest_scan, distance, cos, sin, collision, goal, action\n                    )\n\n                    if terminal:\n                        continue\n\n                    next_sample = samples[i + 1]\n                    next_latest_scan = next_sample[\"latest_scan\"]\n                    next_distance = next_sample[\"distance\"]\n                    next_cos = next_sample[\"cos\"]\n                    next_sin = next_sample[\"sin\"]\n                    next_collision = next_sample[\"collision\"]\n                    next_goal = next_sample[\"goal\"]\n                    next_action = next_sample[\"action\"]\n                    next_state, next_terminal = self.model.prepare_state(\n                        next_latest_scan,\n                        next_distance,\n                        next_cos,\n                        next_sin,\n                        next_collision,\n                        next_goal,\n                        next_action,\n                    )\n                    reward = self.reward_function(\n                        next_goal, next_collision, action, next_latest_scan\n                    )\n                    self.replay_buffer.add(\n                        state, action, reward, next_terminal, next_state\n                    )\n\n        return self.replay_buffer\n\n    def train(\n        self,\n        pretraining_iterations,\n        replay_buffer,\n        iterations,\n        batch_size,\n    ):\n        \"\"\"\n        Run pretraining on the model using the replay buffer.\n\n        Args:\n            pretraining_iterations (int): Number of outer loop iterations for pretraining.\n            replay_buffer (object): Buffer to sample training batches from.\n            iterations (int): Number of training steps per pretraining iteration.\n            batch_size (int): Batch size used during training.\n        \"\"\"\n        print(\"Running Pretraining\")\n        for _ in tqdm(range(pretraining_iterations)):\n            self.model.train(\n                replay_buffer=replay_buffer,\n                iterations=iterations,\n                batch_size=batch_size,\n            )\n        print(\"Model Pretrained\")\n</code></pre>"},{"location":"api/Utils/utils/#robot_nav.utils.Pretraining.load_buffer","title":"<code>load_buffer()</code>","text":"<p>Load samples from the specified files and populate the replay buffer.</p> <p>Returns:</p> Name Type Description <code>object</code> <p>The populated replay buffer.</p> Source code in <code>robot_nav/utils.py</code> <pre><code>def load_buffer(self):\n    \"\"\"\n    Load samples from the specified files and populate the replay buffer.\n\n    Returns:\n        object: The populated replay buffer.\n    \"\"\"\n    for file_name in self.file_names:\n        print(\"Loading file: \", file_name)\n        with open(file_name, \"r\") as file:\n            samples = yaml.full_load(file)\n            for i in tqdm(range(1, len(samples) - 1)):\n                sample = samples[i]\n                latest_scan = sample[\"latest_scan\"]\n                distance = sample[\"distance\"]\n                cos = sample[\"cos\"]\n                sin = sample[\"sin\"]\n                collision = sample[\"collision\"]\n                goal = sample[\"goal\"]\n                action = sample[\"action\"]\n\n                state, terminal = self.model.prepare_state(\n                    latest_scan, distance, cos, sin, collision, goal, action\n                )\n\n                if terminal:\n                    continue\n\n                next_sample = samples[i + 1]\n                next_latest_scan = next_sample[\"latest_scan\"]\n                next_distance = next_sample[\"distance\"]\n                next_cos = next_sample[\"cos\"]\n                next_sin = next_sample[\"sin\"]\n                next_collision = next_sample[\"collision\"]\n                next_goal = next_sample[\"goal\"]\n                next_action = next_sample[\"action\"]\n                next_state, next_terminal = self.model.prepare_state(\n                    next_latest_scan,\n                    next_distance,\n                    next_cos,\n                    next_sin,\n                    next_collision,\n                    next_goal,\n                    next_action,\n                )\n                reward = self.reward_function(\n                    next_goal, next_collision, action, next_latest_scan\n                )\n                self.replay_buffer.add(\n                    state, action, reward, next_terminal, next_state\n                )\n\n    return self.replay_buffer\n</code></pre>"},{"location":"api/Utils/utils/#robot_nav.utils.Pretraining.train","title":"<code>train(pretraining_iterations, replay_buffer, iterations, batch_size)</code>","text":"<p>Run pretraining on the model using the replay buffer.</p> <p>Parameters:</p> Name Type Description Default <code>pretraining_iterations</code> <code>int</code> <p>Number of outer loop iterations for pretraining.</p> required <code>replay_buffer</code> <code>object</code> <p>Buffer to sample training batches from.</p> required <code>iterations</code> <code>int</code> <p>Number of training steps per pretraining iteration.</p> required <code>batch_size</code> <code>int</code> <p>Batch size used during training.</p> required Source code in <code>robot_nav/utils.py</code> <pre><code>def train(\n    self,\n    pretraining_iterations,\n    replay_buffer,\n    iterations,\n    batch_size,\n):\n    \"\"\"\n    Run pretraining on the model using the replay buffer.\n\n    Args:\n        pretraining_iterations (int): Number of outer loop iterations for pretraining.\n        replay_buffer (object): Buffer to sample training batches from.\n        iterations (int): Number of training steps per pretraining iteration.\n        batch_size (int): Batch size used during training.\n    \"\"\"\n    print(\"Running Pretraining\")\n    for _ in tqdm(range(pretraining_iterations)):\n        self.model.train(\n            replay_buffer=replay_buffer,\n            iterations=iterations,\n            batch_size=batch_size,\n        )\n    print(\"Model Pretrained\")\n</code></pre>"},{"location":"api/Utils/utils/#robot_nav.utils.get_buffer","title":"<code>get_buffer(model, sim, load_saved_buffer, pretrain, pretraining_iterations, training_iterations, batch_size, buffer_size=50000, random_seed=666, file_names=['robot_nav/assets/data.yml'], history_len=10)</code>","text":"<p>Get or construct the replay buffer depending on model type and training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>The RL model, can be PPO, RCPG, or other.</p> required <code>sim</code> <code>object</code> <p>Simulation environment with a <code>get_reward</code> function.</p> required <code>load_saved_buffer</code> <code>bool</code> <p>Whether to load experiences from file.</p> required <code>pretrain</code> <code>bool</code> <p>Whether to run pretraining using the buffer.</p> required <code>pretraining_iterations</code> <code>int</code> <p>Number of outer loop iterations for pretraining.</p> required <code>training_iterations</code> <code>int</code> <p>Number of iterations in each training loop.</p> required <code>batch_size</code> <code>int</code> <p>Size of the training batch.</p> required <code>buffer_size</code> <code>int</code> <p>Maximum size of the buffer. Defaults to 50000.</p> <code>50000</code> <code>random_seed</code> <code>int</code> <p>Seed for reproducibility. Defaults to 666.</p> <code>666</code> <code>file_names</code> <code>List[str]</code> <p>List of YAML data file paths. Defaults to [\"robot_nav/assets/data.yml\"].</p> <code>['robot_nav/assets/data.yml']</code> <code>history_len</code> <code>int</code> <p>Used for RCPG buffer configuration. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>object</code> <p>The initialized and optionally pre-populated replay buffer.</p> Source code in <code>robot_nav/utils.py</code> <pre><code>def get_buffer(\n    model,\n    sim,\n    load_saved_buffer,\n    pretrain,\n    pretraining_iterations,\n    training_iterations,\n    batch_size,\n    buffer_size=50000,\n    random_seed=666,\n    file_names=[\"robot_nav/assets/data.yml\"],\n    history_len=10,\n):\n    \"\"\"\n    Get or construct the replay buffer depending on model type and training configuration.\n\n    Args:\n        model (object): The RL model, can be PPO, RCPG, or other.\n        sim (object): Simulation environment with a `get_reward` function.\n        load_saved_buffer (bool): Whether to load experiences from file.\n        pretrain (bool): Whether to run pretraining using the buffer.\n        pretraining_iterations (int): Number of outer loop iterations for pretraining.\n        training_iterations (int): Number of iterations in each training loop.\n        batch_size (int): Size of the training batch.\n        buffer_size (int, optional): Maximum size of the buffer. Defaults to 50000.\n        random_seed (int, optional): Seed for reproducibility. Defaults to 666.\n        file_names (List[str], optional): List of YAML data file paths. Defaults to [\"robot_nav/assets/data.yml\"].\n        history_len (int, optional): Used for RCPG buffer configuration. Defaults to 10.\n\n    Returns:\n        object: The initialized and optionally pre-populated replay buffer.\n    \"\"\"\n    if isinstance(model, PPO):\n        return model.buffer\n\n    if isinstance(model, RCPG):\n        replay_buffer = RolloutReplayBuffer(\n            buffer_size=buffer_size, random_seed=random_seed, history_len=history_len\n        )\n    else:\n        replay_buffer = ReplayBuffer(buffer_size=buffer_size, random_seed=random_seed)\n\n    if pretrain:\n        assert (\n            load_saved_buffer\n        ), \"To pre-train model, load_saved_buffer must be set to True\"\n\n    if load_saved_buffer:\n        pretraining = Pretraining(\n            file_names=file_names,\n            model=model,\n            replay_buffer=replay_buffer,\n            reward_function=sim.get_reward,\n        )  # instantiate pre-trainind\n        replay_buffer = (\n            pretraining.load_buffer()\n        )  # fill buffer with experiences from the data.yml file\n        if pretrain:\n            pretraining.train(\n                pretraining_iterations=pretraining_iterations,\n                replay_buffer=replay_buffer,\n                iterations=training_iterations,\n                batch_size=batch_size,\n            )  # run pre-training\n\n    return replay_buffer\n</code></pre>"},{"location":"api/Utils/utils/#robot_nav.utils.get_max_bound","title":"<code>get_max_bound(next_state, discount, max_ang_vel, max_lin_vel, time_step, distance_norm, goal_reward, reward, done, device)</code>","text":"<p>Estimate the maximum possible return (upper bound) from the next state onward.</p> <p>This is used in constrained RL or safe policy optimization where a conservative estimate of return is useful for policy updates.</p> <p>Parameters:</p> Name Type Description Default <code>next_state</code> <code>Tensor</code> <p>Tensor of next state observations.</p> required <code>discount</code> <code>float</code> <p>Discount factor for future rewards.</p> required <code>max_ang_vel</code> <code>float</code> <p>Maximum angular velocity of the agent.</p> required <code>max_lin_vel</code> <code>float</code> <p>Maximum linear velocity of the agent.</p> required <code>time_step</code> <code>float</code> <p>Duration of one time step.</p> required <code>distance_norm</code> <code>float</code> <p>Normalization factor for distance.</p> required <code>goal_reward</code> <code>float</code> <p>Reward received upon reaching the goal.</p> required <code>reward</code> <code>Tensor</code> <p>Immediate reward from the environment.</p> required <code>done</code> <code>Tensor</code> <p>Binary tensor indicating episode termination.</p> required <code>device</code> <code>device</code> <p>PyTorch device for computation.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Maximum return bound for each sample in the batch.</p> Source code in <code>robot_nav/utils.py</code> <pre><code>def get_max_bound(\n    next_state,\n    discount,\n    max_ang_vel,\n    max_lin_vel,\n    time_step,\n    distance_norm,\n    goal_reward,\n    reward,\n    done,\n    device,\n):\n    \"\"\"\n    Estimate the maximum possible return (upper bound) from the next state onward.\n\n    This is used in constrained RL or safe policy optimization where a conservative\n    estimate of return is useful for policy updates.\n\n    Args:\n        next_state (torch.Tensor): Tensor of next state observations.\n        discount (float): Discount factor for future rewards.\n        max_ang_vel (float): Maximum angular velocity of the agent.\n        max_lin_vel (float): Maximum linear velocity of the agent.\n        time_step (float): Duration of one time step.\n        distance_norm (float): Normalization factor for distance.\n        goal_reward (float): Reward received upon reaching the goal.\n        reward (torch.Tensor): Immediate reward from the environment.\n        done (torch.Tensor): Binary tensor indicating episode termination.\n        device (torch.device): PyTorch device for computation.\n\n    Returns:\n        torch.Tensor: Maximum return bound for each sample in the batch.\n    \"\"\"\n    next_state = next_state.clone()  # Prevents in-place modifications\n    reward = reward.clone()  # Ensures original reward is unchanged\n    done = done.clone()\n    cos = next_state[:, -4]\n    sin = next_state[:, -3]\n    theta = torch.atan2(sin, cos)\n\n    # Compute turning steps\n    turn_steps = theta.abs() / (max_ang_vel * time_step)\n    full_turn_steps = torch.floor(turn_steps)\n    turn_rew = -max_ang_vel * discount**full_turn_steps\n    turn_rew[full_turn_steps == 0] = 0  # Handle zero case\n    final_turn_rew = -(discount ** (full_turn_steps + 1)) * (\n        turn_steps - full_turn_steps\n    )\n    full_turn_rew = turn_rew + final_turn_rew\n\n    # Compute distance-based steps\n    full_turn_steps += 1  # Account for the final turn step\n    distances = (next_state[:, -5] * distance_norm) / (max_lin_vel * time_step)\n    final_steps = torch.ceil(distances) + full_turn_steps\n    inter_steps = torch.trunc(distances) + full_turn_steps\n\n    final_rew = goal_reward * discount**final_steps\n\n    # Compute intermediate rewards using a sum of discounted steps\n    max_inter_steps = inter_steps.max().int().item()\n    discount_exponents = discount ** torch.arange(1, max_inter_steps + 1, device=device)\n    inter_rew = torch.stack(\n        [\n            (max_lin_vel * discount_exponents[int(start) + 1 : int(steps)]).sum()\n            for start, steps in zip(full_turn_steps, inter_steps)\n        ]\n    )\n    # Compute final max bound\n    max_bound = reward + (1 - done) * (full_turn_rew + final_rew + inter_rew).view(\n        -1, 1\n    )\n    return max_bound\n</code></pre>"},{"location":"api/models/DDPG/","title":"DDPG","text":""},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG","title":"<code>robot_nav.models.DDPG.DDPG</code>","text":""},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.Actor","title":"<code>Actor</code>","text":"<p>               Bases: <code>Module</code></p> <p>Actor network for the DDPG algorithm.</p> <p>This network maps input states to actions using a fully connected feedforward architecture. It uses Leaky ReLU activations in the hidden layers and a tanh activation at the output to ensure the output actions are in the range [-1, 1].</p> Architecture <ul> <li>Linear(state_dim \u2192 400) + LeakyReLU</li> <li>Linear(400 \u2192 300) + LeakyReLU</li> <li>Linear(300 \u2192 action_dim) + Tanh</li> </ul> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the input state.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the output action space.</p> required Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>class Actor(nn.Module):\n    \"\"\"\n    Actor network for the DDPG algorithm.\n\n    This network maps input states to actions using a fully connected feedforward architecture.\n    It uses Leaky ReLU activations in the hidden layers and a tanh activation at the output\n    to ensure the output actions are in the range [-1, 1].\n\n    Architecture:\n        - Linear(state_dim \u2192 400) + LeakyReLU\n        - Linear(400 \u2192 300) + LeakyReLU\n        - Linear(300 \u2192 action_dim) + Tanh\n\n    Args:\n        state_dim (int): Dimension of the input state.\n        action_dim (int): Dimension of the output action space.\n    \"\"\"\n\n    def __init__(self, state_dim, action_dim):\n        super(Actor, self).__init__()\n\n        self.layer_1 = nn.Linear(state_dim, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2 = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity=\"leaky_relu\")\n        self.layer_3 = nn.Linear(300, action_dim)\n        self.tanh = nn.Tanh()\n\n    def forward(self, s):\n        \"\"\"\n        Forward pass of the actor network.\n\n        Args:\n            s (torch.Tensor): Input state tensor of shape (batch_size, state_dim).\n\n        Returns:\n            torch.Tensor: Output action tensor of shape (batch_size, action_dim), scaled to [-1, 1].\n        \"\"\"\n        s = F.leaky_relu(self.layer_1(s))\n        s = F.leaky_relu(self.layer_2(s))\n        a = self.tanh(self.layer_3(s))\n        return a\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.Actor.forward","title":"<code>forward(s)</code>","text":"<p>Forward pass of the actor network.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_dim).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output action tensor of shape (batch_size, action_dim), scaled to [-1, 1].</p> Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>def forward(self, s):\n    \"\"\"\n    Forward pass of the actor network.\n\n    Args:\n        s (torch.Tensor): Input state tensor of shape (batch_size, state_dim).\n\n    Returns:\n        torch.Tensor: Output action tensor of shape (batch_size, action_dim), scaled to [-1, 1].\n    \"\"\"\n    s = F.leaky_relu(self.layer_1(s))\n    s = F.leaky_relu(self.layer_2(s))\n    a = self.tanh(self.layer_3(s))\n    return a\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.Critic","title":"<code>Critic</code>","text":"<p>               Bases: <code>Module</code></p> <p>Critic network for the DDPG algorithm.</p> <p>This network evaluates the Q-value of a given state-action pair. It separately processes state and action inputs through linear layers, combines them, and passes the result through another linear layer to predict a scalar Q-value.</p> Architecture <ul> <li>Linear(state_dim \u2192 400) + LeakyReLU</li> <li>Linear(400 \u2192 300) [state branch]</li> <li>Linear(action_dim \u2192 300) [action branch]</li> <li>Combine both branches, apply LeakyReLU</li> <li>Linear(300 \u2192 1) for Q-value output</li> </ul> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the input state.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the input action.</p> required Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>class Critic(nn.Module):\n    \"\"\"\n    Critic network for the DDPG algorithm.\n\n    This network evaluates the Q-value of a given state-action pair. It separately processes\n    state and action inputs through linear layers, combines them, and passes the result through\n    another linear layer to predict a scalar Q-value.\n\n    Architecture:\n        - Linear(state_dim \u2192 400) + LeakyReLU\n        - Linear(400 \u2192 300) [state branch]\n        - Linear(action_dim \u2192 300) [action branch]\n        - Combine both branches, apply LeakyReLU\n        - Linear(300 \u2192 1) for Q-value output\n\n    Args:\n        state_dim (int): Dimension of the input state.\n        action_dim (int): Dimension of the input action.\n    \"\"\"\n\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        self.layer_1 = nn.Linear(state_dim, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2_s = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2_s.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2_a = nn.Linear(action_dim, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2_a.weight, nonlinearity=\"leaky_relu\")\n        self.layer_3 = nn.Linear(300, 1)\n        torch.nn.init.kaiming_uniform_(self.layer_3.weight, nonlinearity=\"leaky_relu\")\n\n    def forward(self, s, a):\n        \"\"\"\n        Forward pass of the critic network.\n\n        Args:\n            s (torch.Tensor): State tensor of shape (batch_size, state_dim).\n            a (torch.Tensor): Action tensor of shape (batch_size, action_dim).\n\n        Returns:\n            torch.Tensor: Q-value tensor of shape (batch_size, 1).\n        \"\"\"\n        s1 = F.leaky_relu(self.layer_1(s))\n        self.layer_2_s(s1)\n        self.layer_2_a(a)\n        s11 = torch.mm(s1, self.layer_2_s.weight.data.t())\n        s12 = torch.mm(a, self.layer_2_a.weight.data.t())\n        s1 = F.leaky_relu(s11 + s12 + self.layer_2_a.bias.data)\n        q = self.layer_3(s1)\n\n        return q\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.Critic.forward","title":"<code>forward(s, a)</code>","text":"<p>Forward pass of the critic network.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Tensor</code> <p>State tensor of shape (batch_size, state_dim).</p> required <code>a</code> <code>Tensor</code> <p>Action tensor of shape (batch_size, action_dim).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Q-value tensor of shape (batch_size, 1).</p> Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>def forward(self, s, a):\n    \"\"\"\n    Forward pass of the critic network.\n\n    Args:\n        s (torch.Tensor): State tensor of shape (batch_size, state_dim).\n        a (torch.Tensor): Action tensor of shape (batch_size, action_dim).\n\n    Returns:\n        torch.Tensor: Q-value tensor of shape (batch_size, 1).\n    \"\"\"\n    s1 = F.leaky_relu(self.layer_1(s))\n    self.layer_2_s(s1)\n    self.layer_2_a(a)\n    s11 = torch.mm(s1, self.layer_2_s.weight.data.t())\n    s12 = torch.mm(a, self.layer_2_a.weight.data.t())\n    s1 = F.leaky_relu(s11 + s12 + self.layer_2_a.bias.data)\n    q = self.layer_3(s1)\n\n    return q\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.DDPG","title":"<code>DDPG</code>","text":"<p>               Bases: <code>object</code></p> <p>Deep Deterministic Policy Gradient (DDPG) agent implementation.</p> <p>This class encapsulates the actor-critic learning framework using DDPG, which is suitable for continuous action spaces. It supports training, action selection, model saving/loading, and state preparation for a reinforcement learning agent, specifically designed for robot navigation.</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the input state.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the action space.</p> required <code>max_action</code> <code>float</code> <p>Maximum action value allowed.</p> required <code>device</code> <code>device</code> <p>Computation device (CPU or GPU).</p> required <code>lr</code> <code>float</code> <p>Learning rate for the optimizers. Default is 1e-4.</p> <code>0.0001</code> <code>save_every</code> <code>int</code> <p>Frequency of saving the model in training iterations. 0 means no saving. Default is 0.</p> <code>0</code> <code>load_model</code> <code>bool</code> <p>Flag indicating whether to load a model from disk. Default is False.</p> <code>False</code> <code>save_directory</code> <code>Path</code> <p>Directory to save the model checkpoints. Default is \"robot_nav/models/DDPG/checkpoint\".</p> <code>Path('robot_nav/models/DDPG/checkpoint')</code> <code>model_name</code> <code>str</code> <p>Name used for saving and TensorBoard logging. Default is \"DDPG\".</p> <code>'DDPG'</code> <code>load_directory</code> <code>Path</code> <p>Directory to load model checkpoints from. Default is \"robot_nav/models/DDPG/checkpoint\".</p> <code>Path('robot_nav/models/DDPG/checkpoint')</code> <code>use_max_bound</code> <code>bool</code> <p>Whether to enforce a learned upper bound on the Q-value. Default is False.</p> <code>False</code> <code>bound_weight</code> <code>float</code> <p>Weight of the upper bound loss penalty. Default is 0.25.</p> <code>0.25</code> Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>class DDPG(object):\n    \"\"\"\n    Deep Deterministic Policy Gradient (DDPG) agent implementation.\n\n    This class encapsulates the actor-critic learning framework using DDPG, which is suitable\n    for continuous action spaces. It supports training, action selection, model saving/loading,\n    and state preparation for a reinforcement learning agent, specifically designed for robot navigation.\n\n    Args:\n        state_dim (int): Dimension of the input state.\n        action_dim (int): Dimension of the action space.\n        max_action (float): Maximum action value allowed.\n        device (torch.device): Computation device (CPU or GPU).\n        lr (float): Learning rate for the optimizers. Default is 1e-4.\n        save_every (int): Frequency of saving the model in training iterations. 0 means no saving. Default is 0.\n        load_model (bool): Flag indicating whether to load a model from disk. Default is False.\n        save_directory (Path): Directory to save the model checkpoints. Default is \"robot_nav/models/DDPG/checkpoint\".\n        model_name (str): Name used for saving and TensorBoard logging. Default is \"DDPG\".\n        load_directory (Path): Directory to load model checkpoints from. Default is \"robot_nav/models/DDPG/checkpoint\".\n        use_max_bound (bool): Whether to enforce a learned upper bound on the Q-value. Default is False.\n        bound_weight (float): Weight of the upper bound loss penalty. Default is 0.25.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        max_action,\n        device,\n        lr=1e-4,\n        save_every=0,\n        load_model=False,\n        save_directory=Path(\"robot_nav/models/DDPG/checkpoint\"),\n        model_name=\"DDPG\",\n        load_directory=Path(\"robot_nav/models/DDPG/checkpoint\"),\n        use_max_bound=False,\n        bound_weight=0.25,\n    ):\n        # Initialize the Actor network\n        self.device = device\n        self.actor = Actor(state_dim, action_dim).to(self.device)\n        self.actor_target = Actor(state_dim, action_dim).to(self.device)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = torch.optim.Adam(params=self.actor.parameters(), lr=lr)\n\n        # Initialize the Critic networks\n        self.critic = Critic(state_dim, action_dim).to(self.device)\n        self.critic_target = Critic(state_dim, action_dim).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = torch.optim.Adam(params=self.critic.parameters(), lr=lr)\n\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.state_dim = state_dim\n        self.writer = SummaryWriter(comment=model_name)\n        self.iter_count = 0\n        if load_model:\n            self.load(filename=model_name, directory=load_directory)\n        self.save_every = save_every\n        self.model_name = model_name\n        self.save_directory = save_directory\n        self.use_max_bound = use_max_bound\n        self.bound_weight = bound_weight\n\n    def get_action(self, obs, add_noise):\n        \"\"\"\n        Selects an action based on the observation.\n\n        Args:\n            obs (np.array): The current state observation.\n            add_noise (bool): Whether to add exploration noise to the action.\n\n        Returns:\n            np.array: Action selected by the actor network.\n        \"\"\"\n        if add_noise:\n            return (\n                self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n            ).clip(-self.max_action, self.max_action)\n        else:\n            return self.act(obs)\n\n    def act(self, state):\n        \"\"\"\n        Computes the action for a given state using the actor network.\n\n        Args:\n            state (np.array): Environment state.\n\n        Returns:\n            np.array: Action values as output by the actor network.\n        \"\"\"\n        state = torch.Tensor(state).to(self.device)\n        return self.actor(state).cpu().data.numpy().flatten()\n\n    # training cycle\n    def train(\n        self,\n        replay_buffer,\n        iterations,\n        batch_size,\n        discount=0.99,\n        tau=0.005,\n        policy_noise=0.2,\n        noise_clip=0.5,\n        policy_freq=2,\n        max_lin_vel=0.5,\n        max_ang_vel=1,\n        goal_reward=100,\n        distance_norm=10,\n        time_step=0.3,\n    ):\n        \"\"\"\n        Trains the actor and critic networks using a replay buffer and soft target updates.\n\n        Args:\n            replay_buffer (object): Replay buffer object with a sample_batch method.\n            iterations (int): Number of training iterations.\n            batch_size (int): Size of each training batch.\n            discount (float): Discount factor for future rewards.\n            tau (float): Soft update factor for target networks.\n            policy_noise (float): Standard deviation of noise added to target policy.\n            noise_clip (float): Maximum value to clip target policy noise.\n            policy_freq (int): Frequency of actor and target updates.\n            max_lin_vel (float): Maximum linear velocity, used in Q-bound calculation.\n            max_ang_vel (float): Maximum angular velocity, used in Q-bound calculation.\n            goal_reward (float): Reward given upon reaching goal.\n            distance_norm (float): Distance normalization factor.\n            time_step (float): Time step used in max bound calculation.\n        \"\"\"\n        av_Q = 0\n        max_Q = -inf\n        av_loss = 0\n        for it in range(iterations):\n            # sample a batch from the replay buffer\n            (\n                batch_states,\n                batch_actions,\n                batch_rewards,\n                batch_dones,\n                batch_next_states,\n            ) = replay_buffer.sample_batch(batch_size)\n            state = torch.Tensor(batch_states).to(self.device)\n            next_state = torch.Tensor(batch_next_states).to(self.device)\n            action = torch.Tensor(batch_actions).to(self.device)\n            reward = torch.Tensor(batch_rewards).to(self.device)\n            done = torch.Tensor(batch_dones).to(self.device)\n\n            # Obtain the estimated action from the next state by using the actor-target\n            next_action = self.actor_target(next_state)\n\n            # Add noise to the action\n            noise = (\n                torch.Tensor(batch_actions)\n                .data.normal_(0, policy_noise)\n                .to(self.device)\n            )\n            noise = noise.clamp(-noise_clip, noise_clip)\n            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\n            # Calculate the Q values from the critic-target network for the next state-action pair\n            target_Q = self.critic_target(next_state, next_action)\n\n            av_Q += torch.mean(target_Q)\n            max_Q = max(max_Q, torch.max(target_Q))\n            # Calculate the final Q value from the target network parameters by using Bellman equation\n            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n\n            # Get the Q values of the basis networks with the current parameters\n            current_Q = self.critic(state, action)\n\n            # Calculate the loss between the current Q value and the target Q value\n            loss = F.mse_loss(current_Q, target_Q)\n\n            if self.use_max_bound:\n                max_bound = get_max_bound(\n                    next_state,\n                    discount,\n                    max_ang_vel,\n                    max_lin_vel,\n                    time_step,\n                    distance_norm,\n                    goal_reward,\n                    reward,\n                    done,\n                    self.device,\n                )\n                max_excess_Q = F.relu(current_Q - max_bound)\n                max_bound_loss = (max_excess_Q**2).mean()\n                # Add loss for Q values exceeding maximum possible upper bound\n                loss += self.bound_weight * max_bound_loss\n\n            # Perform the gradient descent\n            self.critic_optimizer.zero_grad()\n            loss.backward()\n            self.critic_optimizer.step()\n\n            if it % policy_freq == 0:\n                # Maximize the actor output value by performing gradient descent on negative Q values\n                # (essentially perform gradient ascent)\n                actor_grad = self.critic(state, self.actor(state))\n                actor_grad = -actor_grad.mean()\n                self.actor_optimizer.zero_grad()\n                actor_grad.backward()\n                self.actor_optimizer.step()\n\n                # Use soft update to update the actor-target network parameters by\n                # infusing small amount of current parameters\n                for param, target_param in zip(\n                    self.actor.parameters(), self.actor_target.parameters()\n                ):\n                    target_param.data.copy_(\n                        tau * param.data + (1 - tau) * target_param.data\n                    )\n                # Use soft update to update the critic-target network parameters by infusing\n                # small amount of current parameters\n                for param, target_param in zip(\n                    self.critic.parameters(), self.critic_target.parameters()\n                ):\n                    target_param.data.copy_(\n                        tau * param.data + (1 - tau) * target_param.data\n                    )\n\n            av_loss += loss\n        self.iter_count += 1\n        # Write new values for tensorboard\n        self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n        self.writer.add_scalar(\"train/avg_Q\", av_Q / iterations, self.iter_count)\n        self.writer.add_scalar(\"train/max_Q\", max_Q, self.iter_count)\n        if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n            self.save(filename=self.model_name, directory=self.save_directory)\n\n    def save(self, filename, directory):\n        \"\"\"\n        Saves the model parameters to disk.\n\n        Args:\n            filename (str): Base filename for saving the model components.\n            directory (str or Path): Directory where the model files will be saved.\n        \"\"\"\n        Path(directory).mkdir(parents=True, exist_ok=True)\n        torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n        torch.save(\n            self.actor_target.state_dict(),\n            \"%s/%s_actor_target.pth\" % (directory, filename),\n        )\n        torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n        torch.save(\n            self.critic_target.state_dict(),\n            \"%s/%s_critic_target.pth\" % (directory, filename),\n        )\n\n    def load(self, filename, directory):\n        \"\"\"\n        Loads model parameters from disk.\n\n        Args:\n            filename (str): Base filename used for loading model components.\n            directory (str or Path): Directory to load the model files from.\n        \"\"\"\n        self.actor.load_state_dict(\n            torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n        )\n        self.actor_target.load_state_dict(\n            torch.load(\"%s/%s_actor_target.pth\" % (directory, filename))\n        )\n        self.critic.load_state_dict(\n            torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n        )\n        self.critic_target.load_state_dict(\n            torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n        )\n        print(f\"Loaded weights from: {directory}\")\n\n    def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n        \"\"\"\n        Processes raw sensor input and additional information into a normalized state representation.\n\n        Args:\n            latest_scan (list or np.array): Raw LIDAR or laser scan data.\n            distance (float): Distance to the goal.\n            cos (float): Cosine of the angle to the goal.\n            sin (float): Sine of the angle to the goal.\n            collision (bool): Whether a collision has occurred.\n            goal (bool): Whether the goal has been reached.\n            action (list or np.array): The action taken in the previous step.\n\n        Returns:\n            return (tuple): (state vector, terminal flag)\n        \"\"\"\n        latest_scan = np.array(latest_scan)\n\n        inf_mask = np.isinf(latest_scan)\n        latest_scan[inf_mask] = 7.0\n\n        max_bins = self.state_dim - 5\n        bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n        # Initialize the list to store the minimum values of each bin\n        min_values = []\n\n        # Loop through the data and create bins\n        for i in range(0, len(latest_scan), bin_size):\n            # Get the current bin\n            bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n            # Find the minimum value in the current bin and append it to the min_values list\n            min_values.append(min(bin) / 7)\n\n        # Normalize to [0, 1] range\n        distance /= 10\n        lin_vel = action[0] * 2\n        ang_vel = (action[1] + 1) / 2\n        state = min_values + [distance, cos, sin] + [lin_vel, ang_vel]\n\n        assert len(state) == self.state_dim\n        terminal = 1 if collision or goal else 0\n\n        return state, terminal\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.DDPG.act","title":"<code>act(state)</code>","text":"<p>Computes the action for a given state using the actor network.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>array</code> <p>Environment state.</p> required <p>Returns:</p> Type Description <p>np.array: Action values as output by the actor network.</p> Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>def act(self, state):\n    \"\"\"\n    Computes the action for a given state using the actor network.\n\n    Args:\n        state (np.array): Environment state.\n\n    Returns:\n        np.array: Action values as output by the actor network.\n    \"\"\"\n    state = torch.Tensor(state).to(self.device)\n    return self.actor(state).cpu().data.numpy().flatten()\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.DDPG.get_action","title":"<code>get_action(obs, add_noise)</code>","text":"<p>Selects an action based on the observation.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>array</code> <p>The current state observation.</p> required <code>add_noise</code> <code>bool</code> <p>Whether to add exploration noise to the action.</p> required <p>Returns:</p> Type Description <p>np.array: Action selected by the actor network.</p> Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>def get_action(self, obs, add_noise):\n    \"\"\"\n    Selects an action based on the observation.\n\n    Args:\n        obs (np.array): The current state observation.\n        add_noise (bool): Whether to add exploration noise to the action.\n\n    Returns:\n        np.array: Action selected by the actor network.\n    \"\"\"\n    if add_noise:\n        return (\n            self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n        ).clip(-self.max_action, self.max_action)\n    else:\n        return self.act(obs)\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.DDPG.load","title":"<code>load(filename, directory)</code>","text":"<p>Loads model parameters from disk.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base filename used for loading model components.</p> required <code>directory</code> <code>str or Path</code> <p>Directory to load the model files from.</p> required Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>def load(self, filename, directory):\n    \"\"\"\n    Loads model parameters from disk.\n\n    Args:\n        filename (str): Base filename used for loading model components.\n        directory (str or Path): Directory to load the model files from.\n    \"\"\"\n    self.actor.load_state_dict(\n        torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n    )\n    self.actor_target.load_state_dict(\n        torch.load(\"%s/%s_actor_target.pth\" % (directory, filename))\n    )\n    self.critic.load_state_dict(\n        torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n    )\n    self.critic_target.load_state_dict(\n        torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n    )\n    print(f\"Loaded weights from: {directory}\")\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.DDPG.prepare_state","title":"<code>prepare_state(latest_scan, distance, cos, sin, collision, goal, action)</code>","text":"<p>Processes raw sensor input and additional information into a normalized state representation.</p> <p>Parameters:</p> Name Type Description Default <code>latest_scan</code> <code>list or array</code> <p>Raw LIDAR or laser scan data.</p> required <code>distance</code> <code>float</code> <p>Distance to the goal.</p> required <code>cos</code> <code>float</code> <p>Cosine of the angle to the goal.</p> required <code>sin</code> <code>float</code> <p>Sine of the angle to the goal.</p> required <code>collision</code> <code>bool</code> <p>Whether a collision has occurred.</p> required <code>goal</code> <code>bool</code> <p>Whether the goal has been reached.</p> required <code>action</code> <code>list or array</code> <p>The action taken in the previous step.</p> required <p>Returns:</p> Name Type Description <code>return</code> <code>tuple</code> <p>(state vector, terminal flag)</p> Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n    \"\"\"\n    Processes raw sensor input and additional information into a normalized state representation.\n\n    Args:\n        latest_scan (list or np.array): Raw LIDAR or laser scan data.\n        distance (float): Distance to the goal.\n        cos (float): Cosine of the angle to the goal.\n        sin (float): Sine of the angle to the goal.\n        collision (bool): Whether a collision has occurred.\n        goal (bool): Whether the goal has been reached.\n        action (list or np.array): The action taken in the previous step.\n\n    Returns:\n        return (tuple): (state vector, terminal flag)\n    \"\"\"\n    latest_scan = np.array(latest_scan)\n\n    inf_mask = np.isinf(latest_scan)\n    latest_scan[inf_mask] = 7.0\n\n    max_bins = self.state_dim - 5\n    bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n    # Initialize the list to store the minimum values of each bin\n    min_values = []\n\n    # Loop through the data and create bins\n    for i in range(0, len(latest_scan), bin_size):\n        # Get the current bin\n        bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n        # Find the minimum value in the current bin and append it to the min_values list\n        min_values.append(min(bin) / 7)\n\n    # Normalize to [0, 1] range\n    distance /= 10\n    lin_vel = action[0] * 2\n    ang_vel = (action[1] + 1) / 2\n    state = min_values + [distance, cos, sin] + [lin_vel, ang_vel]\n\n    assert len(state) == self.state_dim\n    terminal = 1 if collision or goal else 0\n\n    return state, terminal\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.DDPG.save","title":"<code>save(filename, directory)</code>","text":"<p>Saves the model parameters to disk.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base filename for saving the model components.</p> required <code>directory</code> <code>str or Path</code> <p>Directory where the model files will be saved.</p> required Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>def save(self, filename, directory):\n    \"\"\"\n    Saves the model parameters to disk.\n\n    Args:\n        filename (str): Base filename for saving the model components.\n        directory (str or Path): Directory where the model files will be saved.\n    \"\"\"\n    Path(directory).mkdir(parents=True, exist_ok=True)\n    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n    torch.save(\n        self.actor_target.state_dict(),\n        \"%s/%s_actor_target.pth\" % (directory, filename),\n    )\n    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n    torch.save(\n        self.critic_target.state_dict(),\n        \"%s/%s_critic_target.pth\" % (directory, filename),\n    )\n</code></pre>"},{"location":"api/models/DDPG/#robot_nav.models.DDPG.DDPG.DDPG.train","title":"<code>train(replay_buffer, iterations, batch_size, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2, max_lin_vel=0.5, max_ang_vel=1, goal_reward=100, distance_norm=10, time_step=0.3)</code>","text":"<p>Trains the actor and critic networks using a replay buffer and soft target updates.</p> <p>Parameters:</p> Name Type Description Default <code>replay_buffer</code> <code>object</code> <p>Replay buffer object with a sample_batch method.</p> required <code>iterations</code> <code>int</code> <p>Number of training iterations.</p> required <code>batch_size</code> <code>int</code> <p>Size of each training batch.</p> required <code>discount</code> <code>float</code> <p>Discount factor for future rewards.</p> <code>0.99</code> <code>tau</code> <code>float</code> <p>Soft update factor for target networks.</p> <code>0.005</code> <code>policy_noise</code> <code>float</code> <p>Standard deviation of noise added to target policy.</p> <code>0.2</code> <code>noise_clip</code> <code>float</code> <p>Maximum value to clip target policy noise.</p> <code>0.5</code> <code>policy_freq</code> <code>int</code> <p>Frequency of actor and target updates.</p> <code>2</code> <code>max_lin_vel</code> <code>float</code> <p>Maximum linear velocity, used in Q-bound calculation.</p> <code>0.5</code> <code>max_ang_vel</code> <code>float</code> <p>Maximum angular velocity, used in Q-bound calculation.</p> <code>1</code> <code>goal_reward</code> <code>float</code> <p>Reward given upon reaching goal.</p> <code>100</code> <code>distance_norm</code> <code>float</code> <p>Distance normalization factor.</p> <code>10</code> <code>time_step</code> <code>float</code> <p>Time step used in max bound calculation.</p> <code>0.3</code> Source code in <code>robot_nav/models/DDPG/DDPG.py</code> <pre><code>def train(\n    self,\n    replay_buffer,\n    iterations,\n    batch_size,\n    discount=0.99,\n    tau=0.005,\n    policy_noise=0.2,\n    noise_clip=0.5,\n    policy_freq=2,\n    max_lin_vel=0.5,\n    max_ang_vel=1,\n    goal_reward=100,\n    distance_norm=10,\n    time_step=0.3,\n):\n    \"\"\"\n    Trains the actor and critic networks using a replay buffer and soft target updates.\n\n    Args:\n        replay_buffer (object): Replay buffer object with a sample_batch method.\n        iterations (int): Number of training iterations.\n        batch_size (int): Size of each training batch.\n        discount (float): Discount factor for future rewards.\n        tau (float): Soft update factor for target networks.\n        policy_noise (float): Standard deviation of noise added to target policy.\n        noise_clip (float): Maximum value to clip target policy noise.\n        policy_freq (int): Frequency of actor and target updates.\n        max_lin_vel (float): Maximum linear velocity, used in Q-bound calculation.\n        max_ang_vel (float): Maximum angular velocity, used in Q-bound calculation.\n        goal_reward (float): Reward given upon reaching goal.\n        distance_norm (float): Distance normalization factor.\n        time_step (float): Time step used in max bound calculation.\n    \"\"\"\n    av_Q = 0\n    max_Q = -inf\n    av_loss = 0\n    for it in range(iterations):\n        # sample a batch from the replay buffer\n        (\n            batch_states,\n            batch_actions,\n            batch_rewards,\n            batch_dones,\n            batch_next_states,\n        ) = replay_buffer.sample_batch(batch_size)\n        state = torch.Tensor(batch_states).to(self.device)\n        next_state = torch.Tensor(batch_next_states).to(self.device)\n        action = torch.Tensor(batch_actions).to(self.device)\n        reward = torch.Tensor(batch_rewards).to(self.device)\n        done = torch.Tensor(batch_dones).to(self.device)\n\n        # Obtain the estimated action from the next state by using the actor-target\n        next_action = self.actor_target(next_state)\n\n        # Add noise to the action\n        noise = (\n            torch.Tensor(batch_actions)\n            .data.normal_(0, policy_noise)\n            .to(self.device)\n        )\n        noise = noise.clamp(-noise_clip, noise_clip)\n        next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\n        # Calculate the Q values from the critic-target network for the next state-action pair\n        target_Q = self.critic_target(next_state, next_action)\n\n        av_Q += torch.mean(target_Q)\n        max_Q = max(max_Q, torch.max(target_Q))\n        # Calculate the final Q value from the target network parameters by using Bellman equation\n        target_Q = reward + ((1 - done) * discount * target_Q).detach()\n\n        # Get the Q values of the basis networks with the current parameters\n        current_Q = self.critic(state, action)\n\n        # Calculate the loss between the current Q value and the target Q value\n        loss = F.mse_loss(current_Q, target_Q)\n\n        if self.use_max_bound:\n            max_bound = get_max_bound(\n                next_state,\n                discount,\n                max_ang_vel,\n                max_lin_vel,\n                time_step,\n                distance_norm,\n                goal_reward,\n                reward,\n                done,\n                self.device,\n            )\n            max_excess_Q = F.relu(current_Q - max_bound)\n            max_bound_loss = (max_excess_Q**2).mean()\n            # Add loss for Q values exceeding maximum possible upper bound\n            loss += self.bound_weight * max_bound_loss\n\n        # Perform the gradient descent\n        self.critic_optimizer.zero_grad()\n        loss.backward()\n        self.critic_optimizer.step()\n\n        if it % policy_freq == 0:\n            # Maximize the actor output value by performing gradient descent on negative Q values\n            # (essentially perform gradient ascent)\n            actor_grad = self.critic(state, self.actor(state))\n            actor_grad = -actor_grad.mean()\n            self.actor_optimizer.zero_grad()\n            actor_grad.backward()\n            self.actor_optimizer.step()\n\n            # Use soft update to update the actor-target network parameters by\n            # infusing small amount of current parameters\n            for param, target_param in zip(\n                self.actor.parameters(), self.actor_target.parameters()\n            ):\n                target_param.data.copy_(\n                    tau * param.data + (1 - tau) * target_param.data\n                )\n            # Use soft update to update the critic-target network parameters by infusing\n            # small amount of current parameters\n            for param, target_param in zip(\n                self.critic.parameters(), self.critic_target.parameters()\n            ):\n                target_param.data.copy_(\n                    tau * param.data + (1 - tau) * target_param.data\n                )\n\n        av_loss += loss\n    self.iter_count += 1\n    # Write new values for tensorboard\n    self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n    self.writer.add_scalar(\"train/avg_Q\", av_Q / iterations, self.iter_count)\n    self.writer.add_scalar(\"train/max_Q\", max_Q, self.iter_count)\n    if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n        self.save(filename=self.model_name, directory=self.save_directory)\n</code></pre>"},{"location":"api/models/HCM/","title":"Hardcoded Model","text":""},{"location":"api/models/HCM/#robot_nav.models.HCM.hardcoded_model","title":"<code>robot_nav.models.HCM.hardcoded_model</code>","text":""},{"location":"api/models/HCM/#robot_nav.models.HCM.hardcoded_model.HCM","title":"<code>HCM</code>","text":"<p>               Bases: <code>object</code></p> <p>A class representing a hybrid control model (HCM) for a robot's navigation system.</p> <p>This class contains methods for generating actions based on the robot's state, preparing state representations, training (placeholder method), saving/loading models, and logging experiences.</p> <p>Attributes:</p> Name Type Description <code>max_action</code> <code>float</code> <p>The maximum possible action value.</p> <code>state_dim</code> <code>int</code> <p>The dimension of the state representation.</p> <code>writer</code> <code>SummaryWriter</code> <p>The writer for logging purposes.</p> <code>iterator</code> <code>int</code> <p>A counter for tracking sample addition.</p> <code>save_samples</code> <code>bool</code> <p>Whether to save the samples to a file.</p> <code>max_added_samples</code> <code>int</code> <p>Maximum number of samples to be added to the saved file.</p> <code>file_location</code> <code>str</code> <p>The file location for saving samples.</p> Source code in <code>robot_nav/models/HCM/hardcoded_model.py</code> <pre><code>class HCM(object):\n    \"\"\"\n    A class representing a hybrid control model (HCM) for a robot's navigation system.\n\n    This class contains methods for generating actions based on the robot's state, preparing state\n    representations, training (placeholder method), saving/loading models, and logging experiences.\n\n    Attributes:\n        max_action (float): The maximum possible action value.\n        state_dim (int): The dimension of the state representation.\n        writer (SummaryWriter): The writer for logging purposes.\n        iterator (int): A counter for tracking sample addition.\n        save_samples (bool): Whether to save the samples to a file.\n        max_added_samples (int): Maximum number of samples to be added to the saved file.\n        file_location (str): The file location for saving samples.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        max_action,\n        save_samples,\n        max_added_samples=10_000,\n        file_location=\"robot_nav/assets/data.yml\",\n    ):\n        \"\"\"\n        Initialize the HCM class with the provided configuration.\n\n        Args:\n            state_dim (int): The dimension of the state space.\n            max_action (float): The maximum possible action value.\n            save_samples (bool): Whether to save samples to a file.\n            max_added_samples (int): The maximum number of samples to save.\n            file_location (str): The file path for saving samples.\n        \"\"\"\n        self.max_action = max_action\n        self.state_dim = state_dim\n        self.writer = SummaryWriter()\n        self.iterator = 0\n        self.save_samples = save_samples\n        self.max_added_samples = max_added_samples\n        self.file_location = file_location\n\n    def get_action(self, state, add_noise):\n        \"\"\"\n        Compute the action to be taken based on the current state of the robot.\n\n        Args:\n            state (list): The current state of the robot, including LIDAR scan, distance,\n                          and other relevant features.\n            add_noise (bool): Whether to add noise to the action for exploration.\n\n        Returns:\n            list: The computed action [linear velocity, angular velocity].\n        \"\"\"\n        sin = state[-3]\n        cos = state[-4]\n        angle = atan2(sin, cos)\n        laser_nr = self.state_dim - 5\n        limit = 1.5\n\n        if min(state[4 : self.state_dim - 9]) &lt; limit:\n            state = state.tolist()\n            idx = state[:laser_nr].index(min(state[:laser_nr]))\n            if idx &gt; laser_nr / 2:\n                sign = -1\n            else:\n                sign = 1\n\n            idx = clip(idx + sign * 5 * (limit / min(state[:laser_nr])), 0, laser_nr)\n\n            angle = ((3.14 / (laser_nr)) * idx) - 1.57\n\n        rot_vel = clip(angle, -1.0, 1.0)\n        lin_vel = -abs(rot_vel / 2)\n        return [lin_vel, rot_vel]\n\n    # training cycle\n    def train(\n        self,\n        replay_buffer,\n        iterations,\n        batch_size,\n        discount=0.99999,\n        tau=0.005,\n        policy_noise=0.2,\n        noise_clip=0.5,\n        policy_freq=2,\n    ):\n        \"\"\"\n        Placeholder method for training the hybrid control model.\n\n        Args:\n            replay_buffer (object): The replay buffer containing past experiences.\n            iterations (int): The number of training iterations.\n            batch_size (int): The batch size for training.\n            discount (float): The discount factor for future rewards.\n            tau (float): The soft update parameter for target networks.\n            policy_noise (float): The noise added to actions during training.\n            noise_clip (float): The clipping value for action noise.\n            policy_freq (int): The frequency at which to update the policy.\n\n        Note:\n            This method is a placeholder and currently does nothing.\n        \"\"\"\n        pass\n\n    def save(self, filename, directory):\n        \"\"\"\n        Placeholder method to save the current model state to a file.\n\n        Args:\n            filename (str): The name of the file where the model will be saved.\n            directory (str): The directory where the file will be stored.\n\n        Note:\n            This method is a placeholder and currently does nothing.\n        \"\"\"\n        pass\n\n    def load(self, filename, directory):\n        \"\"\"\n        Placeholder method to load a model state from a file.\n\n        Args:\n            filename (str): The name of the file to load the model from.\n            directory (str): The directory where the model file is stored.\n\n        Note:\n            This method is a placeholder and currently does nothing.\n        \"\"\"\n        pass\n\n    def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n        \"\"\"\n        Prepare the state representation for the model based on the current environment.\n\n        Args:\n            latest_scan (list): The LIDAR scan data.\n            distance (float): The distance to the goal.\n            cos (float): The cosine of the robot's orientation angle.\n            sin (float): The sine of the robot's orientation angle.\n            collision (bool): Whether a collision occurred.\n            goal (bool): Whether the goal has been reached.\n            action (list): The action taken by the robot, [linear velocity, angular velocity].\n\n        Returns:\n            tuple: A tuple containing the prepared state and a terminal flag (1 if terminal state, 0 otherwise).\n        \"\"\"\n        latest_scan = np.array(latest_scan)\n\n        inf_mask = np.isinf(latest_scan)\n        latest_scan[inf_mask] = 7.0\n\n        max_bins = self.state_dim - 5\n        bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n        # Initialize the list to store the minimum values of each bin\n        min_values = []\n\n        # Loop through the data and create bins\n        for i in range(0, len(latest_scan), bin_size):\n            # Get the current bin\n            bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n            # Find the minimum value in the current bin and append it to the min_values list\n            min_values.append(min(bin))\n        state = min_values + [distance, cos, sin] + [action[0], action[1]]\n\n        assert len(state) == self.state_dim\n        terminal = 1 if collision or goal else 0\n\n        self.iterator += 1\n        if self.save_samples and self.iterator &lt; self.max_added_samples:\n            action = action if type(action) is list else action\n            action = [float(a) for a in action]\n            sample = {\n                self.iterator: {\n                    \"latest_scan\": latest_scan.tolist(),\n                    \"distance\": distance.tolist(),\n                    \"cos\": cos.tolist(),\n                    \"sin\": sin.tolist(),\n                    \"collision\": collision,\n                    \"goal\": goal,\n                    \"action\": action,\n                }\n            }\n            with open(self.file_location, \"a\") as outfile:\n                yaml.dump(sample, outfile, default_flow_style=False)\n\n        return state, terminal\n</code></pre>"},{"location":"api/models/HCM/#robot_nav.models.HCM.hardcoded_model.HCM.__init__","title":"<code>__init__(state_dim, max_action, save_samples, max_added_samples=10000, file_location='robot_nav/assets/data.yml')</code>","text":"<p>Initialize the HCM class with the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>The dimension of the state space.</p> required <code>max_action</code> <code>float</code> <p>The maximum possible action value.</p> required <code>save_samples</code> <code>bool</code> <p>Whether to save samples to a file.</p> required <code>max_added_samples</code> <code>int</code> <p>The maximum number of samples to save.</p> <code>10000</code> <code>file_location</code> <code>str</code> <p>The file path for saving samples.</p> <code>'robot_nav/assets/data.yml'</code> Source code in <code>robot_nav/models/HCM/hardcoded_model.py</code> <pre><code>def __init__(\n    self,\n    state_dim,\n    max_action,\n    save_samples,\n    max_added_samples=10_000,\n    file_location=\"robot_nav/assets/data.yml\",\n):\n    \"\"\"\n    Initialize the HCM class with the provided configuration.\n\n    Args:\n        state_dim (int): The dimension of the state space.\n        max_action (float): The maximum possible action value.\n        save_samples (bool): Whether to save samples to a file.\n        max_added_samples (int): The maximum number of samples to save.\n        file_location (str): The file path for saving samples.\n    \"\"\"\n    self.max_action = max_action\n    self.state_dim = state_dim\n    self.writer = SummaryWriter()\n    self.iterator = 0\n    self.save_samples = save_samples\n    self.max_added_samples = max_added_samples\n    self.file_location = file_location\n</code></pre>"},{"location":"api/models/HCM/#robot_nav.models.HCM.hardcoded_model.HCM.get_action","title":"<code>get_action(state, add_noise)</code>","text":"<p>Compute the action to be taken based on the current state of the robot.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list</code> <p>The current state of the robot, including LIDAR scan, distance,           and other relevant features.</p> required <code>add_noise</code> <code>bool</code> <p>Whether to add noise to the action for exploration.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The computed action [linear velocity, angular velocity].</p> Source code in <code>robot_nav/models/HCM/hardcoded_model.py</code> <pre><code>def get_action(self, state, add_noise):\n    \"\"\"\n    Compute the action to be taken based on the current state of the robot.\n\n    Args:\n        state (list): The current state of the robot, including LIDAR scan, distance,\n                      and other relevant features.\n        add_noise (bool): Whether to add noise to the action for exploration.\n\n    Returns:\n        list: The computed action [linear velocity, angular velocity].\n    \"\"\"\n    sin = state[-3]\n    cos = state[-4]\n    angle = atan2(sin, cos)\n    laser_nr = self.state_dim - 5\n    limit = 1.5\n\n    if min(state[4 : self.state_dim - 9]) &lt; limit:\n        state = state.tolist()\n        idx = state[:laser_nr].index(min(state[:laser_nr]))\n        if idx &gt; laser_nr / 2:\n            sign = -1\n        else:\n            sign = 1\n\n        idx = clip(idx + sign * 5 * (limit / min(state[:laser_nr])), 0, laser_nr)\n\n        angle = ((3.14 / (laser_nr)) * idx) - 1.57\n\n    rot_vel = clip(angle, -1.0, 1.0)\n    lin_vel = -abs(rot_vel / 2)\n    return [lin_vel, rot_vel]\n</code></pre>"},{"location":"api/models/HCM/#robot_nav.models.HCM.hardcoded_model.HCM.load","title":"<code>load(filename, directory)</code>","text":"<p>Placeholder method to load a model state from a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to load the model from.</p> required <code>directory</code> <code>str</code> <p>The directory where the model file is stored.</p> required Note <p>This method is a placeholder and currently does nothing.</p> Source code in <code>robot_nav/models/HCM/hardcoded_model.py</code> <pre><code>def load(self, filename, directory):\n    \"\"\"\n    Placeholder method to load a model state from a file.\n\n    Args:\n        filename (str): The name of the file to load the model from.\n        directory (str): The directory where the model file is stored.\n\n    Note:\n        This method is a placeholder and currently does nothing.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/HCM/#robot_nav.models.HCM.hardcoded_model.HCM.prepare_state","title":"<code>prepare_state(latest_scan, distance, cos, sin, collision, goal, action)</code>","text":"<p>Prepare the state representation for the model based on the current environment.</p> <p>Parameters:</p> Name Type Description Default <code>latest_scan</code> <code>list</code> <p>The LIDAR scan data.</p> required <code>distance</code> <code>float</code> <p>The distance to the goal.</p> required <code>cos</code> <code>float</code> <p>The cosine of the robot's orientation angle.</p> required <code>sin</code> <code>float</code> <p>The sine of the robot's orientation angle.</p> required <code>collision</code> <code>bool</code> <p>Whether a collision occurred.</p> required <code>goal</code> <code>bool</code> <p>Whether the goal has been reached.</p> required <code>action</code> <code>list</code> <p>The action taken by the robot, [linear velocity, angular velocity].</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the prepared state and a terminal flag (1 if terminal state, 0 otherwise).</p> Source code in <code>robot_nav/models/HCM/hardcoded_model.py</code> <pre><code>def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n    \"\"\"\n    Prepare the state representation for the model based on the current environment.\n\n    Args:\n        latest_scan (list): The LIDAR scan data.\n        distance (float): The distance to the goal.\n        cos (float): The cosine of the robot's orientation angle.\n        sin (float): The sine of the robot's orientation angle.\n        collision (bool): Whether a collision occurred.\n        goal (bool): Whether the goal has been reached.\n        action (list): The action taken by the robot, [linear velocity, angular velocity].\n\n    Returns:\n        tuple: A tuple containing the prepared state and a terminal flag (1 if terminal state, 0 otherwise).\n    \"\"\"\n    latest_scan = np.array(latest_scan)\n\n    inf_mask = np.isinf(latest_scan)\n    latest_scan[inf_mask] = 7.0\n\n    max_bins = self.state_dim - 5\n    bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n    # Initialize the list to store the minimum values of each bin\n    min_values = []\n\n    # Loop through the data and create bins\n    for i in range(0, len(latest_scan), bin_size):\n        # Get the current bin\n        bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n        # Find the minimum value in the current bin and append it to the min_values list\n        min_values.append(min(bin))\n    state = min_values + [distance, cos, sin] + [action[0], action[1]]\n\n    assert len(state) == self.state_dim\n    terminal = 1 if collision or goal else 0\n\n    self.iterator += 1\n    if self.save_samples and self.iterator &lt; self.max_added_samples:\n        action = action if type(action) is list else action\n        action = [float(a) for a in action]\n        sample = {\n            self.iterator: {\n                \"latest_scan\": latest_scan.tolist(),\n                \"distance\": distance.tolist(),\n                \"cos\": cos.tolist(),\n                \"sin\": sin.tolist(),\n                \"collision\": collision,\n                \"goal\": goal,\n                \"action\": action,\n            }\n        }\n        with open(self.file_location, \"a\") as outfile:\n            yaml.dump(sample, outfile, default_flow_style=False)\n\n    return state, terminal\n</code></pre>"},{"location":"api/models/HCM/#robot_nav.models.HCM.hardcoded_model.HCM.save","title":"<code>save(filename, directory)</code>","text":"<p>Placeholder method to save the current model state to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file where the model will be saved.</p> required <code>directory</code> <code>str</code> <p>The directory where the file will be stored.</p> required Note <p>This method is a placeholder and currently does nothing.</p> Source code in <code>robot_nav/models/HCM/hardcoded_model.py</code> <pre><code>def save(self, filename, directory):\n    \"\"\"\n    Placeholder method to save the current model state to a file.\n\n    Args:\n        filename (str): The name of the file where the model will be saved.\n        directory (str): The directory where the file will be stored.\n\n    Note:\n        This method is a placeholder and currently does nothing.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/HCM/#robot_nav.models.HCM.hardcoded_model.HCM.train","title":"<code>train(replay_buffer, iterations, batch_size, discount=0.99999, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2)</code>","text":"<p>Placeholder method for training the hybrid control model.</p> <p>Parameters:</p> Name Type Description Default <code>replay_buffer</code> <code>object</code> <p>The replay buffer containing past experiences.</p> required <code>iterations</code> <code>int</code> <p>The number of training iterations.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> required <code>discount</code> <code>float</code> <p>The discount factor for future rewards.</p> <code>0.99999</code> <code>tau</code> <code>float</code> <p>The soft update parameter for target networks.</p> <code>0.005</code> <code>policy_noise</code> <code>float</code> <p>The noise added to actions during training.</p> <code>0.2</code> <code>noise_clip</code> <code>float</code> <p>The clipping value for action noise.</p> <code>0.5</code> <code>policy_freq</code> <code>int</code> <p>The frequency at which to update the policy.</p> <code>2</code> Note <p>This method is a placeholder and currently does nothing.</p> Source code in <code>robot_nav/models/HCM/hardcoded_model.py</code> <pre><code>def train(\n    self,\n    replay_buffer,\n    iterations,\n    batch_size,\n    discount=0.99999,\n    tau=0.005,\n    policy_noise=0.2,\n    noise_clip=0.5,\n    policy_freq=2,\n):\n    \"\"\"\n    Placeholder method for training the hybrid control model.\n\n    Args:\n        replay_buffer (object): The replay buffer containing past experiences.\n        iterations (int): The number of training iterations.\n        batch_size (int): The batch size for training.\n        discount (float): The discount factor for future rewards.\n        tau (float): The soft update parameter for target networks.\n        policy_noise (float): The noise added to actions during training.\n        noise_clip (float): The clipping value for action noise.\n        policy_freq (int): The frequency at which to update the policy.\n\n    Note:\n        This method is a placeholder and currently does nothing.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/PPO/","title":"PPO","text":""},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO","title":"<code>robot_nav.models.PPO.PPO</code>","text":""},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.ActorCritic","title":"<code>ActorCritic</code>","text":"<p>               Bases: <code>Module</code></p> <p>Actor-Critic neural network model for PPO.</p> <p>Attributes:</p> Name Type Description <code>actor</code> <code>Sequential</code> <p>Policy network (actor) to output action mean.</p> <code>critic</code> <code>Sequential</code> <p>Value network (critic) to predict state values.</p> <code>action_var</code> <code>Tensor</code> <p>Diagonal covariance matrix for action distribution.</p> <code>device</code> <code>str</code> <p>Device used for computation ('cpu' or 'cuda').</p> <code>max_action</code> <code>float</code> <p>Clipping range for action values.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>class ActorCritic(nn.Module):\n    \"\"\"\n    Actor-Critic neural network model for PPO.\n\n    Attributes:\n        actor (nn.Sequential): Policy network (actor) to output action mean.\n        critic (nn.Sequential): Value network (critic) to predict state values.\n        action_var (Tensor): Diagonal covariance matrix for action distribution.\n        device (str): Device used for computation ('cpu' or 'cuda').\n        max_action (float): Clipping range for action values.\n    \"\"\"\n\n    def __init__(self, state_dim, action_dim, action_std_init, max_action, device):\n        \"\"\"\n        Initialize the Actor and Critic networks.\n\n        Args:\n            state_dim (int): Dimension of the input state.\n            action_dim (int): Dimension of the action space.\n            action_std_init (float): Initial standard deviation of the action distribution.\n            max_action (float): Maximum value allowed for an action (clipping range).\n            device (str): Device to run the model on.\n        \"\"\"\n        super(ActorCritic, self).__init__()\n\n        self.device = device\n        self.max_action = max_action\n\n        self.action_dim = action_dim\n        self.action_var = torch.full(\n            (action_dim,), action_std_init * action_std_init\n        ).to(self.device)\n        # actor\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, 400),\n            nn.Tanh(),\n            nn.Linear(400, 300),\n            nn.Tanh(),\n            nn.Linear(300, action_dim),\n            nn.Tanh(),\n        )\n        # critic\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, 400),\n            nn.Tanh(),\n            nn.Linear(400, 300),\n            nn.Tanh(),\n            nn.Linear(300, 1),\n        )\n\n    def set_action_std(self, new_action_std):\n        \"\"\"\n        Set a new standard deviation for the action distribution.\n\n        Args:\n            new_action_std (float): New standard deviation.\n        \"\"\"\n        self.action_var = torch.full(\n            (self.action_dim,), new_action_std * new_action_std\n        ).to(self.device)\n\n    def forward(self):\n        \"\"\"\n        Forward method is not implemented, as it's unused directly.\n\n        Raises:\n            NotImplementedError: Always raised when called.\n        \"\"\"\n        raise NotImplementedError\n\n    def act(self, state, sample):\n        \"\"\"\n        Compute an action, its log probability, and the state value.\n\n        Args:\n            state (Tensor): Input state tensor.\n            sample (bool): Whether to sample from the action distribution or use mean.\n\n        Returns:\n            Tuple[Tensor, Tensor, Tensor]: Sampled (or mean) action, log probability, and state value.\n        \"\"\"\n        action_mean = self.actor(state)\n        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n        dist = MultivariateNormal(action_mean, cov_mat)\n\n        if sample:\n            action = torch.clip(\n                dist.sample(), min=-self.max_action, max=self.max_action\n            )\n        else:\n            action = dist.mean\n        action_logprob = dist.log_prob(action)\n        state_val = self.critic(state)\n\n        return action.detach(), action_logprob.detach(), state_val.detach()\n\n    def evaluate(self, state, action):\n        \"\"\"\n        Evaluate action log probabilities, entropy, and state values for given states and actions.\n\n        Args:\n            state (Tensor): Batch of states.\n            action (Tensor): Batch of actions.\n\n        Returns:\n            Tuple[Tensor, Tensor, Tensor]: Action log probabilities, state values, and distribution entropy.\n        \"\"\"\n        action_mean = self.actor(state)\n\n        action_var = self.action_var.expand_as(action_mean)\n        cov_mat = torch.diag_embed(action_var).to(self.device)\n        dist = MultivariateNormal(action_mean, cov_mat)\n\n        # For Single Action Environments.\n        if self.action_dim == 1:\n            action = action.reshape(-1, self.action_dim)\n\n        action_logprobs = dist.log_prob(action)\n        dist_entropy = dist.entropy()\n        state_values = self.critic(state)\n\n        return action_logprobs, state_values, dist_entropy\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.ActorCritic.__init__","title":"<code>__init__(state_dim, action_dim, action_std_init, max_action, device)</code>","text":"<p>Initialize the Actor and Critic networks.</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the input state.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the action space.</p> required <code>action_std_init</code> <code>float</code> <p>Initial standard deviation of the action distribution.</p> required <code>max_action</code> <code>float</code> <p>Maximum value allowed for an action (clipping range).</p> required <code>device</code> <code>str</code> <p>Device to run the model on.</p> required Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def __init__(self, state_dim, action_dim, action_std_init, max_action, device):\n    \"\"\"\n    Initialize the Actor and Critic networks.\n\n    Args:\n        state_dim (int): Dimension of the input state.\n        action_dim (int): Dimension of the action space.\n        action_std_init (float): Initial standard deviation of the action distribution.\n        max_action (float): Maximum value allowed for an action (clipping range).\n        device (str): Device to run the model on.\n    \"\"\"\n    super(ActorCritic, self).__init__()\n\n    self.device = device\n    self.max_action = max_action\n\n    self.action_dim = action_dim\n    self.action_var = torch.full(\n        (action_dim,), action_std_init * action_std_init\n    ).to(self.device)\n    # actor\n    self.actor = nn.Sequential(\n        nn.Linear(state_dim, 400),\n        nn.Tanh(),\n        nn.Linear(400, 300),\n        nn.Tanh(),\n        nn.Linear(300, action_dim),\n        nn.Tanh(),\n    )\n    # critic\n    self.critic = nn.Sequential(\n        nn.Linear(state_dim, 400),\n        nn.Tanh(),\n        nn.Linear(400, 300),\n        nn.Tanh(),\n        nn.Linear(300, 1),\n    )\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.ActorCritic.act","title":"<code>act(state, sample)</code>","text":"<p>Compute an action, its log probability, and the state value.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>Input state tensor.</p> required <code>sample</code> <code>bool</code> <p>Whether to sample from the action distribution or use mean.</p> required <p>Returns:</p> Type Description <p>Tuple[Tensor, Tensor, Tensor]: Sampled (or mean) action, log probability, and state value.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def act(self, state, sample):\n    \"\"\"\n    Compute an action, its log probability, and the state value.\n\n    Args:\n        state (Tensor): Input state tensor.\n        sample (bool): Whether to sample from the action distribution or use mean.\n\n    Returns:\n        Tuple[Tensor, Tensor, Tensor]: Sampled (or mean) action, log probability, and state value.\n    \"\"\"\n    action_mean = self.actor(state)\n    cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n    dist = MultivariateNormal(action_mean, cov_mat)\n\n    if sample:\n        action = torch.clip(\n            dist.sample(), min=-self.max_action, max=self.max_action\n        )\n    else:\n        action = dist.mean\n    action_logprob = dist.log_prob(action)\n    state_val = self.critic(state)\n\n    return action.detach(), action_logprob.detach(), state_val.detach()\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.ActorCritic.evaluate","title":"<code>evaluate(state, action)</code>","text":"<p>Evaluate action log probabilities, entropy, and state values for given states and actions.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>Batch of states.</p> required <code>action</code> <code>Tensor</code> <p>Batch of actions.</p> required <p>Returns:</p> Type Description <p>Tuple[Tensor, Tensor, Tensor]: Action log probabilities, state values, and distribution entropy.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def evaluate(self, state, action):\n    \"\"\"\n    Evaluate action log probabilities, entropy, and state values for given states and actions.\n\n    Args:\n        state (Tensor): Batch of states.\n        action (Tensor): Batch of actions.\n\n    Returns:\n        Tuple[Tensor, Tensor, Tensor]: Action log probabilities, state values, and distribution entropy.\n    \"\"\"\n    action_mean = self.actor(state)\n\n    action_var = self.action_var.expand_as(action_mean)\n    cov_mat = torch.diag_embed(action_var).to(self.device)\n    dist = MultivariateNormal(action_mean, cov_mat)\n\n    # For Single Action Environments.\n    if self.action_dim == 1:\n        action = action.reshape(-1, self.action_dim)\n\n    action_logprobs = dist.log_prob(action)\n    dist_entropy = dist.entropy()\n    state_values = self.critic(state)\n\n    return action_logprobs, state_values, dist_entropy\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.ActorCritic.forward","title":"<code>forward()</code>","text":"<p>Forward method is not implemented, as it's unused directly.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always raised when called.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def forward(self):\n    \"\"\"\n    Forward method is not implemented, as it's unused directly.\n\n    Raises:\n        NotImplementedError: Always raised when called.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.ActorCritic.set_action_std","title":"<code>set_action_std(new_action_std)</code>","text":"<p>Set a new standard deviation for the action distribution.</p> <p>Parameters:</p> Name Type Description Default <code>new_action_std</code> <code>float</code> <p>New standard deviation.</p> required Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def set_action_std(self, new_action_std):\n    \"\"\"\n    Set a new standard deviation for the action distribution.\n\n    Args:\n        new_action_std (float): New standard deviation.\n    \"\"\"\n    self.action_var = torch.full(\n        (self.action_dim,), new_action_std * new_action_std\n    ).to(self.device)\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.PPO","title":"<code>PPO</code>","text":"<p>Proximal Policy Optimization (PPO) implementation for continuous control tasks.</p> <p>Attributes:</p> Name Type Description <code>max_action</code> <code>float</code> <p>Maximum action value.</p> <code>action_std</code> <code>float</code> <p>Standard deviation of the action distribution.</p> <code>action_std_decay_rate</code> <code>float</code> <p>Rate at which to decay action standard deviation.</p> <code>min_action_std</code> <code>float</code> <p>Minimum allowed action standard deviation.</p> <code>state_dim</code> <code>int</code> <p>Dimension of the state space.</p> <code>gamma</code> <code>float</code> <p>Discount factor for future rewards.</p> <code>eps_clip</code> <code>float</code> <p>Clipping range for policy updates.</p> <code>device</code> <code>str</code> <p>Device for model computation ('cpu' or 'cuda').</p> <code>save_every</code> <code>int</code> <p>Interval (in iterations) for saving model checkpoints.</p> <code>model_name</code> <code>str</code> <p>Name used when saving/loading model.</p> <code>save_directory</code> <code>Path</code> <p>Directory to save model checkpoints.</p> <code>iter_count</code> <code>int</code> <p>Number of training iterations completed.</p> <code>buffer</code> <code>RolloutBuffer</code> <p>Buffer to store trajectories.</p> <code>policy</code> <code>ActorCritic</code> <p>Current actor-critic network.</p> <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for actor and critic.</p> <code>policy_old</code> <code>ActorCritic</code> <p>Old actor-critic network for computing PPO updates.</p> <code>MseLoss</code> <code>Module</code> <p>Mean squared error loss function.</p> <code>writer</code> <code>SummaryWriter</code> <p>TensorBoard summary writer.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>class PPO:\n    \"\"\"\n    Proximal Policy Optimization (PPO) implementation for continuous control tasks.\n\n    Attributes:\n        max_action (float): Maximum action value.\n        action_std (float): Standard deviation of the action distribution.\n        action_std_decay_rate (float): Rate at which to decay action standard deviation.\n        min_action_std (float): Minimum allowed action standard deviation.\n        state_dim (int): Dimension of the state space.\n        gamma (float): Discount factor for future rewards.\n        eps_clip (float): Clipping range for policy updates.\n        device (str): Device for model computation ('cpu' or 'cuda').\n        save_every (int): Interval (in iterations) for saving model checkpoints.\n        model_name (str): Name used when saving/loading model.\n        save_directory (Path): Directory to save model checkpoints.\n        iter_count (int): Number of training iterations completed.\n        buffer (RolloutBuffer): Buffer to store trajectories.\n        policy (ActorCritic): Current actor-critic network.\n        optimizer (torch.optim.Optimizer): Optimizer for actor and critic.\n        policy_old (ActorCritic): Old actor-critic network for computing PPO updates.\n        MseLoss (nn.Module): Mean squared error loss function.\n        writer (SummaryWriter): TensorBoard summary writer.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        max_action,\n        lr_actor=0.0003,\n        lr_critic=0.001,\n        gamma=0.99,\n        eps_clip=0.2,\n        action_std_init=0.6,\n        action_std_decay_rate=0.015,\n        min_action_std=0.1,\n        device=\"cpu\",\n        save_every=10,\n        load_model=False,\n        save_directory=Path(\"robot_nav/models/PPO/checkpoint\"),\n        model_name=\"PPO\",\n        load_directory=Path(\"robot_nav/models/PPO/checkpoint\"),\n    ):\n        self.max_action = max_action\n        self.action_std = action_std_init\n        self.action_std_decay_rate = action_std_decay_rate\n        self.min_action_std = min_action_std\n        self.state_dim = state_dim\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.device = device\n        self.save_every = save_every\n        self.model_name = model_name\n        self.save_directory = save_directory\n        self.iter_count = 0\n\n        self.buffer = RolloutBuffer()\n\n        self.policy = ActorCritic(\n            state_dim, action_dim, action_std_init, self.max_action, self.device\n        ).to(device)\n        self.optimizer = torch.optim.Adam(\n            [\n                {\"params\": self.policy.actor.parameters(), \"lr\": lr_actor},\n                {\"params\": self.policy.critic.parameters(), \"lr\": lr_critic},\n            ]\n        )\n\n        self.policy_old = ActorCritic(\n            state_dim, action_dim, action_std_init, self.max_action, self.device\n        ).to(device)\n        self.policy_old.load_state_dict(self.policy.state_dict())\n        if load_model:\n            self.load(filename=model_name, directory=load_directory)\n\n        self.MseLoss = nn.MSELoss()\n        self.writer = SummaryWriter(comment=model_name)\n\n    def set_action_std(self, new_action_std):\n        \"\"\"\n        Set a new standard deviation for the action distribution.\n\n        Args:\n            new_action_std (float): New standard deviation value.\n        \"\"\"\n        self.action_std = new_action_std\n        self.policy.set_action_std(new_action_std)\n        self.policy_old.set_action_std(new_action_std)\n\n    def decay_action_std(self, action_std_decay_rate, min_action_std):\n        \"\"\"\n        Decay the action standard deviation by a fixed rate, down to a minimum threshold.\n\n        Args:\n            action_std_decay_rate (float): Amount to reduce standard deviation by.\n            min_action_std (float): Minimum value for standard deviation.\n        \"\"\"\n        print(\n            \"--------------------------------------------------------------------------------------------\"\n        )\n        self.action_std = self.action_std - action_std_decay_rate\n        self.action_std = round(self.action_std, 4)\n        if self.action_std &lt;= min_action_std:\n            self.action_std = min_action_std\n            print(\n                \"setting actor output action_std to min_action_std : \", self.action_std\n            )\n        else:\n            print(\"setting actor output action_std to : \", self.action_std)\n        self.set_action_std(self.action_std)\n        print(\n            \"--------------------------------------------------------------------------------------------\"\n        )\n\n    def get_action(self, state, add_noise):\n        \"\"\"\n        Sample an action using the current policy (optionally with noise), and store in buffer if noise is added.\n\n        Args:\n            state (array_like): Input state for the policy.\n            add_noise (bool): Whether to sample from the distribution (True) or use the deterministic mean (False).\n\n        Returns:\n            np.ndarray: Sampled action.\n        \"\"\"\n\n        with torch.no_grad():\n            state = torch.FloatTensor(state).to(self.device)\n            action, action_logprob, state_val = self.policy_old.act(state, add_noise)\n\n        if add_noise:\n            # self.buffer.states.append(state)\n            self.buffer.actions.append(action)\n            self.buffer.logprobs.append(action_logprob)\n            self.buffer.state_values.append(state_val)\n\n        return action.detach().cpu().numpy().flatten()\n\n    def train(self, replay_buffer, iterations, batch_size):\n        \"\"\"\n        Train the policy and value function using PPO loss based on the stored rollout buffer.\n\n        Args:\n            replay_buffer: Placeholder for compatibility (not used).\n            iterations (int): Number of epochs to optimize the policy per update.\n            batch_size (int): Batch size (not used; training uses the whole buffer).\n        \"\"\"\n        # Monte Carlo estimate of returns\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(\n            reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)\n        ):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # Normalizing the rewards\n        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n\n        # convert list to tensor\n        assert len(self.buffer.actions) == len(self.buffer.states)\n\n        states = [torch.tensor(st, dtype=torch.float32) for st in self.buffer.states]\n        old_states = torch.squeeze(torch.stack(states, dim=0)).detach().to(self.device)\n        old_actions = (\n            torch.squeeze(torch.stack(self.buffer.actions, dim=0))\n            .detach()\n            .to(self.device)\n        )\n        old_logprobs = (\n            torch.squeeze(torch.stack(self.buffer.logprobs, dim=0))\n            .detach()\n            .to(self.device)\n        )\n        old_state_values = (\n            torch.squeeze(torch.stack(self.buffer.state_values, dim=0))\n            .detach()\n            .to(self.device)\n        )\n\n        # calculate advantages\n        advantages = rewards.detach() - old_state_values.detach()\n\n        av_state_values = 0\n        max_state_value = -inf\n        av_loss = 0\n        # Optimize policy for K epochs\n        for _ in range(iterations):\n            # Evaluating old actions and values\n            logprobs, state_values, dist_entropy = self.policy.evaluate(\n                old_states, old_actions\n            )\n\n            # match state_values tensor dimensions with rewards tensor\n            state_values = torch.squeeze(state_values)\n            av_state_values += torch.mean(state_values)\n            max_state_value = max(max_state_value, max(state_values))\n            # Finding the ratio (pi_theta / pi_theta__old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # Finding Surrogate Loss\n            surr1 = ratios * advantages\n            surr2 = (\n                torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n            )\n\n            # final loss of clipped objective PPO\n            loss = (\n                -torch.min(surr1, surr2)\n                + 0.5 * self.MseLoss(state_values, rewards)\n                - 0.01 * dist_entropy\n            )\n\n            # take gradient step\n            self.optimizer.zero_grad()\n            loss.mean().backward()\n            self.optimizer.step()\n            av_loss += loss.mean()\n\n        # Copy new weights into old policy\n        self.policy_old.load_state_dict(self.policy.state_dict())\n        # clear buffer\n        self.buffer.clear()\n        self.decay_action_std(self.action_std_decay_rate, self.min_action_std)\n        self.iter_count += 1\n        # Write new values for tensorboard\n        self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n        self.writer.add_scalar(\n            \"train/avg_value\", av_state_values / iterations, self.iter_count\n        )\n        self.writer.add_scalar(\"train/max_value\", max_state_value, self.iter_count)\n        if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n            self.save(filename=self.model_name, directory=self.save_directory)\n\n    def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n        \"\"\"\n        Convert raw sensor and navigation data into a normalized state vector for the policy.\n\n        Args:\n            latest_scan (list[float]): LIDAR scan data.\n            distance (float): Distance to the goal.\n            cos (float): Cosine of angle to the goal.\n            sin (float): Sine of angle to the goal.\n            collision (bool): Whether the robot has collided.\n            goal (bool): Whether the robot has reached the goal.\n            action (tuple[float, float]): Last action taken (linear and angular velocities).\n\n        Returns:\n            tuple[list[float], int]: Processed state vector and terminal flag (1 if terminal, else 0).\n        \"\"\"\n        latest_scan = np.array(latest_scan)\n\n        inf_mask = np.isinf(latest_scan)\n        latest_scan[inf_mask] = 7.0\n\n        max_bins = self.state_dim - 5\n        bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n        # Initialize the list to store the minimum values of each bin\n        min_values = []\n\n        # Loop through the data and create bins\n        for i in range(0, len(latest_scan), bin_size):\n            # Get the current bin\n            bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n            # Find the minimum value in the current bin and append it to the min_values list\n            min_values.append(min(bin) / 7)\n\n        # Normalize to [0, 1] range\n        distance /= 10\n        lin_vel = action[0] * 2\n        ang_vel = (action[1] + 1) / 2\n        state = min_values + [distance, cos, sin] + [lin_vel, ang_vel]\n\n        assert len(state) == self.state_dim\n        terminal = 1 if collision or goal else 0\n\n        return state, terminal\n\n    def save(self, filename, directory):\n        \"\"\"\n        Save the current policy model to the specified directory.\n\n        Args:\n            filename (str): Base name of the model file.\n            directory (Path): Directory to save the model to.\n        \"\"\"\n        Path(directory).mkdir(parents=True, exist_ok=True)\n        torch.save(\n            self.policy_old.state_dict(), \"%s/%s_policy.pth\" % (directory, filename)\n        )\n\n    def load(self, filename, directory):\n        \"\"\"\n        Load the policy model from a saved checkpoint.\n\n        Args:\n            filename (str): Base name of the model file.\n            directory (Path): Directory to load the model from.\n        \"\"\"\n        self.policy_old.load_state_dict(\n            torch.load(\n                \"%s/%s_policy.pth\" % (directory, filename),\n                map_location=lambda storage, loc: storage,\n            )\n        )\n        self.policy.load_state_dict(\n            torch.load(\n                \"%s/%s_policy.pth\" % (directory, filename),\n                map_location=lambda storage, loc: storage,\n            )\n        )\n        print(f\"Loaded weights from: {directory}\")\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.PPO.decay_action_std","title":"<code>decay_action_std(action_std_decay_rate, min_action_std)</code>","text":"<p>Decay the action standard deviation by a fixed rate, down to a minimum threshold.</p> <p>Parameters:</p> Name Type Description Default <code>action_std_decay_rate</code> <code>float</code> <p>Amount to reduce standard deviation by.</p> required <code>min_action_std</code> <code>float</code> <p>Minimum value for standard deviation.</p> required Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def decay_action_std(self, action_std_decay_rate, min_action_std):\n    \"\"\"\n    Decay the action standard deviation by a fixed rate, down to a minimum threshold.\n\n    Args:\n        action_std_decay_rate (float): Amount to reduce standard deviation by.\n        min_action_std (float): Minimum value for standard deviation.\n    \"\"\"\n    print(\n        \"--------------------------------------------------------------------------------------------\"\n    )\n    self.action_std = self.action_std - action_std_decay_rate\n    self.action_std = round(self.action_std, 4)\n    if self.action_std &lt;= min_action_std:\n        self.action_std = min_action_std\n        print(\n            \"setting actor output action_std to min_action_std : \", self.action_std\n        )\n    else:\n        print(\"setting actor output action_std to : \", self.action_std)\n    self.set_action_std(self.action_std)\n    print(\n        \"--------------------------------------------------------------------------------------------\"\n    )\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.PPO.get_action","title":"<code>get_action(state, add_noise)</code>","text":"<p>Sample an action using the current policy (optionally with noise), and store in buffer if noise is added.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>array_like</code> <p>Input state for the policy.</p> required <code>add_noise</code> <code>bool</code> <p>Whether to sample from the distribution (True) or use the deterministic mean (False).</p> required <p>Returns:</p> Type Description <p>np.ndarray: Sampled action.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def get_action(self, state, add_noise):\n    \"\"\"\n    Sample an action using the current policy (optionally with noise), and store in buffer if noise is added.\n\n    Args:\n        state (array_like): Input state for the policy.\n        add_noise (bool): Whether to sample from the distribution (True) or use the deterministic mean (False).\n\n    Returns:\n        np.ndarray: Sampled action.\n    \"\"\"\n\n    with torch.no_grad():\n        state = torch.FloatTensor(state).to(self.device)\n        action, action_logprob, state_val = self.policy_old.act(state, add_noise)\n\n    if add_noise:\n        # self.buffer.states.append(state)\n        self.buffer.actions.append(action)\n        self.buffer.logprobs.append(action_logprob)\n        self.buffer.state_values.append(state_val)\n\n    return action.detach().cpu().numpy().flatten()\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.PPO.load","title":"<code>load(filename, directory)</code>","text":"<p>Load the policy model from a saved checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base name of the model file.</p> required <code>directory</code> <code>Path</code> <p>Directory to load the model from.</p> required Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def load(self, filename, directory):\n    \"\"\"\n    Load the policy model from a saved checkpoint.\n\n    Args:\n        filename (str): Base name of the model file.\n        directory (Path): Directory to load the model from.\n    \"\"\"\n    self.policy_old.load_state_dict(\n        torch.load(\n            \"%s/%s_policy.pth\" % (directory, filename),\n            map_location=lambda storage, loc: storage,\n        )\n    )\n    self.policy.load_state_dict(\n        torch.load(\n            \"%s/%s_policy.pth\" % (directory, filename),\n            map_location=lambda storage, loc: storage,\n        )\n    )\n    print(f\"Loaded weights from: {directory}\")\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.PPO.prepare_state","title":"<code>prepare_state(latest_scan, distance, cos, sin, collision, goal, action)</code>","text":"<p>Convert raw sensor and navigation data into a normalized state vector for the policy.</p> <p>Parameters:</p> Name Type Description Default <code>latest_scan</code> <code>list[float]</code> <p>LIDAR scan data.</p> required <code>distance</code> <code>float</code> <p>Distance to the goal.</p> required <code>cos</code> <code>float</code> <p>Cosine of angle to the goal.</p> required <code>sin</code> <code>float</code> <p>Sine of angle to the goal.</p> required <code>collision</code> <code>bool</code> <p>Whether the robot has collided.</p> required <code>goal</code> <code>bool</code> <p>Whether the robot has reached the goal.</p> required <code>action</code> <code>tuple[float, float]</code> <p>Last action taken (linear and angular velocities).</p> required <p>Returns:</p> Type Description <p>tuple[list[float], int]: Processed state vector and terminal flag (1 if terminal, else 0).</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n    \"\"\"\n    Convert raw sensor and navigation data into a normalized state vector for the policy.\n\n    Args:\n        latest_scan (list[float]): LIDAR scan data.\n        distance (float): Distance to the goal.\n        cos (float): Cosine of angle to the goal.\n        sin (float): Sine of angle to the goal.\n        collision (bool): Whether the robot has collided.\n        goal (bool): Whether the robot has reached the goal.\n        action (tuple[float, float]): Last action taken (linear and angular velocities).\n\n    Returns:\n        tuple[list[float], int]: Processed state vector and terminal flag (1 if terminal, else 0).\n    \"\"\"\n    latest_scan = np.array(latest_scan)\n\n    inf_mask = np.isinf(latest_scan)\n    latest_scan[inf_mask] = 7.0\n\n    max_bins = self.state_dim - 5\n    bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n    # Initialize the list to store the minimum values of each bin\n    min_values = []\n\n    # Loop through the data and create bins\n    for i in range(0, len(latest_scan), bin_size):\n        # Get the current bin\n        bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n        # Find the minimum value in the current bin and append it to the min_values list\n        min_values.append(min(bin) / 7)\n\n    # Normalize to [0, 1] range\n    distance /= 10\n    lin_vel = action[0] * 2\n    ang_vel = (action[1] + 1) / 2\n    state = min_values + [distance, cos, sin] + [lin_vel, ang_vel]\n\n    assert len(state) == self.state_dim\n    terminal = 1 if collision or goal else 0\n\n    return state, terminal\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.PPO.save","title":"<code>save(filename, directory)</code>","text":"<p>Save the current policy model to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base name of the model file.</p> required <code>directory</code> <code>Path</code> <p>Directory to save the model to.</p> required Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def save(self, filename, directory):\n    \"\"\"\n    Save the current policy model to the specified directory.\n\n    Args:\n        filename (str): Base name of the model file.\n        directory (Path): Directory to save the model to.\n    \"\"\"\n    Path(directory).mkdir(parents=True, exist_ok=True)\n    torch.save(\n        self.policy_old.state_dict(), \"%s/%s_policy.pth\" % (directory, filename)\n    )\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.PPO.set_action_std","title":"<code>set_action_std(new_action_std)</code>","text":"<p>Set a new standard deviation for the action distribution.</p> <p>Parameters:</p> Name Type Description Default <code>new_action_std</code> <code>float</code> <p>New standard deviation value.</p> required Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def set_action_std(self, new_action_std):\n    \"\"\"\n    Set a new standard deviation for the action distribution.\n\n    Args:\n        new_action_std (float): New standard deviation value.\n    \"\"\"\n    self.action_std = new_action_std\n    self.policy.set_action_std(new_action_std)\n    self.policy_old.set_action_std(new_action_std)\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.PPO.train","title":"<code>train(replay_buffer, iterations, batch_size)</code>","text":"<p>Train the policy and value function using PPO loss based on the stored rollout buffer.</p> <p>Parameters:</p> Name Type Description Default <code>replay_buffer</code> <p>Placeholder for compatibility (not used).</p> required <code>iterations</code> <code>int</code> <p>Number of epochs to optimize the policy per update.</p> required <code>batch_size</code> <code>int</code> <p>Batch size (not used; training uses the whole buffer).</p> required Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def train(self, replay_buffer, iterations, batch_size):\n    \"\"\"\n    Train the policy and value function using PPO loss based on the stored rollout buffer.\n\n    Args:\n        replay_buffer: Placeholder for compatibility (not used).\n        iterations (int): Number of epochs to optimize the policy per update.\n        batch_size (int): Batch size (not used; training uses the whole buffer).\n    \"\"\"\n    # Monte Carlo estimate of returns\n    rewards = []\n    discounted_reward = 0\n    for reward, is_terminal in zip(\n        reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)\n    ):\n        if is_terminal:\n            discounted_reward = 0\n        discounted_reward = reward + (self.gamma * discounted_reward)\n        rewards.insert(0, discounted_reward)\n\n    # Normalizing the rewards\n    rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n\n    # convert list to tensor\n    assert len(self.buffer.actions) == len(self.buffer.states)\n\n    states = [torch.tensor(st, dtype=torch.float32) for st in self.buffer.states]\n    old_states = torch.squeeze(torch.stack(states, dim=0)).detach().to(self.device)\n    old_actions = (\n        torch.squeeze(torch.stack(self.buffer.actions, dim=0))\n        .detach()\n        .to(self.device)\n    )\n    old_logprobs = (\n        torch.squeeze(torch.stack(self.buffer.logprobs, dim=0))\n        .detach()\n        .to(self.device)\n    )\n    old_state_values = (\n        torch.squeeze(torch.stack(self.buffer.state_values, dim=0))\n        .detach()\n        .to(self.device)\n    )\n\n    # calculate advantages\n    advantages = rewards.detach() - old_state_values.detach()\n\n    av_state_values = 0\n    max_state_value = -inf\n    av_loss = 0\n    # Optimize policy for K epochs\n    for _ in range(iterations):\n        # Evaluating old actions and values\n        logprobs, state_values, dist_entropy = self.policy.evaluate(\n            old_states, old_actions\n        )\n\n        # match state_values tensor dimensions with rewards tensor\n        state_values = torch.squeeze(state_values)\n        av_state_values += torch.mean(state_values)\n        max_state_value = max(max_state_value, max(state_values))\n        # Finding the ratio (pi_theta / pi_theta__old)\n        ratios = torch.exp(logprobs - old_logprobs.detach())\n\n        # Finding Surrogate Loss\n        surr1 = ratios * advantages\n        surr2 = (\n            torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n        )\n\n        # final loss of clipped objective PPO\n        loss = (\n            -torch.min(surr1, surr2)\n            + 0.5 * self.MseLoss(state_values, rewards)\n            - 0.01 * dist_entropy\n        )\n\n        # take gradient step\n        self.optimizer.zero_grad()\n        loss.mean().backward()\n        self.optimizer.step()\n        av_loss += loss.mean()\n\n    # Copy new weights into old policy\n    self.policy_old.load_state_dict(self.policy.state_dict())\n    # clear buffer\n    self.buffer.clear()\n    self.decay_action_std(self.action_std_decay_rate, self.min_action_std)\n    self.iter_count += 1\n    # Write new values for tensorboard\n    self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n    self.writer.add_scalar(\n        \"train/avg_value\", av_state_values / iterations, self.iter_count\n    )\n    self.writer.add_scalar(\"train/max_value\", max_state_value, self.iter_count)\n    if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n        self.save(filename=self.model_name, directory=self.save_directory)\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.RolloutBuffer","title":"<code>RolloutBuffer</code>","text":"<p>Buffer to store rollout data (transitions) for PPO training.</p> <p>Attributes:</p> Name Type Description <code>actions</code> <code>list</code> <p>Actions taken by the agent.</p> <code>states</code> <code>list</code> <p>States observed by the agent.</p> <code>logprobs</code> <code>list</code> <p>Log probabilities of the actions.</p> <code>rewards</code> <code>list</code> <p>Rewards received from the environment.</p> <code>state_values</code> <code>list</code> <p>Value estimates for the states.</p> <code>is_terminals</code> <code>list</code> <p>Flags indicating episode termination.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>class RolloutBuffer:\n    \"\"\"\n    Buffer to store rollout data (transitions) for PPO training.\n\n    Attributes:\n        actions (list): Actions taken by the agent.\n        states (list): States observed by the agent.\n        logprobs (list): Log probabilities of the actions.\n        rewards (list): Rewards received from the environment.\n        state_values (list): Value estimates for the states.\n        is_terminals (list): Flags indicating episode termination.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize empty lists to store buffer elements.\n        \"\"\"\n        self.actions = []\n        self.states = []\n        self.logprobs = []\n        self.rewards = []\n        self.state_values = []\n        self.is_terminals = []\n\n    def clear(self):\n        \"\"\"\n        Clear all stored data from the buffer.\n        \"\"\"\n        del self.actions[:]\n        del self.states[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.state_values[:]\n        del self.is_terminals[:]\n\n    def add(self, state, action, reward, terminal, next_state):\n        \"\"\"\n        Add a transition to the buffer. (Partial implementation.)\n\n        Args:\n            state: The current observed state.\n            action: The action taken.\n            reward: The reward received after taking the action.\n            terminal (bool): Whether the episode terminated.\n            next_state: The resulting state after taking the action.\n        \"\"\"\n        self.states.append(state)\n        self.rewards.append(reward)\n        self.is_terminals.append(terminal)\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.RolloutBuffer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize empty lists to store buffer elements.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize empty lists to store buffer elements.\n    \"\"\"\n    self.actions = []\n    self.states = []\n    self.logprobs = []\n    self.rewards = []\n    self.state_values = []\n    self.is_terminals = []\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.RolloutBuffer.add","title":"<code>add(state, action, reward, terminal, next_state)</code>","text":"<p>Add a transition to the buffer. (Partial implementation.)</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <p>The current observed state.</p> required <code>action</code> <p>The action taken.</p> required <code>reward</code> <p>The reward received after taking the action.</p> required <code>terminal</code> <code>bool</code> <p>Whether the episode terminated.</p> required <code>next_state</code> <p>The resulting state after taking the action.</p> required Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def add(self, state, action, reward, terminal, next_state):\n    \"\"\"\n    Add a transition to the buffer. (Partial implementation.)\n\n    Args:\n        state: The current observed state.\n        action: The action taken.\n        reward: The reward received after taking the action.\n        terminal (bool): Whether the episode terminated.\n        next_state: The resulting state after taking the action.\n    \"\"\"\n    self.states.append(state)\n    self.rewards.append(reward)\n    self.is_terminals.append(terminal)\n</code></pre>"},{"location":"api/models/PPO/#robot_nav.models.PPO.PPO.RolloutBuffer.clear","title":"<code>clear()</code>","text":"<p>Clear all stored data from the buffer.</p> Source code in <code>robot_nav/models/PPO/PPO.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clear all stored data from the buffer.\n    \"\"\"\n    del self.actions[:]\n    del self.states[:]\n    del self.logprobs[:]\n    del self.rewards[:]\n    del self.state_values[:]\n    del self.is_terminals[:]\n</code></pre>"},{"location":"api/models/RCPG/","title":"RCPG","text":""},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG","title":"<code>robot_nav.models.RCPG.RCPG</code>","text":""},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.Actor","title":"<code>Actor</code>","text":"<p>               Bases: <code>Module</code></p> <p>Actor network that outputs continuous actions for a given state input.</p> Architecture <ul> <li>Processes 1D laser scan inputs through 3 convolutional layers.</li> <li>Embeds goal and previous action inputs using fully connected layers.</li> <li>Combines all features and passes them through an RNN (GRU, LSTM, or RNN).</li> <li>Outputs action values via a fully connected feedforward head with Tanh activation.</li> </ul>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.Actor--parameters","title":"Parameters","text":"<p>action_dim : int     Dimensionality of the action space. rnn : str, optional     Type of RNN layer to use (\"lstm\", \"gru\", or \"rnn\").</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>class Actor(nn.Module):\n    \"\"\"\n    Actor network that outputs continuous actions for a given state input.\n\n    Architecture:\n        - Processes 1D laser scan inputs through 3 convolutional layers.\n        - Embeds goal and previous action inputs using fully connected layers.\n        - Combines all features and passes them through an RNN (GRU, LSTM, or RNN).\n        - Outputs action values via a fully connected feedforward head with Tanh activation.\n\n    Parameters\n    ----------\n    action_dim : int\n        Dimensionality of the action space.\n    rnn : str, optional\n        Type of RNN layer to use (\"lstm\", \"gru\", or \"rnn\").\n    \"\"\"\n\n    def __init__(self, action_dim, rnn=\"gru\"):\n        super(Actor, self).__init__()\n        assert rnn in [\"lstm\", \"gru\", \"rnn\"], \"Unsupported rnn type\"\n\n        self.cnn1 = nn.Conv1d(1, 4, kernel_size=8, stride=4)\n        self.cnn2 = nn.Conv1d(4, 8, kernel_size=8, stride=4)\n        self.cnn3 = nn.Conv1d(8, 4, kernel_size=4, stride=2)\n\n        self.goal_embed = nn.Linear(3, 10)\n        self.action_embed = nn.Linear(2, 10)\n\n        if rnn == \"lstm\":\n            self.rnn = nn.LSTM(\n                input_size=36, hidden_size=36, num_layers=1, batch_first=True\n            )\n        elif rnn == \"gru\":\n            self.rnn = nn.GRU(\n                input_size=36, hidden_size=36, num_layers=1, batch_first=True\n            )\n        else:\n            self.rnn = nn.RNN(\n                input_size=36, hidden_size=36, num_layers=1, batch_first=True\n            )\n\n        self.layer_1 = nn.Linear(36, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2 = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity=\"leaky_relu\")\n        self.layer_3 = nn.Linear(300, action_dim)\n        self.tanh = nn.Tanh()\n\n    def forward(self, s):\n        if len(s.shape) == 2:\n            s = s.unsqueeze(0)\n\n        batch_n, hist_n, state_n = s.shape\n        s = s.reshape(batch_n * hist_n, state_n)\n\n        laser = s[:, :-5]\n        goal = s[:, -5:-2]\n        act = s[:, -2:]\n        laser = laser.unsqueeze(1)\n\n        l = F.leaky_relu(self.cnn1(laser))\n        l = F.leaky_relu(self.cnn2(l))\n        l = F.leaky_relu(self.cnn3(l))\n        l = l.flatten(start_dim=1)\n\n        g = F.leaky_relu(self.goal_embed(goal))\n\n        a = F.leaky_relu(self.action_embed(act))\n\n        s = torch.concat((l, g, a), dim=-1)\n\n        s = s.reshape(batch_n, hist_n, -1)\n        output, _ = self.rnn(s)\n        last_output = output[:, -1, :]\n        s = F.leaky_relu(self.layer_1(last_output))\n        s = F.leaky_relu(self.layer_2(s))\n        a = self.tanh(self.layer_3(s))\n        return a\n</code></pre>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.Critic","title":"<code>Critic</code>","text":"<p>               Bases: <code>Module</code></p> <p>Critic network that estimates Q-values for state-action pairs.</p> Architecture <ul> <li>Processes the same input as the Actor (laser scan, goal, and previous action).</li> <li>Uses two separate Q-networks (double Q-learning) for stability.</li> <li>Each Q-network receives both the RNN-processed state and current action.</li> </ul>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.Critic--parameters","title":"Parameters","text":"<p>action_dim : int     Dimensionality of the action space. rnn : str, optional     Type of RNN layer to use (\"lstm\", \"gru\", or \"rnn\").</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>class Critic(nn.Module):\n    \"\"\"\n    Critic network that estimates Q-values for state-action pairs.\n\n    Architecture:\n        - Processes the same input as the Actor (laser scan, goal, and previous action).\n        - Uses two separate Q-networks (double Q-learning) for stability.\n        - Each Q-network receives both the RNN-processed state and current action.\n\n    Parameters\n    ----------\n    action_dim : int\n        Dimensionality of the action space.\n    rnn : str, optional\n        Type of RNN layer to use (\"lstm\", \"gru\", or \"rnn\").\n    \"\"\"\n\n    def __init__(self, action_dim, rnn=\"gru\"):\n        super(Critic, self).__init__()\n        assert rnn in [\"lstm\", \"gru\", \"rnn\"], \"Unsupported rnn type\"\n\n        self.cnn1 = nn.Conv1d(1, 4, kernel_size=8, stride=4)\n        self.cnn2 = nn.Conv1d(4, 8, kernel_size=8, stride=4)\n        self.cnn3 = nn.Conv1d(8, 4, kernel_size=4, stride=2)\n\n        self.goal_embed = nn.Linear(3, 10)\n        self.action_embed = nn.Linear(2, 10)\n\n        if rnn == \"lstm\":\n            self.rnn = nn.LSTM(\n                input_size=36, hidden_size=36, num_layers=1, batch_first=True\n            )\n        elif rnn == \"gru\":\n            self.rnn = nn.GRU(\n                input_size=36, hidden_size=36, num_layers=1, batch_first=True\n            )\n        else:\n            self.rnn = nn.RNN(\n                input_size=36, hidden_size=36, num_layers=1, batch_first=True\n            )\n\n        self.layer_1 = nn.Linear(36, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2_s = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2_s.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2_a = nn.Linear(action_dim, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2_a.weight, nonlinearity=\"leaky_relu\")\n        self.layer_3 = nn.Linear(300, 1)\n        torch.nn.init.kaiming_uniform_(self.layer_3.weight, nonlinearity=\"leaky_relu\")\n\n        self.layer_4 = nn.Linear(36, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_5_s = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_5_s.weight, nonlinearity=\"leaky_relu\")\n        self.layer_5_a = nn.Linear(action_dim, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_5_a.weight, nonlinearity=\"leaky_relu\")\n        self.layer_6 = nn.Linear(300, 1)\n        torch.nn.init.kaiming_uniform_(self.layer_6.weight, nonlinearity=\"leaky_relu\")\n\n    def forward(self, s, action):\n        batch_n, hist_n, state_n = s.shape\n        s = s.reshape(batch_n * hist_n, state_n)\n\n        laser = s[:, :-5]\n        goal = s[:, -5:-2]\n        act = s[:, -2:]\n        laser = laser.unsqueeze(1)\n\n        l = F.leaky_relu(self.cnn1(laser))\n        l = F.leaky_relu(self.cnn2(l))\n        l = F.leaky_relu(self.cnn3(l))\n        l = l.flatten(start_dim=1)\n\n        g = F.leaky_relu(self.goal_embed(goal))\n\n        a = F.leaky_relu(self.action_embed(act))\n\n        s = torch.concat((l, g, a), dim=-1)\n\n        s = s.reshape(batch_n, hist_n, -1)\n        output, _ = self.rnn(s)\n        last_output = output[:, -1, :]\n\n        s1 = F.leaky_relu(self.layer_1(last_output))\n        self.layer_2_s(s1)\n        self.layer_2_a(action)\n        s11 = torch.mm(s1, self.layer_2_s.weight.data.t())\n        s12 = torch.mm(action, self.layer_2_a.weight.data.t())\n        s1 = F.leaky_relu(s11 + s12 + self.layer_2_a.bias.data)\n        q1 = self.layer_3(s1)\n\n        s2 = F.leaky_relu(self.layer_4(last_output))\n        self.layer_5_s(s2)\n        self.layer_5_a(action)\n        s21 = torch.mm(s2, self.layer_5_s.weight.data.t())\n        s22 = torch.mm(action, self.layer_5_a.weight.data.t())\n        s2 = F.leaky_relu(s21 + s22 + self.layer_5_a.bias.data)\n        q2 = self.layer_6(s2)\n        return q1, q2\n</code></pre>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG","title":"<code>RCPG</code>","text":"<p>               Bases: <code>object</code></p> <p>Recurrent Convolutional Policy Gradient (RCPG) agent for continuous control tasks.</p> <p>This class implements a recurrent actor-critic architecture using twin Q-networks and soft target updates. It includes model initialization, training, inference, saving/loading, and ROS-based state preparation.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG--parameters","title":"Parameters","text":"<p>state_dim : int     Dimensionality of the input state. action_dim : int     Dimensionality of the action space. max_action : float     Maximum allowable action value. device : torch.device     Device to run the model on (e.g., 'cuda' or 'cpu'). lr : float, optional     Learning rate for actor and critic optimizers. Default is 1e-4. save_every : int, optional     Frequency (in iterations) to save model checkpoints. Default is 0 (disabled). load_model : bool, optional     Whether to load pretrained model weights. Default is False. save_directory : Path, optional     Directory where models are saved. Default is \"robot_nav/models/RCPG/checkpoint\". model_name : str, optional     Name prefix for model checkpoint files. Default is \"RCPG\". load_directory : Path, optional     Directory to load pretrained models from. Default is \"robot_nav/models/RCPG/checkpoint\". rnn : str, optional     Type of RNN to use in networks (\"lstm\", \"gru\", or \"rnn\"). Default is \"gru\".</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>class RCPG(object):\n    \"\"\"\n    Recurrent Convolutional Policy Gradient (RCPG) agent for continuous control tasks.\n\n    This class implements a recurrent actor-critic architecture using twin Q-networks and soft target updates.\n    It includes model initialization, training, inference, saving/loading, and ROS-based state preparation.\n\n    Parameters\n    ----------\n    state_dim : int\n        Dimensionality of the input state.\n    action_dim : int\n        Dimensionality of the action space.\n    max_action : float\n        Maximum allowable action value.\n    device : torch.device\n        Device to run the model on (e.g., 'cuda' or 'cpu').\n    lr : float, optional\n        Learning rate for actor and critic optimizers. Default is 1e-4.\n    save_every : int, optional\n        Frequency (in iterations) to save model checkpoints. Default is 0 (disabled).\n    load_model : bool, optional\n        Whether to load pretrained model weights. Default is False.\n    save_directory : Path, optional\n        Directory where models are saved. Default is \"robot_nav/models/RCPG/checkpoint\".\n    model_name : str, optional\n        Name prefix for model checkpoint files. Default is \"RCPG\".\n    load_directory : Path, optional\n        Directory to load pretrained models from. Default is \"robot_nav/models/RCPG/checkpoint\".\n    rnn : str, optional\n        Type of RNN to use in networks (\"lstm\", \"gru\", or \"rnn\"). Default is \"gru\".\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        max_action,\n        device,\n        lr=1e-4,\n        save_every=0,\n        load_model=False,\n        save_directory=Path(\"robot_nav/models/RCPG/checkpoint\"),\n        model_name=\"RCPG\",\n        load_directory=Path(\"robot_nav/models/RCPG/checkpoint\"),\n        rnn=\"gru\",\n    ):\n        # Initialize the Actor network\n        self.device = device\n        self.actor = Actor(action_dim, rnn).to(self.device)\n        self.actor_target = Actor(action_dim, rnn).to(self.device)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = torch.optim.Adam(params=self.actor.parameters(), lr=lr)\n\n        # Initialize the Critic networks\n        self.critic = Critic(action_dim, rnn).to(self.device)\n        self.critic_target = Critic(action_dim, rnn).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = torch.optim.Adam(params=self.critic.parameters(), lr=lr)\n\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.state_dim = state_dim\n        self.writer = SummaryWriter(comment=model_name)\n        self.iter_count = 0\n        self.model_name = model_name + rnn\n        if load_model:\n            self.load(filename=self.model_name, directory=load_directory)\n        self.save_every = save_every\n        self.save_directory = save_directory\n\n    def get_action(self, obs, add_noise):\n        \"\"\"\n        Computes an action for the given observation, with optional exploration noise.\n\n        Parameters\n        ----------\n        obs : array_like\n            Input observation (state).\n        add_noise : bool\n            If True, adds Gaussian noise for exploration.\n\n        Returns\n        -------\n        np.ndarray\n            Action vector clipped to [-max_action, max_action].\n        \"\"\"\n        if add_noise:\n            return (\n                self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n            ).clip(-self.max_action, self.max_action)\n        else:\n            return self.act(obs)\n\n    def act(self, state):\n        \"\"\"\n        Returns the actor network's raw output for a given input state.\n\n        Parameters\n        ----------\n        state : array_like\n            State input.\n\n        Returns\n        -------\n        np.ndarray\n            Deterministic action vector from actor network.\n        \"\"\"\n        # Function to get the action from the actor\n        state = torch.Tensor(state).to(self.device)\n        return self.actor(state).cpu().data.numpy().flatten()\n\n    # training cycle\n    def train(\n        self,\n        replay_buffer,\n        iterations,\n        batch_size,\n        discount=0.99,\n        tau=0.005,\n        policy_noise=0.2,\n        noise_clip=0.5,\n        policy_freq=2,\n    ):\n        \"\"\"\n        Performs training over a number of iterations using batches from a replay buffer.\n\n        Parameters\n        ----------\n        replay_buffer : object\n            Experience replay buffer with a sample_batch method.\n        iterations : int\n            Number of training iterations.\n        batch_size : int\n            Size of each training batch.\n        discount : float, optional\n            Discount factor for future rewards (\u03b3). Default is 0.99.\n        tau : float, optional\n            Soft update parameter for target networks. Default is 0.005.\n        policy_noise : float, optional\n            Standard deviation of noise added to target actions. Default is 0.2.\n        noise_clip : float, optional\n            Range to clip the noise. Default is 0.5.\n        policy_freq : int, optional\n            Frequency of policy updates relative to critic updates. Default is 2.\n        \"\"\"\n        av_Q = 0\n        max_Q = -inf\n        av_loss = 0\n        for it in range(iterations):\n            # sample a batch from the replay buffer\n            (\n                batch_states,\n                batch_actions,\n                batch_rewards,\n                batch_dones,\n                batch_next_states,\n            ) = replay_buffer.sample_batch(batch_size)\n            state = torch.Tensor(batch_states).to(self.device)\n            next_state = torch.Tensor(batch_next_states).to(self.device)\n            action = torch.Tensor(batch_actions).to(self.device)\n            reward = torch.Tensor(batch_rewards).to(self.device)\n            done = torch.Tensor(batch_dones).to(self.device)\n\n            # Obtain the estimated action from the next state by using the actor-target\n            next_action = self.actor_target(next_state)\n\n            # Add noise to the action\n            noise = (\n                torch.Tensor(batch_actions)\n                .data.normal_(0, policy_noise)\n                .to(self.device)\n            )\n            noise = noise.clamp(-noise_clip, noise_clip)\n            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\n            # Calculate the Q values from the critic-target network for the next state-action pair\n            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n\n            # Select the minimal Q value from the 2 calculated values\n            target_Q = torch.min(target_Q1, target_Q2)\n            av_Q += torch.mean(target_Q)\n            max_Q = max(max_Q, torch.max(target_Q))\n            # Calculate the final Q value from the target network parameters by using Bellman equation\n            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n\n            # Get the Q values of the basis networks with the current parameters\n            current_Q1, current_Q2 = self.critic(state, action)\n\n            # Calculate the loss between the current Q value and the target Q value\n            loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\n            # Perform the gradient descent\n            self.critic_optimizer.zero_grad()\n            loss.backward()\n            self.critic_optimizer.step()\n\n            if it % policy_freq == 0:\n                # Maximize the actor output value by performing gradient descent on negative Q values\n                # (essentially perform gradient ascent)\n                actor_grad, _ = self.critic(state, self.actor(state))\n                actor_grad = -actor_grad.mean()\n                self.actor_optimizer.zero_grad()\n                actor_grad.backward()\n                self.actor_optimizer.step()\n\n                # Use soft update to update the actor-target network parameters by\n                # infusing small amount of current parameters\n                for param, target_param in zip(\n                    self.actor.parameters(), self.actor_target.parameters()\n                ):\n                    target_param.data.copy_(\n                        tau * param.data + (1 - tau) * target_param.data\n                    )\n                # Use soft update to update the critic-target network parameters by infusing\n                # small amount of current parameters\n                for param, target_param in zip(\n                    self.critic.parameters(), self.critic_target.parameters()\n                ):\n                    target_param.data.copy_(\n                        tau * param.data + (1 - tau) * target_param.data\n                    )\n\n            av_loss += loss\n        self.iter_count += 1\n        # Write new values for tensorboard\n        self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n        self.writer.add_scalar(\"train/avg_Q\", av_Q / iterations, self.iter_count)\n        self.writer.add_scalar(\"train/max_Q\", max_Q, self.iter_count)\n        if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n            self.save(filename=self.model_name, directory=self.save_directory)\n\n    def save(self, filename, directory):\n        \"\"\"\n        Saves actor and critic model weights to disk.\n\n        Parameters\n        ----------\n        filename : str\n            Base name for saved model files.\n        directory : str or Path\n            Target directory to save the models.\n        \"\"\"\n        Path(directory).mkdir(parents=True, exist_ok=True)\n        torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n        torch.save(\n            self.actor_target.state_dict(),\n            \"%s/%s_actor_target.pth\" % (directory, filename),\n        )\n        torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n        torch.save(\n            self.critic_target.state_dict(),\n            \"%s/%s_critic_target.pth\" % (directory, filename),\n        )\n\n    def load(self, filename, directory):\n        \"\"\"\n        Loads model weights for actor and critic networks from disk.\n\n        Parameters\n        ----------\n        filename : str\n            Base name of saved model files.\n        directory : str or Path\n            Directory from which to load model files.\n        \"\"\"\n        self.actor.load_state_dict(\n            torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n        )\n        self.actor_target.load_state_dict(\n            torch.load(\"%s/%s_actor_target.pth\" % (directory, filename))\n        )\n        self.critic.load_state_dict(\n            torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n        )\n        self.critic_target.load_state_dict(\n            torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n        )\n        print(f\"Loaded weights from: {directory}\")\n\n    def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n        \"\"\"\n        Converts raw sensor and environment data into a normalized input state vector.\n\n        Parameters\n        ----------\n        latest_scan : list or np.ndarray\n            Laser scan data.\n        distance : float\n            Distance to the goal.\n        cos : float\n            Cosine of the heading angle.\n        sin : float\n            Sine of the heading angle.\n        collision : bool\n            Whether a collision has occurred.\n        goal : bool\n            Whether the goal has been reached.\n        action : list or np.ndarray\n            Previous action taken [linear, angular].\n\n        Returns\n        -------\n        state : list\n            Normalized input state vector.\n        terminal : int\n            Terminal flag: 1 if goal reached or collision, otherwise 0.\n        \"\"\"\n        latest_scan = np.array(latest_scan)\n\n        inf_mask = np.isinf(latest_scan)\n        latest_scan[inf_mask] = 7.0\n        latest_scan /= 7\n\n        # Normalize to [0, 1] range\n        distance /= 10\n        lin_vel = action[0] * 2\n        ang_vel = (action[1] + 1) / 2\n        state = latest_scan.tolist() + [distance, cos, sin] + [lin_vel, ang_vel]\n\n        assert len(state) == self.state_dim\n        terminal = 1 if collision or goal else 0\n\n        return state, terminal\n</code></pre>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.act","title":"<code>act(state)</code>","text":"<p>Returns the actor network's raw output for a given input state.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.act--parameters","title":"Parameters","text":"<p>state : array_like     State input.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.act--returns","title":"Returns","text":"<p>np.ndarray     Deterministic action vector from actor network.</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>def act(self, state):\n    \"\"\"\n    Returns the actor network's raw output for a given input state.\n\n    Parameters\n    ----------\n    state : array_like\n        State input.\n\n    Returns\n    -------\n    np.ndarray\n        Deterministic action vector from actor network.\n    \"\"\"\n    # Function to get the action from the actor\n    state = torch.Tensor(state).to(self.device)\n    return self.actor(state).cpu().data.numpy().flatten()\n</code></pre>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.get_action","title":"<code>get_action(obs, add_noise)</code>","text":"<p>Computes an action for the given observation, with optional exploration noise.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.get_action--parameters","title":"Parameters","text":"<p>obs : array_like     Input observation (state). add_noise : bool     If True, adds Gaussian noise for exploration.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.get_action--returns","title":"Returns","text":"<p>np.ndarray     Action vector clipped to [-max_action, max_action].</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>def get_action(self, obs, add_noise):\n    \"\"\"\n    Computes an action for the given observation, with optional exploration noise.\n\n    Parameters\n    ----------\n    obs : array_like\n        Input observation (state).\n    add_noise : bool\n        If True, adds Gaussian noise for exploration.\n\n    Returns\n    -------\n    np.ndarray\n        Action vector clipped to [-max_action, max_action].\n    \"\"\"\n    if add_noise:\n        return (\n            self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n        ).clip(-self.max_action, self.max_action)\n    else:\n        return self.act(obs)\n</code></pre>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.load","title":"<code>load(filename, directory)</code>","text":"<p>Loads model weights for actor and critic networks from disk.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.load--parameters","title":"Parameters","text":"<p>filename : str     Base name of saved model files. directory : str or Path     Directory from which to load model files.</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>def load(self, filename, directory):\n    \"\"\"\n    Loads model weights for actor and critic networks from disk.\n\n    Parameters\n    ----------\n    filename : str\n        Base name of saved model files.\n    directory : str or Path\n        Directory from which to load model files.\n    \"\"\"\n    self.actor.load_state_dict(\n        torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n    )\n    self.actor_target.load_state_dict(\n        torch.load(\"%s/%s_actor_target.pth\" % (directory, filename))\n    )\n    self.critic.load_state_dict(\n        torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n    )\n    self.critic_target.load_state_dict(\n        torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n    )\n    print(f\"Loaded weights from: {directory}\")\n</code></pre>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.prepare_state","title":"<code>prepare_state(latest_scan, distance, cos, sin, collision, goal, action)</code>","text":"<p>Converts raw sensor and environment data into a normalized input state vector.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.prepare_state--parameters","title":"Parameters","text":"<p>latest_scan : list or np.ndarray     Laser scan data. distance : float     Distance to the goal. cos : float     Cosine of the heading angle. sin : float     Sine of the heading angle. collision : bool     Whether a collision has occurred. goal : bool     Whether the goal has been reached. action : list or np.ndarray     Previous action taken [linear, angular].</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.prepare_state--returns","title":"Returns","text":"<p>state : list     Normalized input state vector. terminal : int     Terminal flag: 1 if goal reached or collision, otherwise 0.</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n    \"\"\"\n    Converts raw sensor and environment data into a normalized input state vector.\n\n    Parameters\n    ----------\n    latest_scan : list or np.ndarray\n        Laser scan data.\n    distance : float\n        Distance to the goal.\n    cos : float\n        Cosine of the heading angle.\n    sin : float\n        Sine of the heading angle.\n    collision : bool\n        Whether a collision has occurred.\n    goal : bool\n        Whether the goal has been reached.\n    action : list or np.ndarray\n        Previous action taken [linear, angular].\n\n    Returns\n    -------\n    state : list\n        Normalized input state vector.\n    terminal : int\n        Terminal flag: 1 if goal reached or collision, otherwise 0.\n    \"\"\"\n    latest_scan = np.array(latest_scan)\n\n    inf_mask = np.isinf(latest_scan)\n    latest_scan[inf_mask] = 7.0\n    latest_scan /= 7\n\n    # Normalize to [0, 1] range\n    distance /= 10\n    lin_vel = action[0] * 2\n    ang_vel = (action[1] + 1) / 2\n    state = latest_scan.tolist() + [distance, cos, sin] + [lin_vel, ang_vel]\n\n    assert len(state) == self.state_dim\n    terminal = 1 if collision or goal else 0\n\n    return state, terminal\n</code></pre>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.save","title":"<code>save(filename, directory)</code>","text":"<p>Saves actor and critic model weights to disk.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.save--parameters","title":"Parameters","text":"<p>filename : str     Base name for saved model files. directory : str or Path     Target directory to save the models.</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>def save(self, filename, directory):\n    \"\"\"\n    Saves actor and critic model weights to disk.\n\n    Parameters\n    ----------\n    filename : str\n        Base name for saved model files.\n    directory : str or Path\n        Target directory to save the models.\n    \"\"\"\n    Path(directory).mkdir(parents=True, exist_ok=True)\n    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n    torch.save(\n        self.actor_target.state_dict(),\n        \"%s/%s_actor_target.pth\" % (directory, filename),\n    )\n    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n    torch.save(\n        self.critic_target.state_dict(),\n        \"%s/%s_critic_target.pth\" % (directory, filename),\n    )\n</code></pre>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.train","title":"<code>train(replay_buffer, iterations, batch_size, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2)</code>","text":"<p>Performs training over a number of iterations using batches from a replay buffer.</p>"},{"location":"api/models/RCPG/#robot_nav.models.RCPG.RCPG.RCPG.train--parameters","title":"Parameters","text":"<p>replay_buffer : object     Experience replay buffer with a sample_batch method. iterations : int     Number of training iterations. batch_size : int     Size of each training batch. discount : float, optional     Discount factor for future rewards (\u03b3). Default is 0.99. tau : float, optional     Soft update parameter for target networks. Default is 0.005. policy_noise : float, optional     Standard deviation of noise added to target actions. Default is 0.2. noise_clip : float, optional     Range to clip the noise. Default is 0.5. policy_freq : int, optional     Frequency of policy updates relative to critic updates. Default is 2.</p> Source code in <code>robot_nav/models/RCPG/RCPG.py</code> <pre><code>def train(\n    self,\n    replay_buffer,\n    iterations,\n    batch_size,\n    discount=0.99,\n    tau=0.005,\n    policy_noise=0.2,\n    noise_clip=0.5,\n    policy_freq=2,\n):\n    \"\"\"\n    Performs training over a number of iterations using batches from a replay buffer.\n\n    Parameters\n    ----------\n    replay_buffer : object\n        Experience replay buffer with a sample_batch method.\n    iterations : int\n        Number of training iterations.\n    batch_size : int\n        Size of each training batch.\n    discount : float, optional\n        Discount factor for future rewards (\u03b3). Default is 0.99.\n    tau : float, optional\n        Soft update parameter for target networks. Default is 0.005.\n    policy_noise : float, optional\n        Standard deviation of noise added to target actions. Default is 0.2.\n    noise_clip : float, optional\n        Range to clip the noise. Default is 0.5.\n    policy_freq : int, optional\n        Frequency of policy updates relative to critic updates. Default is 2.\n    \"\"\"\n    av_Q = 0\n    max_Q = -inf\n    av_loss = 0\n    for it in range(iterations):\n        # sample a batch from the replay buffer\n        (\n            batch_states,\n            batch_actions,\n            batch_rewards,\n            batch_dones,\n            batch_next_states,\n        ) = replay_buffer.sample_batch(batch_size)\n        state = torch.Tensor(batch_states).to(self.device)\n        next_state = torch.Tensor(batch_next_states).to(self.device)\n        action = torch.Tensor(batch_actions).to(self.device)\n        reward = torch.Tensor(batch_rewards).to(self.device)\n        done = torch.Tensor(batch_dones).to(self.device)\n\n        # Obtain the estimated action from the next state by using the actor-target\n        next_action = self.actor_target(next_state)\n\n        # Add noise to the action\n        noise = (\n            torch.Tensor(batch_actions)\n            .data.normal_(0, policy_noise)\n            .to(self.device)\n        )\n        noise = noise.clamp(-noise_clip, noise_clip)\n        next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\n        # Calculate the Q values from the critic-target network for the next state-action pair\n        target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n\n        # Select the minimal Q value from the 2 calculated values\n        target_Q = torch.min(target_Q1, target_Q2)\n        av_Q += torch.mean(target_Q)\n        max_Q = max(max_Q, torch.max(target_Q))\n        # Calculate the final Q value from the target network parameters by using Bellman equation\n        target_Q = reward + ((1 - done) * discount * target_Q).detach()\n\n        # Get the Q values of the basis networks with the current parameters\n        current_Q1, current_Q2 = self.critic(state, action)\n\n        # Calculate the loss between the current Q value and the target Q value\n        loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\n        # Perform the gradient descent\n        self.critic_optimizer.zero_grad()\n        loss.backward()\n        self.critic_optimizer.step()\n\n        if it % policy_freq == 0:\n            # Maximize the actor output value by performing gradient descent on negative Q values\n            # (essentially perform gradient ascent)\n            actor_grad, _ = self.critic(state, self.actor(state))\n            actor_grad = -actor_grad.mean()\n            self.actor_optimizer.zero_grad()\n            actor_grad.backward()\n            self.actor_optimizer.step()\n\n            # Use soft update to update the actor-target network parameters by\n            # infusing small amount of current parameters\n            for param, target_param in zip(\n                self.actor.parameters(), self.actor_target.parameters()\n            ):\n                target_param.data.copy_(\n                    tau * param.data + (1 - tau) * target_param.data\n                )\n            # Use soft update to update the critic-target network parameters by infusing\n            # small amount of current parameters\n            for param, target_param in zip(\n                self.critic.parameters(), self.critic_target.parameters()\n            ):\n                target_param.data.copy_(\n                    tau * param.data + (1 - tau) * target_param.data\n                )\n\n        av_loss += loss\n    self.iter_count += 1\n    # Write new values for tensorboard\n    self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n    self.writer.add_scalar(\"train/avg_Q\", av_Q / iterations, self.iter_count)\n    self.writer.add_scalar(\"train/max_Q\", max_Q, self.iter_count)\n    if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n        self.save(filename=self.model_name, directory=self.save_directory)\n</code></pre>"},{"location":"api/models/SAC/","title":"SAC","text":""},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC","title":"<code>robot_nav.models.SAC.SAC</code>","text":""},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC","title":"<code>SAC</code>","text":"<p>               Bases: <code>object</code></p> <p>Soft Actor-Critic (SAC) implementation.</p> <p>This class implements the SAC algorithm using a Gaussian policy actor and double Q-learning critic. It supports automatic entropy tuning, model saving/loading, and logging via TensorBoard.</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the observation/state space.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the action space.</p> required <code>device</code> <code>str</code> <p>PyTorch device (e.g., 'cpu' or 'cuda').</p> required <code>max_action</code> <code>float</code> <p>Maximum magnitude of actions.</p> required <code>discount</code> <code>float</code> <p>Discount factor for rewards.</p> <code>0.99</code> <code>init_temperature</code> <code>float</code> <p>Initial entropy temperature.</p> <code>0.1</code> <code>alpha_lr</code> <code>float</code> <p>Learning rate for entropy temperature alpha.</p> <code>0.0001</code> <code>alpha_betas</code> <code>tuple</code> <p>Adam optimizer betas for alpha.</p> <code>(0.9, 0.999)</code> <code>actor_lr</code> <code>float</code> <p>Learning rate for actor network.</p> <code>0.0001</code> <code>actor_betas</code> <code>tuple</code> <p>Adam optimizer betas for actor.</p> <code>(0.9, 0.999)</code> <code>actor_update_frequency</code> <code>int</code> <p>Frequency of actor updates.</p> <code>1</code> <code>critic_lr</code> <code>float</code> <p>Learning rate for critic network.</p> <code>0.0001</code> <code>critic_betas</code> <code>tuple</code> <p>Adam optimizer betas for critic.</p> <code>(0.9, 0.999)</code> <code>critic_tau</code> <code>float</code> <p>Soft update parameter for critic target.</p> <code>0.005</code> <code>critic_target_update_frequency</code> <code>int</code> <p>Frequency of critic target updates.</p> <code>2</code> <code>learnable_temperature</code> <code>bool</code> <p>Whether alpha is learnable.</p> <code>True</code> <code>save_every</code> <code>int</code> <p>Save model every N training steps. Set 0 to disable.</p> <code>0</code> <code>load_model</code> <code>bool</code> <p>Whether to load model from disk at init.</p> <code>False</code> <code>log_dist_and_hist</code> <code>bool</code> <p>Log distribution and histogram if True.</p> <code>False</code> <code>save_directory</code> <code>Path</code> <p>Directory to save models.</p> <code>Path('robot_nav/models/SAC/checkpoint')</code> <code>model_name</code> <code>str</code> <p>Name for model checkpoints.</p> <code>'SAC'</code> <code>load_directory</code> <code>Path</code> <p>Directory to load model checkpoints from.</p> <code>Path('robot_nav/models/SAC/checkpoint')</code> Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>class SAC(object):\n    \"\"\"\n    Soft Actor-Critic (SAC) implementation.\n\n    This class implements the SAC algorithm using a Gaussian policy actor and double Q-learning critic.\n    It supports automatic entropy tuning, model saving/loading, and logging via TensorBoard.\n\n    Args:\n        state_dim (int): Dimension of the observation/state space.\n        action_dim (int): Dimension of the action space.\n        device (str): PyTorch device (e.g., 'cpu' or 'cuda').\n        max_action (float): Maximum magnitude of actions.\n        discount (float): Discount factor for rewards.\n        init_temperature (float): Initial entropy temperature.\n        alpha_lr (float): Learning rate for entropy temperature alpha.\n        alpha_betas (tuple): Adam optimizer betas for alpha.\n        actor_lr (float): Learning rate for actor network.\n        actor_betas (tuple): Adam optimizer betas for actor.\n        actor_update_frequency (int): Frequency of actor updates.\n        critic_lr (float): Learning rate for critic network.\n        critic_betas (tuple): Adam optimizer betas for critic.\n        critic_tau (float): Soft update parameter for critic target.\n        critic_target_update_frequency (int): Frequency of critic target updates.\n        learnable_temperature (bool): Whether alpha is learnable.\n        save_every (int): Save model every N training steps. Set 0 to disable.\n        load_model (bool): Whether to load model from disk at init.\n        log_dist_and_hist (bool): Log distribution and histogram if True.\n        save_directory (Path): Directory to save models.\n        model_name (str): Name for model checkpoints.\n        load_directory (Path): Directory to load model checkpoints from.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        device,\n        max_action,\n        discount=0.99,\n        init_temperature=0.1,\n        alpha_lr=1e-4,\n        alpha_betas=(0.9, 0.999),\n        actor_lr=1e-4,\n        actor_betas=(0.9, 0.999),\n        actor_update_frequency=1,\n        critic_lr=1e-4,\n        critic_betas=(0.9, 0.999),\n        critic_tau=0.005,\n        critic_target_update_frequency=2,\n        learnable_temperature=True,\n        save_every=0,\n        load_model=False,\n        log_dist_and_hist=False,\n        save_directory=Path(\"robot_nav/models/SAC/checkpoint\"),\n        model_name=\"SAC\",\n        load_directory=Path(\"robot_nav/models/SAC/checkpoint\"),\n    ):\n        super().__init__()\n\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.action_range = (-max_action, max_action)\n        self.device = torch.device(device)\n        self.discount = discount\n        self.critic_tau = critic_tau\n        self.actor_update_frequency = actor_update_frequency\n        self.critic_target_update_frequency = critic_target_update_frequency\n        self.learnable_temperature = learnable_temperature\n        self.save_every = save_every\n        self.model_name = model_name\n        self.save_directory = save_directory\n        self.log_dist_and_hist = log_dist_and_hist\n\n        self.train_metrics_dict = {\n            \"train_critic/loss_av\": [],\n            \"train_actor/loss_av\": [],\n            \"train_actor/target_entropy_av\": [],\n            \"train_actor/entropy_av\": [],\n            \"train_alpha/loss_av\": [],\n            \"train_alpha/value_av\": [],\n            \"train/batch_reward_av\": [],\n        }\n\n        self.critic = critic_model(\n            obs_dim=self.state_dim,\n            action_dim=action_dim,\n            hidden_dim=400,\n            hidden_depth=2,\n        ).to(self.device)\n        self.critic_target = critic_model(\n            obs_dim=self.state_dim,\n            action_dim=action_dim,\n            hidden_dim=400,\n            hidden_depth=2,\n        ).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n        self.actor = actor_model(\n            obs_dim=self.state_dim,\n            action_dim=action_dim,\n            hidden_dim=400,\n            hidden_depth=2,\n            log_std_bounds=[-5, 2],\n        ).to(self.device)\n\n        if load_model:\n            self.load(filename=model_name, directory=load_directory)\n\n        self.log_alpha = torch.tensor(np.log(init_temperature)).to(self.device)\n        self.log_alpha.requires_grad = True\n        # set target entropy to -|A|\n        self.target_entropy = -action_dim\n\n        # optimizers\n        self.actor_optimizer = torch.optim.Adam(\n            self.actor.parameters(), lr=actor_lr, betas=actor_betas\n        )\n\n        self.critic_optimizer = torch.optim.Adam(\n            self.critic.parameters(), lr=critic_lr, betas=critic_betas\n        )\n\n        self.log_alpha_optimizer = torch.optim.Adam(\n            [self.log_alpha], lr=alpha_lr, betas=alpha_betas\n        )\n\n        self.critic_target.train()\n\n        self.actor.train(True)\n        self.critic.train(True)\n        self.step = 0\n        self.writer = SummaryWriter(comment=model_name)\n\n    def save(self, filename, directory):\n        \"\"\"\n        Save the actor, critic, and target critic models to the specified directory.\n\n        Args:\n            filename (str): Base name of the saved files.\n            directory (Path): Directory where models are saved.\n        \"\"\"\n        Path(directory).mkdir(parents=True, exist_ok=True)\n        torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n        torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n        torch.save(\n            self.critic_target.state_dict(),\n            \"%s/%s_critic_target.pth\" % (directory, filename),\n        )\n\n    def load(self, filename, directory):\n        \"\"\"\n        Load the actor, critic, and target critic models from the specified directory.\n\n        Args:\n            filename (str): Base name of the saved files.\n            directory (Path): Directory where models are loaded from.\n        \"\"\"\n        self.actor.load_state_dict(\n            torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n        )\n        self.critic.load_state_dict(\n            torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n        )\n        self.critic_target.load_state_dict(\n            torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n        )\n        print(f\"Loaded weights from: {directory}\")\n\n    def train(self, replay_buffer, iterations, batch_size):\n        \"\"\"\n        Run multiple training updates using data from the replay buffer.\n\n        Args:\n            replay_buffer: Buffer from which to sample training data.\n            iterations (int): Number of training iterations to run.\n            batch_size (int): Batch size for each update.\n        \"\"\"\n        for _ in range(iterations):\n            self.update(\n                replay_buffer=replay_buffer, step=self.step, batch_size=batch_size\n            )\n\n        for key, value in self.train_metrics_dict.items():\n            if len(value):\n                self.writer.add_scalar(key, mean(value), self.step)\n            self.train_metrics_dict[key] = []\n        self.step += 1\n\n        if self.save_every &gt; 0 and self.step % self.save_every == 0:\n            self.save(filename=self.model_name, directory=self.save_directory)\n\n    @property\n    def alpha(self):\n        \"\"\"\n        Returns:\n            torch.Tensor: Current value of the entropy temperature alpha.\n        \"\"\"\n        return self.log_alpha.exp()\n\n    def get_action(self, obs, add_noise):\n        \"\"\"\n        Select an action given an observation.\n\n        Args:\n            obs (np.ndarray): Input observation.\n            add_noise (bool): Whether to add exploration noise.\n\n        Returns:\n            np.ndarray: Action vector.\n        \"\"\"\n        if add_noise:\n            return (\n                self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n            ).clip(self.action_range[0], self.action_range[1])\n        else:\n            return self.act(obs)\n\n    def act(self, obs, sample=False):\n        \"\"\"\n        Generate an action from the actor network.\n\n        Args:\n            obs (np.ndarray): Input observation.\n            sample (bool): If True, sample from the policy; otherwise use the mean.\n\n        Returns:\n            np.ndarray: Action vector.\n        \"\"\"\n        obs = torch.FloatTensor(obs).to(self.device)\n        obs = obs.unsqueeze(0)\n        dist = self.actor(obs)\n        action = dist.sample() if sample else dist.mean\n        action = action.clamp(*self.action_range)\n        assert action.ndim == 2 and action.shape[0] == 1\n        return utils.to_np(action[0])\n\n    def update_critic(self, obs, action, reward, next_obs, done, step):\n        \"\"\"\n        Update the critic network based on a batch of transitions.\n\n        Args:\n            obs (torch.Tensor): Batch of current observations.\n            action (torch.Tensor): Batch of actions taken.\n            reward (torch.Tensor): Batch of received rewards.\n            next_obs (torch.Tensor): Batch of next observations.\n            done (torch.Tensor): Batch of done flags.\n            step (int): Current training step (for logging).\n        \"\"\"\n        dist = self.actor(next_obs)\n        next_action = dist.rsample()\n        log_prob = dist.log_prob(next_action).sum(-1, keepdim=True)\n        target_Q1, target_Q2 = self.critic_target(next_obs, next_action)\n        target_V = torch.min(target_Q1, target_Q2) - self.alpha.detach() * log_prob\n        target_Q = reward + ((1 - done) * self.discount * target_V)\n        target_Q = target_Q.detach()\n\n        # get current Q estimates\n        current_Q1, current_Q2 = self.critic(obs, action)\n        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(\n            current_Q2, target_Q\n        )\n        self.train_metrics_dict[\"train_critic/loss_av\"].append(critic_loss.item())\n        self.writer.add_scalar(\"train_critic/loss\", critic_loss, step)\n\n        # Optimize the critic\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n        if self.log_dist_and_hist:\n            self.critic.log(self.writer, step)\n\n    def update_actor_and_alpha(self, obs, step):\n        \"\"\"\n        Update the actor and optionally the entropy temperature.\n\n        Args:\n            obs (torch.Tensor): Batch of observations.\n            step (int): Current training step (for logging).\n        \"\"\"\n        dist = self.actor(obs)\n        action = dist.rsample()\n        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n        actor_Q1, actor_Q2 = self.critic(obs, action)\n\n        actor_Q = torch.min(actor_Q1, actor_Q2)\n        actor_loss = (self.alpha.detach() * log_prob - actor_Q).mean()\n        self.train_metrics_dict[\"train_actor/loss_av\"].append(actor_loss.item())\n        self.train_metrics_dict[\"train_actor/target_entropy_av\"].append(\n            self.target_entropy\n        )\n        self.train_metrics_dict[\"train_actor/entropy_av\"].append(\n            -log_prob.mean().item()\n        )\n        self.writer.add_scalar(\"train_actor/loss\", actor_loss, step)\n        self.writer.add_scalar(\"train_actor/target_entropy\", self.target_entropy, step)\n        self.writer.add_scalar(\"train_actor/entropy\", -log_prob.mean(), step)\n\n        # optimize the actor\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n        if self.log_dist_and_hist:\n            self.actor.log(self.writer, step)\n\n        if self.learnable_temperature:\n            self.log_alpha_optimizer.zero_grad()\n            alpha_loss = (\n                self.alpha * (-log_prob - self.target_entropy).detach()\n            ).mean()\n            self.train_metrics_dict[\"train_alpha/loss_av\"].append(alpha_loss.item())\n            self.train_metrics_dict[\"train_alpha/value_av\"].append(self.alpha.item())\n            self.writer.add_scalar(\"train_alpha/loss\", alpha_loss, step)\n            self.writer.add_scalar(\"train_alpha/value\", self.alpha, step)\n            alpha_loss.backward()\n            self.log_alpha_optimizer.step()\n\n    def update(self, replay_buffer, step, batch_size):\n        \"\"\"\n        Perform a full update step (critic, actor, alpha, target critic).\n\n        Args:\n            replay_buffer: Buffer to sample from.\n            step (int): Current training step.\n            batch_size (int): Size of sample batch.\n        \"\"\"\n        (\n            batch_states,\n            batch_actions,\n            batch_rewards,\n            batch_dones,\n            batch_next_states,\n        ) = replay_buffer.sample_batch(batch_size)\n\n        state = torch.Tensor(batch_states).to(self.device)\n        next_state = torch.Tensor(batch_next_states).to(self.device)\n        action = torch.Tensor(batch_actions).to(self.device)\n        reward = torch.Tensor(batch_rewards).to(self.device)\n        done = torch.Tensor(batch_dones).to(self.device)\n        self.train_metrics_dict[\"train/batch_reward_av\"].append(\n            batch_rewards.mean().item()\n        )\n        self.writer.add_scalar(\"train/batch_reward\", batch_rewards.mean(), step)\n\n        self.update_critic(state, action, reward, next_state, done, step)\n\n        if step % self.actor_update_frequency == 0:\n            self.update_actor_and_alpha(state, step)\n\n        if step % self.critic_target_update_frequency == 0:\n            utils.soft_update_params(self.critic, self.critic_target, self.critic_tau)\n\n    def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n        \"\"\"\n        Convert raw sensor input into a normalized state vector.\n\n        Args:\n            latest_scan (list or np.ndarray): Laser scan distances.\n            distance (float): Distance to goal.\n            cos (float): Cosine of heading angle to goal.\n            sin (float): Sine of heading angle to goal.\n            collision (bool): Whether the robot has collided.\n            goal (bool): Whether the goal has been reached.\n            action (list): Last action taken [linear_vel, angular_vel].\n\n        Returns:\n            tuple: (state vector as list, terminal flag as int)\n        \"\"\"\n        latest_scan = np.array(latest_scan)\n\n        inf_mask = np.isinf(latest_scan)\n        latest_scan[inf_mask] = 7.0\n\n        max_bins = self.state_dim - 5\n        bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n        # Initialize the list to store the minimum values of each bin\n        min_values = []\n\n        # Loop through the data and create bins\n        for i in range(0, len(latest_scan), bin_size):\n            # Get the current bin\n            bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n            # Find the minimum value in the current bin and append it to the min_values list\n            min_values.append(min(bin) / 7)\n\n        # Normalize to [0, 1] range\n        distance /= 10\n        lin_vel = action[0] * 2\n        ang_vel = (action[1] + 1) / 2\n        state = min_values + [distance, cos, sin] + [lin_vel, ang_vel]\n\n        assert len(state) == self.state_dim\n        terminal = 1 if collision or goal else 0\n\n        return state, terminal\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.alpha","title":"<code>alpha</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>torch.Tensor: Current value of the entropy temperature alpha.</p>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.act","title":"<code>act(obs, sample=False)</code>","text":"<p>Generate an action from the actor network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>ndarray</code> <p>Input observation.</p> required <code>sample</code> <code>bool</code> <p>If True, sample from the policy; otherwise use the mean.</p> <code>False</code> <p>Returns:</p> Type Description <p>np.ndarray: Action vector.</p> Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def act(self, obs, sample=False):\n    \"\"\"\n    Generate an action from the actor network.\n\n    Args:\n        obs (np.ndarray): Input observation.\n        sample (bool): If True, sample from the policy; otherwise use the mean.\n\n    Returns:\n        np.ndarray: Action vector.\n    \"\"\"\n    obs = torch.FloatTensor(obs).to(self.device)\n    obs = obs.unsqueeze(0)\n    dist = self.actor(obs)\n    action = dist.sample() if sample else dist.mean\n    action = action.clamp(*self.action_range)\n    assert action.ndim == 2 and action.shape[0] == 1\n    return utils.to_np(action[0])\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.get_action","title":"<code>get_action(obs, add_noise)</code>","text":"<p>Select an action given an observation.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>ndarray</code> <p>Input observation.</p> required <code>add_noise</code> <code>bool</code> <p>Whether to add exploration noise.</p> required <p>Returns:</p> Type Description <p>np.ndarray: Action vector.</p> Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def get_action(self, obs, add_noise):\n    \"\"\"\n    Select an action given an observation.\n\n    Args:\n        obs (np.ndarray): Input observation.\n        add_noise (bool): Whether to add exploration noise.\n\n    Returns:\n        np.ndarray: Action vector.\n    \"\"\"\n    if add_noise:\n        return (\n            self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n        ).clip(self.action_range[0], self.action_range[1])\n    else:\n        return self.act(obs)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.load","title":"<code>load(filename, directory)</code>","text":"<p>Load the actor, critic, and target critic models from the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base name of the saved files.</p> required <code>directory</code> <code>Path</code> <p>Directory where models are loaded from.</p> required Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def load(self, filename, directory):\n    \"\"\"\n    Load the actor, critic, and target critic models from the specified directory.\n\n    Args:\n        filename (str): Base name of the saved files.\n        directory (Path): Directory where models are loaded from.\n    \"\"\"\n    self.actor.load_state_dict(\n        torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n    )\n    self.critic.load_state_dict(\n        torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n    )\n    self.critic_target.load_state_dict(\n        torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n    )\n    print(f\"Loaded weights from: {directory}\")\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.prepare_state","title":"<code>prepare_state(latest_scan, distance, cos, sin, collision, goal, action)</code>","text":"<p>Convert raw sensor input into a normalized state vector.</p> <p>Parameters:</p> Name Type Description Default <code>latest_scan</code> <code>list or ndarray</code> <p>Laser scan distances.</p> required <code>distance</code> <code>float</code> <p>Distance to goal.</p> required <code>cos</code> <code>float</code> <p>Cosine of heading angle to goal.</p> required <code>sin</code> <code>float</code> <p>Sine of heading angle to goal.</p> required <code>collision</code> <code>bool</code> <p>Whether the robot has collided.</p> required <code>goal</code> <code>bool</code> <p>Whether the goal has been reached.</p> required <code>action</code> <code>list</code> <p>Last action taken [linear_vel, angular_vel].</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(state vector as list, terminal flag as int)</p> Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n    \"\"\"\n    Convert raw sensor input into a normalized state vector.\n\n    Args:\n        latest_scan (list or np.ndarray): Laser scan distances.\n        distance (float): Distance to goal.\n        cos (float): Cosine of heading angle to goal.\n        sin (float): Sine of heading angle to goal.\n        collision (bool): Whether the robot has collided.\n        goal (bool): Whether the goal has been reached.\n        action (list): Last action taken [linear_vel, angular_vel].\n\n    Returns:\n        tuple: (state vector as list, terminal flag as int)\n    \"\"\"\n    latest_scan = np.array(latest_scan)\n\n    inf_mask = np.isinf(latest_scan)\n    latest_scan[inf_mask] = 7.0\n\n    max_bins = self.state_dim - 5\n    bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n    # Initialize the list to store the minimum values of each bin\n    min_values = []\n\n    # Loop through the data and create bins\n    for i in range(0, len(latest_scan), bin_size):\n        # Get the current bin\n        bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n        # Find the minimum value in the current bin and append it to the min_values list\n        min_values.append(min(bin) / 7)\n\n    # Normalize to [0, 1] range\n    distance /= 10\n    lin_vel = action[0] * 2\n    ang_vel = (action[1] + 1) / 2\n    state = min_values + [distance, cos, sin] + [lin_vel, ang_vel]\n\n    assert len(state) == self.state_dim\n    terminal = 1 if collision or goal else 0\n\n    return state, terminal\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.save","title":"<code>save(filename, directory)</code>","text":"<p>Save the actor, critic, and target critic models to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base name of the saved files.</p> required <code>directory</code> <code>Path</code> <p>Directory where models are saved.</p> required Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def save(self, filename, directory):\n    \"\"\"\n    Save the actor, critic, and target critic models to the specified directory.\n\n    Args:\n        filename (str): Base name of the saved files.\n        directory (Path): Directory where models are saved.\n    \"\"\"\n    Path(directory).mkdir(parents=True, exist_ok=True)\n    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n    torch.save(\n        self.critic_target.state_dict(),\n        \"%s/%s_critic_target.pth\" % (directory, filename),\n    )\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.train","title":"<code>train(replay_buffer, iterations, batch_size)</code>","text":"<p>Run multiple training updates using data from the replay buffer.</p> <p>Parameters:</p> Name Type Description Default <code>replay_buffer</code> <p>Buffer from which to sample training data.</p> required <code>iterations</code> <code>int</code> <p>Number of training iterations to run.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for each update.</p> required Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def train(self, replay_buffer, iterations, batch_size):\n    \"\"\"\n    Run multiple training updates using data from the replay buffer.\n\n    Args:\n        replay_buffer: Buffer from which to sample training data.\n        iterations (int): Number of training iterations to run.\n        batch_size (int): Batch size for each update.\n    \"\"\"\n    for _ in range(iterations):\n        self.update(\n            replay_buffer=replay_buffer, step=self.step, batch_size=batch_size\n        )\n\n    for key, value in self.train_metrics_dict.items():\n        if len(value):\n            self.writer.add_scalar(key, mean(value), self.step)\n        self.train_metrics_dict[key] = []\n    self.step += 1\n\n    if self.save_every &gt; 0 and self.step % self.save_every == 0:\n        self.save(filename=self.model_name, directory=self.save_directory)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.update","title":"<code>update(replay_buffer, step, batch_size)</code>","text":"<p>Perform a full update step (critic, actor, alpha, target critic).</p> <p>Parameters:</p> Name Type Description Default <code>replay_buffer</code> <p>Buffer to sample from.</p> required <code>step</code> <code>int</code> <p>Current training step.</p> required <code>batch_size</code> <code>int</code> <p>Size of sample batch.</p> required Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def update(self, replay_buffer, step, batch_size):\n    \"\"\"\n    Perform a full update step (critic, actor, alpha, target critic).\n\n    Args:\n        replay_buffer: Buffer to sample from.\n        step (int): Current training step.\n        batch_size (int): Size of sample batch.\n    \"\"\"\n    (\n        batch_states,\n        batch_actions,\n        batch_rewards,\n        batch_dones,\n        batch_next_states,\n    ) = replay_buffer.sample_batch(batch_size)\n\n    state = torch.Tensor(batch_states).to(self.device)\n    next_state = torch.Tensor(batch_next_states).to(self.device)\n    action = torch.Tensor(batch_actions).to(self.device)\n    reward = torch.Tensor(batch_rewards).to(self.device)\n    done = torch.Tensor(batch_dones).to(self.device)\n    self.train_metrics_dict[\"train/batch_reward_av\"].append(\n        batch_rewards.mean().item()\n    )\n    self.writer.add_scalar(\"train/batch_reward\", batch_rewards.mean(), step)\n\n    self.update_critic(state, action, reward, next_state, done, step)\n\n    if step % self.actor_update_frequency == 0:\n        self.update_actor_and_alpha(state, step)\n\n    if step % self.critic_target_update_frequency == 0:\n        utils.soft_update_params(self.critic, self.critic_target, self.critic_tau)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.update_actor_and_alpha","title":"<code>update_actor_and_alpha(obs, step)</code>","text":"<p>Update the actor and optionally the entropy temperature.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Tensor</code> <p>Batch of observations.</p> required <code>step</code> <code>int</code> <p>Current training step (for logging).</p> required Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def update_actor_and_alpha(self, obs, step):\n    \"\"\"\n    Update the actor and optionally the entropy temperature.\n\n    Args:\n        obs (torch.Tensor): Batch of observations.\n        step (int): Current training step (for logging).\n    \"\"\"\n    dist = self.actor(obs)\n    action = dist.rsample()\n    log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n    actor_Q1, actor_Q2 = self.critic(obs, action)\n\n    actor_Q = torch.min(actor_Q1, actor_Q2)\n    actor_loss = (self.alpha.detach() * log_prob - actor_Q).mean()\n    self.train_metrics_dict[\"train_actor/loss_av\"].append(actor_loss.item())\n    self.train_metrics_dict[\"train_actor/target_entropy_av\"].append(\n        self.target_entropy\n    )\n    self.train_metrics_dict[\"train_actor/entropy_av\"].append(\n        -log_prob.mean().item()\n    )\n    self.writer.add_scalar(\"train_actor/loss\", actor_loss, step)\n    self.writer.add_scalar(\"train_actor/target_entropy\", self.target_entropy, step)\n    self.writer.add_scalar(\"train_actor/entropy\", -log_prob.mean(), step)\n\n    # optimize the actor\n    self.actor_optimizer.zero_grad()\n    actor_loss.backward()\n    self.actor_optimizer.step()\n    if self.log_dist_and_hist:\n        self.actor.log(self.writer, step)\n\n    if self.learnable_temperature:\n        self.log_alpha_optimizer.zero_grad()\n        alpha_loss = (\n            self.alpha * (-log_prob - self.target_entropy).detach()\n        ).mean()\n        self.train_metrics_dict[\"train_alpha/loss_av\"].append(alpha_loss.item())\n        self.train_metrics_dict[\"train_alpha/value_av\"].append(self.alpha.item())\n        self.writer.add_scalar(\"train_alpha/loss\", alpha_loss, step)\n        self.writer.add_scalar(\"train_alpha/value\", self.alpha, step)\n        alpha_loss.backward()\n        self.log_alpha_optimizer.step()\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC.SAC.update_critic","title":"<code>update_critic(obs, action, reward, next_obs, done, step)</code>","text":"<p>Update the critic network based on a batch of transitions.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Tensor</code> <p>Batch of current observations.</p> required <code>action</code> <code>Tensor</code> <p>Batch of actions taken.</p> required <code>reward</code> <code>Tensor</code> <p>Batch of received rewards.</p> required <code>next_obs</code> <code>Tensor</code> <p>Batch of next observations.</p> required <code>done</code> <code>Tensor</code> <p>Batch of done flags.</p> required <code>step</code> <code>int</code> <p>Current training step (for logging).</p> required Source code in <code>robot_nav/models/SAC/SAC.py</code> <pre><code>def update_critic(self, obs, action, reward, next_obs, done, step):\n    \"\"\"\n    Update the critic network based on a batch of transitions.\n\n    Args:\n        obs (torch.Tensor): Batch of current observations.\n        action (torch.Tensor): Batch of actions taken.\n        reward (torch.Tensor): Batch of received rewards.\n        next_obs (torch.Tensor): Batch of next observations.\n        done (torch.Tensor): Batch of done flags.\n        step (int): Current training step (for logging).\n    \"\"\"\n    dist = self.actor(next_obs)\n    next_action = dist.rsample()\n    log_prob = dist.log_prob(next_action).sum(-1, keepdim=True)\n    target_Q1, target_Q2 = self.critic_target(next_obs, next_action)\n    target_V = torch.min(target_Q1, target_Q2) - self.alpha.detach() * log_prob\n    target_Q = reward + ((1 - done) * self.discount * target_V)\n    target_Q = target_Q.detach()\n\n    # get current Q estimates\n    current_Q1, current_Q2 = self.critic(obs, action)\n    critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(\n        current_Q2, target_Q\n    )\n    self.train_metrics_dict[\"train_critic/loss_av\"].append(critic_loss.item())\n    self.writer.add_scalar(\"train_critic/loss\", critic_loss, step)\n\n    # Optimize the critic\n    self.critic_optimizer.zero_grad()\n    critic_loss.backward()\n    self.critic_optimizer.step()\n    if self.log_dist_and_hist:\n        self.critic.log(self.writer, step)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor","title":"<code>robot_nav.models.SAC.SAC_actor</code>","text":""},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.DiagGaussianActor","title":"<code>DiagGaussianActor</code>","text":"<p>               Bases: <code>Module</code></p> <p>Diagonal Gaussian policy network with tanh squashing.</p> <p>This network outputs a squashed Gaussian distribution given an observation, suitable for continuous control tasks.</p> <p>Parameters:</p> Name Type Description Default <code>obs_dim</code> <code>int</code> <p>Dimension of the observation space.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the action space.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of units in hidden layers.</p> required <code>hidden_depth</code> <code>int</code> <p>Number of hidden layers.</p> required <code>log_std_bounds</code> <code>list</code> <p>Min and max bounds for log standard deviation.</p> required Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>class DiagGaussianActor(nn.Module):\n    \"\"\"\n    Diagonal Gaussian policy network with tanh squashing.\n\n    This network outputs a squashed Gaussian distribution given an observation,\n    suitable for continuous control tasks.\n\n    Args:\n        obs_dim (int): Dimension of the observation space.\n        action_dim (int): Dimension of the action space.\n        hidden_dim (int): Number of units in hidden layers.\n        hidden_depth (int): Number of hidden layers.\n        log_std_bounds (list): Min and max bounds for log standard deviation.\n    \"\"\"\n\n    def __init__(self, obs_dim, action_dim, hidden_dim, hidden_depth, log_std_bounds):\n        \"\"\"\n        Initialize the actor network.\n        \"\"\"\n        super().__init__()\n\n        self.log_std_bounds = log_std_bounds\n        self.trunk = utils.mlp(obs_dim, hidden_dim, 2 * action_dim, hidden_depth)\n\n        self.outputs = dict()\n        self.apply(utils.weight_init)\n\n    def forward(self, obs):\n        \"\"\"\n        Forward pass through the network.\n\n        Args:\n            obs (Tensor): Observation input.\n\n        Returns:\n            SquashedNormal: Action distribution with mean and std tracked in `self.outputs`.\n        \"\"\"\n        mu, log_std = self.trunk(obs).chunk(2, dim=-1)\n\n        # constrain log_std inside [log_std_min, log_std_max]\n        log_std = torch.tanh(log_std)\n        log_std_min, log_std_max = self.log_std_bounds\n        log_std = log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std + 1)\n\n        std = log_std.exp()\n\n        self.outputs[\"mu\"] = mu\n        self.outputs[\"std\"] = std\n\n        dist = SquashedNormal(mu, std)\n        return dist\n\n    def log(self, writer, step):\n        \"\"\"\n        Log network outputs (mu and std histograms) to TensorBoard.\n\n        Args:\n            writer (SummaryWriter): TensorBoard writer instance.\n            step (int): Current global training step.\n        \"\"\"\n        for k, v in self.outputs.items():\n            writer.add_histogram(f\"train_actor/{k}_hist\", v, step)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.DiagGaussianActor.__init__","title":"<code>__init__(obs_dim, action_dim, hidden_dim, hidden_depth, log_std_bounds)</code>","text":"<p>Initialize the actor network.</p> Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>def __init__(self, obs_dim, action_dim, hidden_dim, hidden_depth, log_std_bounds):\n    \"\"\"\n    Initialize the actor network.\n    \"\"\"\n    super().__init__()\n\n    self.log_std_bounds = log_std_bounds\n    self.trunk = utils.mlp(obs_dim, hidden_dim, 2 * action_dim, hidden_depth)\n\n    self.outputs = dict()\n    self.apply(utils.weight_init)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.DiagGaussianActor.forward","title":"<code>forward(obs)</code>","text":"<p>Forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Tensor</code> <p>Observation input.</p> required <p>Returns:</p> Name Type Description <code>SquashedNormal</code> <p>Action distribution with mean and std tracked in <code>self.outputs</code>.</p> Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>def forward(self, obs):\n    \"\"\"\n    Forward pass through the network.\n\n    Args:\n        obs (Tensor): Observation input.\n\n    Returns:\n        SquashedNormal: Action distribution with mean and std tracked in `self.outputs`.\n    \"\"\"\n    mu, log_std = self.trunk(obs).chunk(2, dim=-1)\n\n    # constrain log_std inside [log_std_min, log_std_max]\n    log_std = torch.tanh(log_std)\n    log_std_min, log_std_max = self.log_std_bounds\n    log_std = log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std + 1)\n\n    std = log_std.exp()\n\n    self.outputs[\"mu\"] = mu\n    self.outputs[\"std\"] = std\n\n    dist = SquashedNormal(mu, std)\n    return dist\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.DiagGaussianActor.log","title":"<code>log(writer, step)</code>","text":"<p>Log network outputs (mu and std histograms) to TensorBoard.</p> <p>Parameters:</p> Name Type Description Default <code>writer</code> <code>SummaryWriter</code> <p>TensorBoard writer instance.</p> required <code>step</code> <code>int</code> <p>Current global training step.</p> required Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>def log(self, writer, step):\n    \"\"\"\n    Log network outputs (mu and std histograms) to TensorBoard.\n\n    Args:\n        writer (SummaryWriter): TensorBoard writer instance.\n        step (int): Current global training step.\n    \"\"\"\n    for k, v in self.outputs.items():\n        writer.add_histogram(f\"train_actor/{k}_hist\", v, step)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.SquashedNormal","title":"<code>SquashedNormal</code>","text":"<p>               Bases: <code>TransformedDistribution</code></p> <p>A squashed (tanh-transformed) diagonal Gaussian distribution.</p> <p>This is used for stochastic policies where actions must be within bounded intervals.</p> Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>class SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n    \"\"\"\n    A squashed (tanh-transformed) diagonal Gaussian distribution.\n\n    This is used for stochastic policies where actions must be within bounded intervals.\n    \"\"\"\n\n    def __init__(self, loc, scale):\n        \"\"\"\n        Initialize the squashed normal distribution.\n\n        Args:\n            loc (Tensor): Mean of the Gaussian.\n            scale (Tensor): Standard deviation of the Gaussian.\n        \"\"\"\n        self.loc = loc\n        self.scale = scale\n\n        self.base_dist = pyd.Normal(loc, scale)\n        transforms = [TanhTransform()]\n        super().__init__(self.base_dist, transforms)\n\n    @property\n    def mean(self):\n        \"\"\"\n        Compute the mean of the transformed distribution.\n\n        Returns:\n            Tensor: Mean of the squashed distribution.\n        \"\"\"\n        mu = self.loc\n        for tr in self.transforms:\n            mu = tr(mu)\n        return mu\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.SquashedNormal.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Compute the mean of the transformed distribution.</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Mean of the squashed distribution.</p>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.SquashedNormal.__init__","title":"<code>__init__(loc, scale)</code>","text":"<p>Initialize the squashed normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>Tensor</code> <p>Mean of the Gaussian.</p> required <code>scale</code> <code>Tensor</code> <p>Standard deviation of the Gaussian.</p> required Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>def __init__(self, loc, scale):\n    \"\"\"\n    Initialize the squashed normal distribution.\n\n    Args:\n        loc (Tensor): Mean of the Gaussian.\n        scale (Tensor): Standard deviation of the Gaussian.\n    \"\"\"\n    self.loc = loc\n    self.scale = scale\n\n    self.base_dist = pyd.Normal(loc, scale)\n    transforms = [TanhTransform()]\n    super().__init__(self.base_dist, transforms)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.TanhTransform","title":"<code>TanhTransform</code>","text":"<p>               Bases: <code>Transform</code></p> <p>A bijective transformation that applies the hyperbolic tangent function.</p> <p>This is used to squash the output of a normal distribution to be within [-1, 1], making it suitable for bounded continuous action spaces.</p> <p>Attributes:</p> Name Type Description <code>domain</code> <p>The input domain (real numbers).</p> <code>codomain</code> <p>The output codomain (interval between -1 and 1).</p> <code>bijective</code> <p>Whether the transform is bijective (True).</p> <code>sign</code> <p>The sign of the Jacobian determinant (positive).</p> Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>class TanhTransform(pyd.transforms.Transform):\n    \"\"\"\n    A bijective transformation that applies the hyperbolic tangent function.\n\n    This is used to squash the output of a normal distribution to be within [-1, 1],\n    making it suitable for bounded continuous action spaces.\n\n    Attributes:\n        domain: The input domain (real numbers).\n        codomain: The output codomain (interval between -1 and 1).\n        bijective: Whether the transform is bijective (True).\n        sign: The sign of the Jacobian determinant (positive).\n    \"\"\"\n\n    domain = pyd.constraints.real\n    codomain = pyd.constraints.interval(-1.0, 1.0)\n    bijective = True\n    sign = +1\n\n    def __init__(self, cache_size=1):\n        \"\"\"\n        Initialize the TanhTransform.\n\n        Args:\n            cache_size (int): Size of the cache for storing intermediate values.\n        \"\"\"\n        super().__init__(cache_size=cache_size)\n\n    @staticmethod\n    def atanh(x):\n        \"\"\"\n        Inverse hyperbolic tangent function.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            Tensor: atanh(x)\n        \"\"\"\n        return 0.5 * (x.log1p() - (-x).log1p())\n\n    def __eq__(self, other):\n        \"\"\"\n        Equality check for the transform.\n\n        Returns:\n            bool: True if the other object is also a TanhTransform.\n        \"\"\"\n        return isinstance(other, TanhTransform)\n\n    def _call(self, x):\n        \"\"\"\n        Forward transformation.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            Tensor: tanh(x)\n        \"\"\"\n        return x.tanh()\n\n    def _inverse(self, y):\n        \"\"\"\n        Inverse transformation.\n\n        Args:\n            y (Tensor): Input tensor in [-1, 1].\n\n        Returns:\n            Tensor: atanh(y)\n        \"\"\"\n        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n        # one should use `cache_size=1` instead\n        return self.atanh(y)\n\n    def log_abs_det_jacobian(self, x, y):\n        \"\"\"\n        Log absolute determinant of the Jacobian of the transformation.\n\n        Args:\n            x (Tensor): Input tensor.\n            y (Tensor): Output tensor.\n\n        Returns:\n            Tensor: log|det(Jacobian)|\n        \"\"\"\n        # We use a formula that is more numerically stable, see details in the following link\n        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n        return 2.0 * (math.log(2.0) - x - F.softplus(-2.0 * x))\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.TanhTransform.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Equality check for the transform.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the other object is also a TanhTransform.</p> Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"\n    Equality check for the transform.\n\n    Returns:\n        bool: True if the other object is also a TanhTransform.\n    \"\"\"\n    return isinstance(other, TanhTransform)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.TanhTransform.__init__","title":"<code>__init__(cache_size=1)</code>","text":"<p>Initialize the TanhTransform.</p> <p>Parameters:</p> Name Type Description Default <code>cache_size</code> <code>int</code> <p>Size of the cache for storing intermediate values.</p> <code>1</code> Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>def __init__(self, cache_size=1):\n    \"\"\"\n    Initialize the TanhTransform.\n\n    Args:\n        cache_size (int): Size of the cache for storing intermediate values.\n    \"\"\"\n    super().__init__(cache_size=cache_size)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.TanhTransform.atanh","title":"<code>atanh(x)</code>  <code>staticmethod</code>","text":"<p>Inverse hyperbolic tangent function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>atanh(x)</p> Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>@staticmethod\ndef atanh(x):\n    \"\"\"\n    Inverse hyperbolic tangent function.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        Tensor: atanh(x)\n    \"\"\"\n    return 0.5 * (x.log1p() - (-x).log1p())\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_actor.TanhTransform.log_abs_det_jacobian","title":"<code>log_abs_det_jacobian(x, y)</code>","text":"<p>Log absolute determinant of the Jacobian of the transformation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor</code> <p>Output tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>log|det(Jacobian)|</p> Source code in <code>robot_nav/models/SAC/SAC_actor.py</code> <pre><code>def log_abs_det_jacobian(self, x, y):\n    \"\"\"\n    Log absolute determinant of the Jacobian of the transformation.\n\n    Args:\n        x (Tensor): Input tensor.\n        y (Tensor): Output tensor.\n\n    Returns:\n        Tensor: log|det(Jacobian)|\n    \"\"\"\n    # We use a formula that is more numerically stable, see details in the following link\n    # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n    return 2.0 * (math.log(2.0) - x - F.softplus(-2.0 * x))\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_critic","title":"<code>robot_nav.models.SAC.SAC_critic</code>","text":""},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_critic.DoubleQCritic","title":"<code>DoubleQCritic</code>","text":"<p>               Bases: <code>Module</code></p> <p>Double Q-learning critic network.</p> <p>Implements two independent Q-functions (Q1 and Q2) to mitigate overestimation bias in value estimates, as introduced in the Twin Delayed Deep Deterministic Policy Gradient (TD3) and Soft Actor-Critic (SAC) algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>obs_dim</code> <code>int</code> <p>Dimension of the observation space.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the action space.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of units in each hidden layer.</p> required <code>hidden_depth</code> <code>int</code> <p>Number of hidden layers.</p> required Source code in <code>robot_nav/models/SAC/SAC_critic.py</code> <pre><code>class DoubleQCritic(nn.Module):\n    \"\"\"\n    Double Q-learning critic network.\n\n    Implements two independent Q-functions (Q1 and Q2) to mitigate overestimation bias in value estimates,\n    as introduced in the Twin Delayed Deep Deterministic Policy Gradient (TD3) and Soft Actor-Critic (SAC) algorithms.\n\n    Args:\n        obs_dim (int): Dimension of the observation space.\n        action_dim (int): Dimension of the action space.\n        hidden_dim (int): Number of units in each hidden layer.\n        hidden_depth (int): Number of hidden layers.\n    \"\"\"\n\n    def __init__(self, obs_dim, action_dim, hidden_dim, hidden_depth):\n        \"\"\"\n        Initialize the Double Q-critic network with two MLPs.\n\n        Q1 and Q2 share the same architecture but have separate weights.\n        \"\"\"\n        super().__init__()\n\n        self.Q1 = utils.mlp(obs_dim + action_dim, hidden_dim, 1, hidden_depth)\n        self.Q2 = utils.mlp(obs_dim + action_dim, hidden_dim, 1, hidden_depth)\n\n        self.outputs = dict()\n        self.apply(utils.weight_init)\n\n    def forward(self, obs, action):\n        \"\"\"\n        Compute Q-values for the given observation-action pairs.\n\n        Args:\n            obs (Tensor): Observations of shape (batch_size, obs_dim).\n            action (Tensor): Actions of shape (batch_size, action_dim).\n\n        Returns:\n            Tuple[Tensor, Tensor]: Q1 and Q2 values, each of shape (batch_size, 1).\n        \"\"\"\n        assert obs.size(0) == action.size(0)\n\n        obs_action = torch.cat([obs, action], dim=-1)\n        q1 = self.Q1(obs_action)\n        q2 = self.Q2(obs_action)\n\n        self.outputs[\"q1\"] = q1\n        self.outputs[\"q2\"] = q2\n\n        return q1, q2\n\n    def log(self, writer, step):\n        \"\"\"\n        Log histograms of Q-value distributions to TensorBoard.\n\n        Args:\n            writer (SummaryWriter): TensorBoard writer instance.\n            step (int): Current training step (global).\n        \"\"\"\n        for k, v in self.outputs.items():\n            writer.add_histogram(f\"train_critic/{k}_hist\", v, step)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_critic.DoubleQCritic.__init__","title":"<code>__init__(obs_dim, action_dim, hidden_dim, hidden_depth)</code>","text":"<p>Initialize the Double Q-critic network with two MLPs.</p> <p>Q1 and Q2 share the same architecture but have separate weights.</p> Source code in <code>robot_nav/models/SAC/SAC_critic.py</code> <pre><code>def __init__(self, obs_dim, action_dim, hidden_dim, hidden_depth):\n    \"\"\"\n    Initialize the Double Q-critic network with two MLPs.\n\n    Q1 and Q2 share the same architecture but have separate weights.\n    \"\"\"\n    super().__init__()\n\n    self.Q1 = utils.mlp(obs_dim + action_dim, hidden_dim, 1, hidden_depth)\n    self.Q2 = utils.mlp(obs_dim + action_dim, hidden_dim, 1, hidden_depth)\n\n    self.outputs = dict()\n    self.apply(utils.weight_init)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_critic.DoubleQCritic.forward","title":"<code>forward(obs, action)</code>","text":"<p>Compute Q-values for the given observation-action pairs.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Tensor</code> <p>Observations of shape (batch_size, obs_dim).</p> required <code>action</code> <code>Tensor</code> <p>Actions of shape (batch_size, action_dim).</p> required <p>Returns:</p> Type Description <p>Tuple[Tensor, Tensor]: Q1 and Q2 values, each of shape (batch_size, 1).</p> Source code in <code>robot_nav/models/SAC/SAC_critic.py</code> <pre><code>def forward(self, obs, action):\n    \"\"\"\n    Compute Q-values for the given observation-action pairs.\n\n    Args:\n        obs (Tensor): Observations of shape (batch_size, obs_dim).\n        action (Tensor): Actions of shape (batch_size, action_dim).\n\n    Returns:\n        Tuple[Tensor, Tensor]: Q1 and Q2 values, each of shape (batch_size, 1).\n    \"\"\"\n    assert obs.size(0) == action.size(0)\n\n    obs_action = torch.cat([obs, action], dim=-1)\n    q1 = self.Q1(obs_action)\n    q2 = self.Q2(obs_action)\n\n    self.outputs[\"q1\"] = q1\n    self.outputs[\"q2\"] = q2\n\n    return q1, q2\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_critic.DoubleQCritic.log","title":"<code>log(writer, step)</code>","text":"<p>Log histograms of Q-value distributions to TensorBoard.</p> <p>Parameters:</p> Name Type Description Default <code>writer</code> <code>SummaryWriter</code> <p>TensorBoard writer instance.</p> required <code>step</code> <code>int</code> <p>Current training step (global).</p> required Source code in <code>robot_nav/models/SAC/SAC_critic.py</code> <pre><code>def log(self, writer, step):\n    \"\"\"\n    Log histograms of Q-value distributions to TensorBoard.\n\n    Args:\n        writer (SummaryWriter): TensorBoard writer instance.\n        step (int): Current training step (global).\n    \"\"\"\n    for k, v in self.outputs.items():\n        writer.add_histogram(f\"train_critic/{k}_hist\", v, step)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_utils","title":"<code>robot_nav.models.SAC.SAC_utils</code>","text":""},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_utils.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-layer perceptron (MLP) with configurable depth and optional output activation.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of hidden units in each hidden layer.</p> required <code>output_dim</code> <code>int</code> <p>Number of output features.</p> required <code>hidden_depth</code> <code>int</code> <p>Number of hidden layers.</p> required <code>output_mod</code> <code>Module</code> <p>Optional output activation module (e.g., Tanh, Sigmoid).</p> <code>None</code> Source code in <code>robot_nav/models/SAC/SAC_utils.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"\n    Multi-layer perceptron (MLP) with configurable depth and optional output activation.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_dim (int): Number of hidden units in each hidden layer.\n        output_dim (int): Number of output features.\n        hidden_depth (int): Number of hidden layers.\n        output_mod (nn.Module, optional): Optional output activation module (e.g., Tanh, Sigmoid).\n    \"\"\"\n\n    def __init__(\n        self, input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None\n    ):\n        super().__init__()\n        self.trunk = mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod)\n        self.apply(weight_init)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the MLP.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, input_dim).\n\n        Returns:\n            Tensor: Output tensor of shape (batch_size, output_dim).\n        \"\"\"\n        return self.trunk(x)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_utils.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, output_dim).</p> Source code in <code>robot_nav/models/SAC/SAC_utils.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass through the MLP.\n\n    Args:\n        x (Tensor): Input tensor of shape (batch_size, input_dim).\n\n    Returns:\n        Tensor: Output tensor of shape (batch_size, output_dim).\n    \"\"\"\n    return self.trunk(x)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_utils.make_dir","title":"<code>make_dir(*path_parts)</code>","text":"<p>Create a directory if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>*path_parts</code> <code>str</code> <p>Components of the path to be joined into the directory.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The full path of the created or existing directory.</p> Source code in <code>robot_nav/models/SAC/SAC_utils.py</code> <pre><code>def make_dir(*path_parts):\n    \"\"\"\n    Create a directory if it does not exist.\n\n    Args:\n        *path_parts (str): Components of the path to be joined into the directory.\n\n    Returns:\n        str: The full path of the created or existing directory.\n    \"\"\"\n    dir_path = os.path.join(*path_parts)\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_utils.mlp","title":"<code>mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None)</code>","text":"<p>Create an MLP as a <code>nn.Sequential</code> module.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input feature dimension.</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden layer size.</p> required <code>output_dim</code> <code>int</code> <p>Output feature dimension.</p> required <code>hidden_depth</code> <code>int</code> <p>Number of hidden layers.</p> required <code>output_mod</code> <code>Module</code> <p>Output activation module.</p> <code>None</code> <p>Returns:</p> Type Description <p>nn.Sequential: The constructed MLP.</p> Source code in <code>robot_nav/models/SAC/SAC_utils.py</code> <pre><code>def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):\n    \"\"\"\n    Create an MLP as a `nn.Sequential` module.\n\n    Args:\n        input_dim (int): Input feature dimension.\n        hidden_dim (int): Hidden layer size.\n        output_dim (int): Output feature dimension.\n        hidden_depth (int): Number of hidden layers.\n        output_mod (nn.Module, optional): Output activation module.\n\n    Returns:\n        nn.Sequential: The constructed MLP.\n    \"\"\"\n    if hidden_depth == 0:\n        mods = [nn.Linear(input_dim, output_dim)]\n    else:\n        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n        for i in range(hidden_depth - 1):\n            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n        mods.append(nn.Linear(hidden_dim, output_dim))\n    if output_mod is not None:\n        mods.append(output_mod)\n    trunk = nn.Sequential(*mods)\n    return trunk\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_utils.set_seed_everywhere","title":"<code>set_seed_everywhere(seed)</code>","text":"<p>Set random seed for reproducibility across NumPy, random, and PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed.</p> required Source code in <code>robot_nav/models/SAC/SAC_utils.py</code> <pre><code>def set_seed_everywhere(seed):\n    \"\"\"\n    Set random seed for reproducibility across NumPy, random, and PyTorch.\n\n    Args:\n        seed (int): Random seed.\n    \"\"\"\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_utils.soft_update_params","title":"<code>soft_update_params(net, target_net, tau)</code>","text":"<p>Perform a soft update of the parameters of the target network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>Source network whose parameters are used for updating.</p> required <code>target_net</code> <code>Module</code> <p>Target network to be updated.</p> required <code>tau</code> <code>float</code> <p>Interpolation parameter (0 &lt; tau &lt; 1) for soft updates.          A value closer to 1 means faster updates.</p> required Source code in <code>robot_nav/models/SAC/SAC_utils.py</code> <pre><code>def soft_update_params(net, target_net, tau):\n    \"\"\"\n    Perform a soft update of the parameters of the target network.\n\n    Args:\n        net (nn.Module): Source network whose parameters are used for updating.\n        target_net (nn.Module): Target network to be updated.\n        tau (float): Interpolation parameter (0 &lt; tau &lt; 1) for soft updates.\n                     A value closer to 1 means faster updates.\n    \"\"\"\n    for param, target_param in zip(net.parameters(), target_net.parameters()):\n        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n</code></pre>"},{"location":"api/models/SAC/#robot_nav.models.SAC.SAC_utils.weight_init","title":"<code>weight_init(m)</code>","text":"<p>Custom weight initialization for layers.</p> <p>Applies orthogonal initialization to Linear layers and zero initialization to biases.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Module</code> <p>Layer to initialize.</p> required Source code in <code>robot_nav/models/SAC/SAC_utils.py</code> <pre><code>def weight_init(m):\n    \"\"\"\n    Custom weight initialization for layers.\n\n    Applies orthogonal initialization to Linear layers and zero initialization to biases.\n\n    Args:\n        m (nn.Module): Layer to initialize.\n    \"\"\"\n    if isinstance(m, nn.Linear):\n        nn.init.orthogonal_(m.weight.data)\n        if hasattr(m.bias, \"data\"):\n            m.bias.data.fill_(0.0)\n</code></pre>"},{"location":"api/models/TD3/","title":"TD3","text":""},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3","title":"<code>robot_nav.models.TD3.TD3</code>","text":""},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.Actor","title":"<code>Actor</code>","text":"<p>               Bases: <code>Module</code></p> <p>Actor network for the TD3 algorithm.</p> <p>This neural network maps states to actions using a feedforward architecture with LeakyReLU activations and a final Tanh output to bound the actions in [-1, 1].</p> Architecture <p>Input: state_dim Hidden Layer 1: 400 units, LeakyReLU Hidden Layer 2: 300 units, LeakyReLU Output Layer: action_dim, Tanh</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the input state.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the action output.</p> required Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>class Actor(nn.Module):\n    \"\"\"\n    Actor network for the TD3 algorithm.\n\n    This neural network maps states to actions using a feedforward architecture with\n    LeakyReLU activations and a final Tanh output to bound the actions in [-1, 1].\n\n    Architecture:\n        Input: state_dim\n        Hidden Layer 1: 400 units, LeakyReLU\n        Hidden Layer 2: 300 units, LeakyReLU\n        Output Layer: action_dim, Tanh\n\n    Args:\n        state_dim (int): Dimension of the input state.\n        action_dim (int): Dimension of the action output.\n    \"\"\"\n\n    def __init__(self, state_dim, action_dim):\n        super(Actor, self).__init__()\n\n        self.layer_1 = nn.Linear(state_dim, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2 = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity=\"leaky_relu\")\n        self.layer_3 = nn.Linear(300, action_dim)\n        self.tanh = nn.Tanh()\n\n    def forward(self, s):\n        \"\"\"\n        Perform a forward pass through the actor network.\n\n        Args:\n            s (torch.Tensor): Input state tensor.\n\n        Returns:\n            torch.Tensor: Action output tensor after Tanh activation.\n        \"\"\"\n        s = F.leaky_relu(self.layer_1(s))\n        s = F.leaky_relu(self.layer_2(s))\n        a = self.tanh(self.layer_3(s))\n        return a\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.Actor.forward","title":"<code>forward(s)</code>","text":"<p>Perform a forward pass through the actor network.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Tensor</code> <p>Input state tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Action output tensor after Tanh activation.</p> Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def forward(self, s):\n    \"\"\"\n    Perform a forward pass through the actor network.\n\n    Args:\n        s (torch.Tensor): Input state tensor.\n\n    Returns:\n        torch.Tensor: Action output tensor after Tanh activation.\n    \"\"\"\n    s = F.leaky_relu(self.layer_1(s))\n    s = F.leaky_relu(self.layer_2(s))\n    a = self.tanh(self.layer_3(s))\n    return a\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.Critic","title":"<code>Critic</code>","text":"<p>               Bases: <code>Module</code></p> <p>Critic network for the TD3 algorithm.</p> <p>This class defines two Q-value estimators (Q1 and Q2) using separate subnetworks. Each Q-network takes both state and action as input and outputs a scalar Q-value.</p> Architecture for each Q-network <p>Input: state_dim and action_dim - State pathway: Linear + LeakyReLU \u2192 400 \u2192 300 - Action pathway: Linear \u2192 300 - Combined pathway: LeakyReLU(Linear(state) + Linear(action) + bias) \u2192 1</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the input state.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the input action.</p> required Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>class Critic(nn.Module):\n    \"\"\"\n    Critic network for the TD3 algorithm.\n\n    This class defines two Q-value estimators (Q1 and Q2) using separate subnetworks.\n    Each Q-network takes both state and action as input and outputs a scalar Q-value.\n\n    Architecture for each Q-network:\n        Input: state_dim and action_dim\n        - State pathway: Linear + LeakyReLU \u2192 400 \u2192 300\n        - Action pathway: Linear \u2192 300\n        - Combined pathway: LeakyReLU(Linear(state) + Linear(action) + bias) \u2192 1\n\n    Args:\n        state_dim (int): Dimension of the input state.\n        action_dim (int): Dimension of the input action.\n    \"\"\"\n\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        self.layer_1 = nn.Linear(state_dim, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2_s = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2_s.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2_a = nn.Linear(action_dim, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2_a.weight, nonlinearity=\"leaky_relu\")\n        self.layer_3 = nn.Linear(300, 1)\n        torch.nn.init.kaiming_uniform_(self.layer_3.weight, nonlinearity=\"leaky_relu\")\n\n        self.layer_4 = nn.Linear(state_dim, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_5_s = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_5_s.weight, nonlinearity=\"leaky_relu\")\n        self.layer_5_a = nn.Linear(action_dim, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_5_a.weight, nonlinearity=\"leaky_relu\")\n        self.layer_6 = nn.Linear(300, 1)\n        torch.nn.init.kaiming_uniform_(self.layer_6.weight, nonlinearity=\"leaky_relu\")\n\n    def forward(self, s, a):\n        \"\"\"\n        Perform a forward pass through both Q-networks.\n\n        Args:\n            s (torch.Tensor): Input state tensor.\n            a (torch.Tensor): Input action tensor.\n\n        Returns:\n            tuple:\n                - q1 (torch.Tensor): Output Q-value from the first critic network.\n                - q2 (torch.Tensor): Output Q-value from the second critic network.\n        \"\"\"\n        s1 = F.leaky_relu(self.layer_1(s))\n        self.layer_2_s(s1)\n        self.layer_2_a(a)\n        s11 = torch.mm(s1, self.layer_2_s.weight.data.t())\n        s12 = torch.mm(a, self.layer_2_a.weight.data.t())\n        s1 = F.leaky_relu(s11 + s12 + self.layer_2_a.bias.data)\n        q1 = self.layer_3(s1)\n\n        s2 = F.leaky_relu(self.layer_4(s))\n        self.layer_5_s(s2)\n        self.layer_5_a(a)\n        s21 = torch.mm(s2, self.layer_5_s.weight.data.t())\n        s22 = torch.mm(a, self.layer_5_a.weight.data.t())\n        s2 = F.leaky_relu(s21 + s22 + self.layer_5_a.bias.data)\n        q2 = self.layer_6(s2)\n        return q1, q2\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.Critic.forward","title":"<code>forward(s, a)</code>","text":"<p>Perform a forward pass through both Q-networks.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Tensor</code> <p>Input state tensor.</p> required <code>a</code> <code>Tensor</code> <p>Input action tensor.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>q1 (torch.Tensor): Output Q-value from the first critic network.</li> <li>q2 (torch.Tensor): Output Q-value from the second critic network.</li> </ul> Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def forward(self, s, a):\n    \"\"\"\n    Perform a forward pass through both Q-networks.\n\n    Args:\n        s (torch.Tensor): Input state tensor.\n        a (torch.Tensor): Input action tensor.\n\n    Returns:\n        tuple:\n            - q1 (torch.Tensor): Output Q-value from the first critic network.\n            - q2 (torch.Tensor): Output Q-value from the second critic network.\n    \"\"\"\n    s1 = F.leaky_relu(self.layer_1(s))\n    self.layer_2_s(s1)\n    self.layer_2_a(a)\n    s11 = torch.mm(s1, self.layer_2_s.weight.data.t())\n    s12 = torch.mm(a, self.layer_2_a.weight.data.t())\n    s1 = F.leaky_relu(s11 + s12 + self.layer_2_a.bias.data)\n    q1 = self.layer_3(s1)\n\n    s2 = F.leaky_relu(self.layer_4(s))\n    self.layer_5_s(s2)\n    self.layer_5_a(a)\n    s21 = torch.mm(s2, self.layer_5_s.weight.data.t())\n    s22 = torch.mm(a, self.layer_5_a.weight.data.t())\n    s2 = F.leaky_relu(s21 + s22 + self.layer_5_a.bias.data)\n    q2 = self.layer_6(s2)\n    return q1, q2\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.TD3","title":"<code>TD3</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>class TD3(object):\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        max_action,\n        device,\n        lr=1e-4,\n        save_every=0,\n        load_model=False,\n        save_directory=Path(\"robot_nav/models/TD3/checkpoint\"),\n        model_name=\"TD3\",\n        load_directory=Path(\"robot_nav/models/TD3/checkpoint\"),\n        use_max_bound=False,\n        bound_weight=0.25,\n    ):\n        \"\"\"\n        Twin Delayed Deep Deterministic Policy Gradient (TD3) agent.\n\n        This class implements the TD3 reinforcement learning algorithm for continuous control.\n        It uses an Actor-Critic architecture with target networks and delayed policy updates.\n\n        Args:\n            state_dim (int): Dimension of the input state.\n            action_dim (int): Dimension of the action space.\n            max_action (float): Maximum allowed value for actions.\n            device (torch.device): Device to run the model on (CPU or CUDA).\n            lr (float, optional): Learning rate for both actor and critic. Default is 1e-4.\n            save_every (int, optional): Save model every `save_every` iterations. Default is 0.\n            load_model (bool, optional): Whether to load model from checkpoint. Default is False.\n            save_directory (Path, optional): Directory to save model checkpoints.\n            model_name (str, optional): Name to use when saving/loading models.\n            load_directory (Path, optional): Directory to load model checkpoints from.\n            use_max_bound (bool, optional): Whether to apply maximum Q-value bounding during training.\n            bound_weight (float, optional): Weight for the max-bound loss penalty.\n        \"\"\"\n        self.device = device\n        # Initialize the Actor network\n        self.actor = Actor(state_dim, action_dim).to(self.device)\n        self.actor_target = Actor(state_dim, action_dim).to(self.device)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = torch.optim.Adam(params=self.actor.parameters(), lr=lr)\n\n        # Initialize the Critic networks\n        self.critic = Critic(state_dim, action_dim).to(self.device)\n        self.critic_target = Critic(state_dim, action_dim).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = torch.optim.Adam(params=self.critic.parameters(), lr=lr)\n\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.state_dim = state_dim\n        self.writer = SummaryWriter(comment=model_name)\n        self.iter_count = 0\n        if load_model:\n            self.load(filename=model_name, directory=load_directory)\n        self.save_every = save_every\n        self.model_name = model_name\n        self.save_directory = save_directory\n        self.use_max_bound = use_max_bound\n        self.bound_weight = bound_weight\n\n    def get_action(self, obs, add_noise):\n        \"\"\"\n        Get an action from the current policy with optional exploration noise.\n\n        Args:\n            obs (np.ndarray): The current state observation.\n            add_noise (bool): Whether to add exploration noise.\n\n        Returns:\n            np.ndarray: The chosen action clipped to [-max_action, max_action].\n        \"\"\"\n        if add_noise:\n            return (\n                self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n            ).clip(-self.max_action, self.max_action)\n        else:\n            return self.act(obs)\n\n    def act(self, state):\n        \"\"\"\n        Compute the action using the actor network without exploration noise.\n\n        Args:\n            state (np.ndarray): The current environment state.\n\n        Returns:\n            np.ndarray: The deterministic action predicted by the actor.\n        \"\"\"\n        state = torch.Tensor(state).to(self.device)\n        return self.actor(state).cpu().data.numpy().flatten()\n\n    # training cycle\n    def train(\n        self,\n        replay_buffer,\n        iterations,\n        batch_size,\n        discount=0.99,\n        tau=0.005,\n        policy_noise=0.2,\n        noise_clip=0.5,\n        policy_freq=2,\n        max_lin_vel=0.5,\n        max_ang_vel=1,\n        goal_reward=100,\n        distance_norm=10,\n        time_step=0.3,\n    ):\n        \"\"\"\n        Train the TD3 agent using batches sampled from the replay buffer.\n\n        Args:\n            replay_buffer: The replay buffer to sample experiences from.\n            iterations (int): Number of training iterations to perform.\n            batch_size (int): Size of each mini-batch.\n            discount (float): Discount factor gamma for future rewards.\n            tau (float): Soft update rate for target networks.\n            policy_noise (float): Stddev of Gaussian noise added to target actions.\n            noise_clip (float): Maximum magnitude of noise added to target actions.\n            policy_freq (int): Frequency of policy (actor) updates.\n            max_lin_vel (float): Max linear velocity used for upper bound estimation.\n            max_ang_vel (float): Max angular velocity used for upper bound estimation.\n            goal_reward (float): Reward given for reaching the goal.\n            distance_norm (float): Distance normalization factor.\n            time_step (float): Time step used in upper bound calculations.\n        \"\"\"\n        av_Q = 0\n        max_Q = -inf\n        av_loss = 0\n        for it in range(iterations):\n            # sample a batch from the replay buffer\n            (\n                batch_states,\n                batch_actions,\n                batch_rewards,\n                batch_dones,\n                batch_next_states,\n            ) = replay_buffer.sample_batch(batch_size)\n            state = torch.Tensor(batch_states).to(self.device)\n            next_state = torch.Tensor(batch_next_states).to(self.device)\n            action = torch.Tensor(batch_actions).to(self.device)\n            reward = torch.Tensor(batch_rewards).to(self.device)\n            done = torch.Tensor(batch_dones).to(self.device)\n\n            # Obtain the estimated action from the next state by using the actor-target\n            next_action = self.actor_target(next_state)\n\n            # Add noise to the action\n            noise = (\n                torch.Tensor(batch_actions)\n                .data.normal_(0, policy_noise)\n                .to(self.device)\n            )\n            noise = noise.clamp(-noise_clip, noise_clip)\n            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\n            # Calculate the Q values from the critic-target network for the next state-action pair\n            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n\n            # Select the minimal Q value from the 2 calculated values\n            target_Q = torch.min(target_Q1, target_Q2)\n            av_Q += torch.mean(target_Q)\n            max_Q = max(max_Q, torch.max(target_Q))\n            # Calculate the final Q value from the target network parameters by using Bellman equation\n            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n\n            # Get the Q values of the basis networks with the current parameters\n            current_Q1, current_Q2 = self.critic(state, action)\n\n            # Calculate the loss between the current Q value and the target Q value\n            loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\n            if self.use_max_bound:\n                max_bound = get_max_bound(\n                    next_state,\n                    discount,\n                    max_ang_vel,\n                    max_lin_vel,\n                    time_step,\n                    distance_norm,\n                    goal_reward,\n                    reward,\n                    done,\n                    self.device,\n                )\n                max_excess_Q1 = F.relu(current_Q1 - max_bound)\n                max_excess_Q2 = F.relu(current_Q2 - max_bound)\n                max_bound_loss = (max_excess_Q1**2).mean() + (max_excess_Q2**2).mean()\n                # Add loss for Q values exceeding maximum possible upper bound\n                loss += self.bound_weight * max_bound_loss\n\n            # Perform the gradient descent\n            self.critic_optimizer.zero_grad()\n            loss.backward()\n            self.critic_optimizer.step()\n\n            if it % policy_freq == 0:\n                # Maximize the actor output value by performing gradient descent on negative Q values\n                # (essentially perform gradient ascent)\n                actor_grad, _ = self.critic(state, self.actor(state))\n                actor_grad = -actor_grad.mean()\n                self.actor_optimizer.zero_grad()\n                actor_grad.backward()\n                self.actor_optimizer.step()\n\n                # Use soft update to update the actor-target network parameters by\n                # infusing small amount of current parameters\n                for param, target_param in zip(\n                    self.actor.parameters(), self.actor_target.parameters()\n                ):\n                    target_param.data.copy_(\n                        tau * param.data + (1 - tau) * target_param.data\n                    )\n                # Use soft update to update the critic-target network parameters by infusing\n                # small amount of current parameters\n                for param, target_param in zip(\n                    self.critic.parameters(), self.critic_target.parameters()\n                ):\n                    target_param.data.copy_(\n                        tau * param.data + (1 - tau) * target_param.data\n                    )\n\n            av_loss += loss\n        self.iter_count += 1\n        # Write new values for tensorboard\n        self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n        self.writer.add_scalar(\"train/avg_Q\", av_Q / iterations, self.iter_count)\n        self.writer.add_scalar(\"train/max_Q\", max_Q, self.iter_count)\n        if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n            self.save(filename=self.model_name, directory=self.save_directory)\n\n    def save(self, filename, directory):\n        \"\"\"\n        Save the actor and critic networks (and their targets) to disk.\n\n        Args:\n            filename (str): Name to use when saving model files.\n            directory (Path): Directory where models should be saved.\n        \"\"\"\n        Path(directory).mkdir(parents=True, exist_ok=True)\n        torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n        torch.save(\n            self.actor_target.state_dict(),\n            \"%s/%s_actor_target.pth\" % (directory, filename),\n        )\n        torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n        torch.save(\n            self.critic_target.state_dict(),\n            \"%s/%s_critic_target.pth\" % (directory, filename),\n        )\n\n    def load(self, filename, directory):\n        \"\"\"\n        Load the actor and critic networks (and their targets) from disk.\n\n        Args:\n            filename (str): Name used when saving the models.\n            directory (Path): Directory where models are saved.\n        \"\"\"\n        self.actor.load_state_dict(\n            torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n        )\n        self.actor_target.load_state_dict(\n            torch.load(\"%s/%s_actor_target.pth\" % (directory, filename))\n        )\n        self.critic.load_state_dict(\n            torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n        )\n        self.critic_target.load_state_dict(\n            torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n        )\n        print(f\"Loaded weights from: {directory}\")\n\n    def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n        \"\"\"\n        Prepare the input state vector for training or inference.\n\n        Combines processed laser scan data, goal vector, and past action\n        into a normalized state input matching the input dimension.\n\n        Args:\n            latest_scan (list or np.ndarray): Laser scan data.\n            distance (float): Distance to goal.\n            cos (float): Cosine of the heading angle to goal.\n            sin (float): Sine of the heading angle to goal.\n            collision (bool): Whether a collision occurred.\n            goal (bool): Whether the goal has been reached.\n            action (list or np.ndarray): Last executed action [linear_vel, angular_vel].\n\n        Returns:\n            tuple:\n                - state (list): Prepared and normalized state vector.\n                - terminal (int): 1 if episode should terminate (goal or collision), else 0.\n        \"\"\"\n        latest_scan = np.array(latest_scan)\n\n        inf_mask = np.isinf(latest_scan)\n        latest_scan[inf_mask] = 7.0\n\n        max_bins = self.state_dim - 5\n        bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n        # Initialize the list to store the minimum values of each bin\n        min_values = []\n\n        # Loop through the data and create bins\n        for i in range(0, len(latest_scan), bin_size):\n            # Get the current bin\n            bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n            # Find the minimum value in the current bin and append it to the min_values list\n            min_values.append(min(bin) / 7)\n\n        # Normalize to [0, 1] range\n        distance /= 10\n        lin_vel = action[0] * 2\n        ang_vel = (action[1] + 1) / 2\n        state = min_values + [distance, cos, sin] + [lin_vel, ang_vel]\n\n        assert len(state) == self.state_dim\n        terminal = 1 if collision or goal else 0\n\n        return state, terminal\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.TD3.__init__","title":"<code>__init__(state_dim, action_dim, max_action, device, lr=0.0001, save_every=0, load_model=False, save_directory=Path('robot_nav/models/TD3/checkpoint'), model_name='TD3', load_directory=Path('robot_nav/models/TD3/checkpoint'), use_max_bound=False, bound_weight=0.25)</code>","text":"<p>Twin Delayed Deep Deterministic Policy Gradient (TD3) agent.</p> <p>This class implements the TD3 reinforcement learning algorithm for continuous control. It uses an Actor-Critic architecture with target networks and delayed policy updates.</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the input state.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the action space.</p> required <code>max_action</code> <code>float</code> <p>Maximum allowed value for actions.</p> required <code>device</code> <code>device</code> <p>Device to run the model on (CPU or CUDA).</p> required <code>lr</code> <code>float</code> <p>Learning rate for both actor and critic. Default is 1e-4.</p> <code>0.0001</code> <code>save_every</code> <code>int</code> <p>Save model every <code>save_every</code> iterations. Default is 0.</p> <code>0</code> <code>load_model</code> <code>bool</code> <p>Whether to load model from checkpoint. Default is False.</p> <code>False</code> <code>save_directory</code> <code>Path</code> <p>Directory to save model checkpoints.</p> <code>Path('robot_nav/models/TD3/checkpoint')</code> <code>model_name</code> <code>str</code> <p>Name to use when saving/loading models.</p> <code>'TD3'</code> <code>load_directory</code> <code>Path</code> <p>Directory to load model checkpoints from.</p> <code>Path('robot_nav/models/TD3/checkpoint')</code> <code>use_max_bound</code> <code>bool</code> <p>Whether to apply maximum Q-value bounding during training.</p> <code>False</code> <code>bound_weight</code> <code>float</code> <p>Weight for the max-bound loss penalty.</p> <code>0.25</code> Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def __init__(\n    self,\n    state_dim,\n    action_dim,\n    max_action,\n    device,\n    lr=1e-4,\n    save_every=0,\n    load_model=False,\n    save_directory=Path(\"robot_nav/models/TD3/checkpoint\"),\n    model_name=\"TD3\",\n    load_directory=Path(\"robot_nav/models/TD3/checkpoint\"),\n    use_max_bound=False,\n    bound_weight=0.25,\n):\n    \"\"\"\n    Twin Delayed Deep Deterministic Policy Gradient (TD3) agent.\n\n    This class implements the TD3 reinforcement learning algorithm for continuous control.\n    It uses an Actor-Critic architecture with target networks and delayed policy updates.\n\n    Args:\n        state_dim (int): Dimension of the input state.\n        action_dim (int): Dimension of the action space.\n        max_action (float): Maximum allowed value for actions.\n        device (torch.device): Device to run the model on (CPU or CUDA).\n        lr (float, optional): Learning rate for both actor and critic. Default is 1e-4.\n        save_every (int, optional): Save model every `save_every` iterations. Default is 0.\n        load_model (bool, optional): Whether to load model from checkpoint. Default is False.\n        save_directory (Path, optional): Directory to save model checkpoints.\n        model_name (str, optional): Name to use when saving/loading models.\n        load_directory (Path, optional): Directory to load model checkpoints from.\n        use_max_bound (bool, optional): Whether to apply maximum Q-value bounding during training.\n        bound_weight (float, optional): Weight for the max-bound loss penalty.\n    \"\"\"\n    self.device = device\n    # Initialize the Actor network\n    self.actor = Actor(state_dim, action_dim).to(self.device)\n    self.actor_target = Actor(state_dim, action_dim).to(self.device)\n    self.actor_target.load_state_dict(self.actor.state_dict())\n    self.actor_optimizer = torch.optim.Adam(params=self.actor.parameters(), lr=lr)\n\n    # Initialize the Critic networks\n    self.critic = Critic(state_dim, action_dim).to(self.device)\n    self.critic_target = Critic(state_dim, action_dim).to(self.device)\n    self.critic_target.load_state_dict(self.critic.state_dict())\n    self.critic_optimizer = torch.optim.Adam(params=self.critic.parameters(), lr=lr)\n\n    self.action_dim = action_dim\n    self.max_action = max_action\n    self.state_dim = state_dim\n    self.writer = SummaryWriter(comment=model_name)\n    self.iter_count = 0\n    if load_model:\n        self.load(filename=model_name, directory=load_directory)\n    self.save_every = save_every\n    self.model_name = model_name\n    self.save_directory = save_directory\n    self.use_max_bound = use_max_bound\n    self.bound_weight = bound_weight\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.TD3.act","title":"<code>act(state)</code>","text":"<p>Compute the action using the actor network without exploration noise.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>The current environment state.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The deterministic action predicted by the actor.</p> Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def act(self, state):\n    \"\"\"\n    Compute the action using the actor network without exploration noise.\n\n    Args:\n        state (np.ndarray): The current environment state.\n\n    Returns:\n        np.ndarray: The deterministic action predicted by the actor.\n    \"\"\"\n    state = torch.Tensor(state).to(self.device)\n    return self.actor(state).cpu().data.numpy().flatten()\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.TD3.get_action","title":"<code>get_action(obs, add_noise)</code>","text":"<p>Get an action from the current policy with optional exploration noise.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>ndarray</code> <p>The current state observation.</p> required <code>add_noise</code> <code>bool</code> <p>Whether to add exploration noise.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The chosen action clipped to [-max_action, max_action].</p> Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def get_action(self, obs, add_noise):\n    \"\"\"\n    Get an action from the current policy with optional exploration noise.\n\n    Args:\n        obs (np.ndarray): The current state observation.\n        add_noise (bool): Whether to add exploration noise.\n\n    Returns:\n        np.ndarray: The chosen action clipped to [-max_action, max_action].\n    \"\"\"\n    if add_noise:\n        return (\n            self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n        ).clip(-self.max_action, self.max_action)\n    else:\n        return self.act(obs)\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.TD3.load","title":"<code>load(filename, directory)</code>","text":"<p>Load the actor and critic networks (and their targets) from disk.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name used when saving the models.</p> required <code>directory</code> <code>Path</code> <p>Directory where models are saved.</p> required Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def load(self, filename, directory):\n    \"\"\"\n    Load the actor and critic networks (and their targets) from disk.\n\n    Args:\n        filename (str): Name used when saving the models.\n        directory (Path): Directory where models are saved.\n    \"\"\"\n    self.actor.load_state_dict(\n        torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n    )\n    self.actor_target.load_state_dict(\n        torch.load(\"%s/%s_actor_target.pth\" % (directory, filename))\n    )\n    self.critic.load_state_dict(\n        torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n    )\n    self.critic_target.load_state_dict(\n        torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n    )\n    print(f\"Loaded weights from: {directory}\")\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.TD3.prepare_state","title":"<code>prepare_state(latest_scan, distance, cos, sin, collision, goal, action)</code>","text":"<p>Prepare the input state vector for training or inference.</p> <p>Combines processed laser scan data, goal vector, and past action into a normalized state input matching the input dimension.</p> <p>Parameters:</p> Name Type Description Default <code>latest_scan</code> <code>list or ndarray</code> <p>Laser scan data.</p> required <code>distance</code> <code>float</code> <p>Distance to goal.</p> required <code>cos</code> <code>float</code> <p>Cosine of the heading angle to goal.</p> required <code>sin</code> <code>float</code> <p>Sine of the heading angle to goal.</p> required <code>collision</code> <code>bool</code> <p>Whether a collision occurred.</p> required <code>goal</code> <code>bool</code> <p>Whether the goal has been reached.</p> required <code>action</code> <code>list or ndarray</code> <p>Last executed action [linear_vel, angular_vel].</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>state (list): Prepared and normalized state vector.</li> <li>terminal (int): 1 if episode should terminate (goal or collision), else 0.</li> </ul> Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n    \"\"\"\n    Prepare the input state vector for training or inference.\n\n    Combines processed laser scan data, goal vector, and past action\n    into a normalized state input matching the input dimension.\n\n    Args:\n        latest_scan (list or np.ndarray): Laser scan data.\n        distance (float): Distance to goal.\n        cos (float): Cosine of the heading angle to goal.\n        sin (float): Sine of the heading angle to goal.\n        collision (bool): Whether a collision occurred.\n        goal (bool): Whether the goal has been reached.\n        action (list or np.ndarray): Last executed action [linear_vel, angular_vel].\n\n    Returns:\n        tuple:\n            - state (list): Prepared and normalized state vector.\n            - terminal (int): 1 if episode should terminate (goal or collision), else 0.\n    \"\"\"\n    latest_scan = np.array(latest_scan)\n\n    inf_mask = np.isinf(latest_scan)\n    latest_scan[inf_mask] = 7.0\n\n    max_bins = self.state_dim - 5\n    bin_size = int(np.ceil(len(latest_scan) / max_bins))\n\n    # Initialize the list to store the minimum values of each bin\n    min_values = []\n\n    # Loop through the data and create bins\n    for i in range(0, len(latest_scan), bin_size):\n        # Get the current bin\n        bin = latest_scan[i : i + min(bin_size, len(latest_scan) - i)]\n        # Find the minimum value in the current bin and append it to the min_values list\n        min_values.append(min(bin) / 7)\n\n    # Normalize to [0, 1] range\n    distance /= 10\n    lin_vel = action[0] * 2\n    ang_vel = (action[1] + 1) / 2\n    state = min_values + [distance, cos, sin] + [lin_vel, ang_vel]\n\n    assert len(state) == self.state_dim\n    terminal = 1 if collision or goal else 0\n\n    return state, terminal\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.TD3.save","title":"<code>save(filename, directory)</code>","text":"<p>Save the actor and critic networks (and their targets) to disk.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name to use when saving model files.</p> required <code>directory</code> <code>Path</code> <p>Directory where models should be saved.</p> required Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def save(self, filename, directory):\n    \"\"\"\n    Save the actor and critic networks (and their targets) to disk.\n\n    Args:\n        filename (str): Name to use when saving model files.\n        directory (Path): Directory where models should be saved.\n    \"\"\"\n    Path(directory).mkdir(parents=True, exist_ok=True)\n    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n    torch.save(\n        self.actor_target.state_dict(),\n        \"%s/%s_actor_target.pth\" % (directory, filename),\n    )\n    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n    torch.save(\n        self.critic_target.state_dict(),\n        \"%s/%s_critic_target.pth\" % (directory, filename),\n    )\n</code></pre>"},{"location":"api/models/TD3/#robot_nav.models.TD3.TD3.TD3.train","title":"<code>train(replay_buffer, iterations, batch_size, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2, max_lin_vel=0.5, max_ang_vel=1, goal_reward=100, distance_norm=10, time_step=0.3)</code>","text":"<p>Train the TD3 agent using batches sampled from the replay buffer.</p> <p>Parameters:</p> Name Type Description Default <code>replay_buffer</code> <p>The replay buffer to sample experiences from.</p> required <code>iterations</code> <code>int</code> <p>Number of training iterations to perform.</p> required <code>batch_size</code> <code>int</code> <p>Size of each mini-batch.</p> required <code>discount</code> <code>float</code> <p>Discount factor gamma for future rewards.</p> <code>0.99</code> <code>tau</code> <code>float</code> <p>Soft update rate for target networks.</p> <code>0.005</code> <code>policy_noise</code> <code>float</code> <p>Stddev of Gaussian noise added to target actions.</p> <code>0.2</code> <code>noise_clip</code> <code>float</code> <p>Maximum magnitude of noise added to target actions.</p> <code>0.5</code> <code>policy_freq</code> <code>int</code> <p>Frequency of policy (actor) updates.</p> <code>2</code> <code>max_lin_vel</code> <code>float</code> <p>Max linear velocity used for upper bound estimation.</p> <code>0.5</code> <code>max_ang_vel</code> <code>float</code> <p>Max angular velocity used for upper bound estimation.</p> <code>1</code> <code>goal_reward</code> <code>float</code> <p>Reward given for reaching the goal.</p> <code>100</code> <code>distance_norm</code> <code>float</code> <p>Distance normalization factor.</p> <code>10</code> <code>time_step</code> <code>float</code> <p>Time step used in upper bound calculations.</p> <code>0.3</code> Source code in <code>robot_nav/models/TD3/TD3.py</code> <pre><code>def train(\n    self,\n    replay_buffer,\n    iterations,\n    batch_size,\n    discount=0.99,\n    tau=0.005,\n    policy_noise=0.2,\n    noise_clip=0.5,\n    policy_freq=2,\n    max_lin_vel=0.5,\n    max_ang_vel=1,\n    goal_reward=100,\n    distance_norm=10,\n    time_step=0.3,\n):\n    \"\"\"\n    Train the TD3 agent using batches sampled from the replay buffer.\n\n    Args:\n        replay_buffer: The replay buffer to sample experiences from.\n        iterations (int): Number of training iterations to perform.\n        batch_size (int): Size of each mini-batch.\n        discount (float): Discount factor gamma for future rewards.\n        tau (float): Soft update rate for target networks.\n        policy_noise (float): Stddev of Gaussian noise added to target actions.\n        noise_clip (float): Maximum magnitude of noise added to target actions.\n        policy_freq (int): Frequency of policy (actor) updates.\n        max_lin_vel (float): Max linear velocity used for upper bound estimation.\n        max_ang_vel (float): Max angular velocity used for upper bound estimation.\n        goal_reward (float): Reward given for reaching the goal.\n        distance_norm (float): Distance normalization factor.\n        time_step (float): Time step used in upper bound calculations.\n    \"\"\"\n    av_Q = 0\n    max_Q = -inf\n    av_loss = 0\n    for it in range(iterations):\n        # sample a batch from the replay buffer\n        (\n            batch_states,\n            batch_actions,\n            batch_rewards,\n            batch_dones,\n            batch_next_states,\n        ) = replay_buffer.sample_batch(batch_size)\n        state = torch.Tensor(batch_states).to(self.device)\n        next_state = torch.Tensor(batch_next_states).to(self.device)\n        action = torch.Tensor(batch_actions).to(self.device)\n        reward = torch.Tensor(batch_rewards).to(self.device)\n        done = torch.Tensor(batch_dones).to(self.device)\n\n        # Obtain the estimated action from the next state by using the actor-target\n        next_action = self.actor_target(next_state)\n\n        # Add noise to the action\n        noise = (\n            torch.Tensor(batch_actions)\n            .data.normal_(0, policy_noise)\n            .to(self.device)\n        )\n        noise = noise.clamp(-noise_clip, noise_clip)\n        next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\n        # Calculate the Q values from the critic-target network for the next state-action pair\n        target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n\n        # Select the minimal Q value from the 2 calculated values\n        target_Q = torch.min(target_Q1, target_Q2)\n        av_Q += torch.mean(target_Q)\n        max_Q = max(max_Q, torch.max(target_Q))\n        # Calculate the final Q value from the target network parameters by using Bellman equation\n        target_Q = reward + ((1 - done) * discount * target_Q).detach()\n\n        # Get the Q values of the basis networks with the current parameters\n        current_Q1, current_Q2 = self.critic(state, action)\n\n        # Calculate the loss between the current Q value and the target Q value\n        loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\n        if self.use_max_bound:\n            max_bound = get_max_bound(\n                next_state,\n                discount,\n                max_ang_vel,\n                max_lin_vel,\n                time_step,\n                distance_norm,\n                goal_reward,\n                reward,\n                done,\n                self.device,\n            )\n            max_excess_Q1 = F.relu(current_Q1 - max_bound)\n            max_excess_Q2 = F.relu(current_Q2 - max_bound)\n            max_bound_loss = (max_excess_Q1**2).mean() + (max_excess_Q2**2).mean()\n            # Add loss for Q values exceeding maximum possible upper bound\n            loss += self.bound_weight * max_bound_loss\n\n        # Perform the gradient descent\n        self.critic_optimizer.zero_grad()\n        loss.backward()\n        self.critic_optimizer.step()\n\n        if it % policy_freq == 0:\n            # Maximize the actor output value by performing gradient descent on negative Q values\n            # (essentially perform gradient ascent)\n            actor_grad, _ = self.critic(state, self.actor(state))\n            actor_grad = -actor_grad.mean()\n            self.actor_optimizer.zero_grad()\n            actor_grad.backward()\n            self.actor_optimizer.step()\n\n            # Use soft update to update the actor-target network parameters by\n            # infusing small amount of current parameters\n            for param, target_param in zip(\n                self.actor.parameters(), self.actor_target.parameters()\n            ):\n                target_param.data.copy_(\n                    tau * param.data + (1 - tau) * target_param.data\n                )\n            # Use soft update to update the critic-target network parameters by infusing\n            # small amount of current parameters\n            for param, target_param in zip(\n                self.critic.parameters(), self.critic_target.parameters()\n            ):\n                target_param.data.copy_(\n                    tau * param.data + (1 - tau) * target_param.data\n                )\n\n        av_loss += loss\n    self.iter_count += 1\n    # Write new values for tensorboard\n    self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n    self.writer.add_scalar(\"train/avg_Q\", av_Q / iterations, self.iter_count)\n    self.writer.add_scalar(\"train/max_Q\", max_Q, self.iter_count)\n    if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n        self.save(filename=self.model_name, directory=self.save_directory)\n</code></pre>"},{"location":"api/models/__init__/","title":"init","text":""},{"location":"api/models/__init__/#documentation","title":"Documentation","text":"<pre><code>No documentation available for __init__.\n</code></pre>"},{"location":"api/models/cnntd3/","title":"CNNTD3","text":""},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3","title":"<code>robot_nav.models.CNNTD3.CNNTD3</code>","text":""},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.Actor","title":"<code>Actor</code>","text":"<p>               Bases: <code>Module</code></p> <p>Actor network for the CNNTD3 agent.</p> <p>This network takes as input a state composed of laser scan data, goal position encoding, and previous action. It processes the scan through a 1D CNN stack and embeds the other inputs before merging all features through fully connected layers to output a continuous action vector.</p> <p>Parameters:</p> Name Type Description Default <code>action_dim</code> <code>int</code> <p>The dimension of the action space.</p> required Architecture <ul> <li>1D CNN layers process the laser scan data.</li> <li>Fully connected layers embed the goal vector (cos, sin, distance) and last action.</li> <li>Combined features are passed through two fully connected layers with LeakyReLU.</li> <li>Final action output is scaled with Tanh to bound the values.</li> </ul> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>class Actor(nn.Module):\n    \"\"\"\n    Actor network for the CNNTD3 agent.\n\n    This network takes as input a state composed of laser scan data, goal position encoding,\n    and previous action. It processes the scan through a 1D CNN stack and embeds the other\n    inputs before merging all features through fully connected layers to output a continuous\n    action vector.\n\n    Args:\n        action_dim (int): The dimension of the action space.\n\n    Architecture:\n        - 1D CNN layers process the laser scan data.\n        - Fully connected layers embed the goal vector (cos, sin, distance) and last action.\n        - Combined features are passed through two fully connected layers with LeakyReLU.\n        - Final action output is scaled with Tanh to bound the values.\n    \"\"\"\n\n    def __init__(self, action_dim):\n        super(Actor, self).__init__()\n\n        self.cnn1 = nn.Conv1d(1, 4, kernel_size=8, stride=4)\n        self.cnn2 = nn.Conv1d(4, 8, kernel_size=8, stride=4)\n        self.cnn3 = nn.Conv1d(8, 4, kernel_size=4, stride=2)\n\n        self.goal_embed = nn.Linear(3, 10)\n        self.action_embed = nn.Linear(2, 10)\n\n        self.layer_1 = nn.Linear(36, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2 = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity=\"leaky_relu\")\n        self.layer_3 = nn.Linear(300, action_dim)\n        self.tanh = nn.Tanh()\n\n    def forward(self, s):\n        \"\"\"\n        Forward pass through the Actor network.\n\n        Args:\n            s (torch.Tensor): Input state tensor of shape (batch_size, state_dim).\n                              The last 5 elements are [distance, cos, sin, lin_vel, ang_vel].\n\n        Returns:\n            torch.Tensor: Action tensor of shape (batch_size, action_dim),\n                          with values in range [-1, 1] due to tanh activation.\n        \"\"\"\n        if len(s.shape) == 1:\n            s = s.unsqueeze(0)\n        laser = s[:, :-5]\n        goal = s[:, -5:-2]\n        act = s[:, -2:]\n        laser = laser.unsqueeze(1)\n\n        l = F.leaky_relu(self.cnn1(laser))\n        l = F.leaky_relu(self.cnn2(l))\n        l = F.leaky_relu(self.cnn3(l))\n        l = l.flatten(start_dim=1)\n\n        g = F.leaky_relu(self.goal_embed(goal))\n\n        a = F.leaky_relu(self.action_embed(act))\n\n        s = torch.concat((l, g, a), dim=-1)\n\n        s = F.leaky_relu(self.layer_1(s))\n        s = F.leaky_relu(self.layer_2(s))\n        a = self.tanh(self.layer_3(s))\n        return a\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.Actor.forward","title":"<code>forward(s)</code>","text":"<p>Forward pass through the Actor network.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_dim).               The last 5 elements are [distance, cos, sin, lin_vel, ang_vel].</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Action tensor of shape (batch_size, action_dim),           with values in range [-1, 1] due to tanh activation.</p> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>def forward(self, s):\n    \"\"\"\n    Forward pass through the Actor network.\n\n    Args:\n        s (torch.Tensor): Input state tensor of shape (batch_size, state_dim).\n                          The last 5 elements are [distance, cos, sin, lin_vel, ang_vel].\n\n    Returns:\n        torch.Tensor: Action tensor of shape (batch_size, action_dim),\n                      with values in range [-1, 1] due to tanh activation.\n    \"\"\"\n    if len(s.shape) == 1:\n        s = s.unsqueeze(0)\n    laser = s[:, :-5]\n    goal = s[:, -5:-2]\n    act = s[:, -2:]\n    laser = laser.unsqueeze(1)\n\n    l = F.leaky_relu(self.cnn1(laser))\n    l = F.leaky_relu(self.cnn2(l))\n    l = F.leaky_relu(self.cnn3(l))\n    l = l.flatten(start_dim=1)\n\n    g = F.leaky_relu(self.goal_embed(goal))\n\n    a = F.leaky_relu(self.action_embed(act))\n\n    s = torch.concat((l, g, a), dim=-1)\n\n    s = F.leaky_relu(self.layer_1(s))\n    s = F.leaky_relu(self.layer_2(s))\n    a = self.tanh(self.layer_3(s))\n    return a\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.CNNTD3","title":"<code>CNNTD3</code>","text":"<p>               Bases: <code>object</code></p> <p>CNNTD3 (Twin Delayed Deep Deterministic Policy Gradient with CNN-based inputs) agent for continuous control tasks.</p> <p>This class encapsulates the full implementation of the TD3 algorithm using neural network architectures for the actor and critic, with optional bounding for critic outputs to regularize learning. The agent is designed to train in environments where sensor observations (e.g., LiDAR) are used for navigation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of the input state.</p> required <code>action_dim</code> <code>int</code> <p>Dimension of the output action.</p> required <code>max_action</code> <code>float</code> <p>Maximum magnitude of the action.</p> required <code>device</code> <code>device</code> <p>Torch device to use (CPU or GPU).</p> required <code>lr</code> <code>float</code> <p>Learning rate for both actor and critic optimizers.</p> <code>0.0001</code> <code>save_every</code> <code>int</code> <p>Save model every N training iterations (0 to disable).</p> <code>0</code> <code>load_model</code> <code>bool</code> <p>Whether to load a pre-trained model at initialization.</p> <code>False</code> <code>save_directory</code> <code>Path</code> <p>Path to the directory for saving model checkpoints.</p> <code>Path('robot_nav/models/CNNTD3/checkpoint')</code> <code>model_name</code> <code>str</code> <p>Base name for the saved model files.</p> <code>'CNNTD3'</code> <code>load_directory</code> <code>Path</code> <p>Path to load model checkpoints from (if <code>load_model=True</code>).</p> <code>Path('robot_nav/models/CNNTD3/checkpoint')</code> <code>use_max_bound</code> <code>bool</code> <p>Whether to apply maximum Q-value bounding during training.</p> <code>False</code> <code>bound_weight</code> <code>float</code> <p>Weight for the bounding loss term in total loss.</p> <code>0.25</code> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>class CNNTD3(object):\n    \"\"\"\n    CNNTD3 (Twin Delayed Deep Deterministic Policy Gradient with CNN-based inputs) agent for\n    continuous control tasks.\n\n    This class encapsulates the full implementation of the TD3 algorithm using neural network\n    architectures for the actor and critic, with optional bounding for critic outputs to\n    regularize learning. The agent is designed to train in environments where sensor\n    observations (e.g., LiDAR) are used for navigation tasks.\n\n    Args:\n        state_dim (int): Dimension of the input state.\n        action_dim (int): Dimension of the output action.\n        max_action (float): Maximum magnitude of the action.\n        device (torch.device): Torch device to use (CPU or GPU).\n        lr (float): Learning rate for both actor and critic optimizers.\n        save_every (int): Save model every N training iterations (0 to disable).\n        load_model (bool): Whether to load a pre-trained model at initialization.\n        save_directory (Path): Path to the directory for saving model checkpoints.\n        model_name (str): Base name for the saved model files.\n        load_directory (Path): Path to load model checkpoints from (if `load_model=True`).\n        use_max_bound (bool): Whether to apply maximum Q-value bounding during training.\n        bound_weight (float): Weight for the bounding loss term in total loss.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        max_action,\n        device,\n        lr=1e-4,\n        save_every=0,\n        load_model=False,\n        save_directory=Path(\"robot_nav/models/CNNTD3/checkpoint\"),\n        model_name=\"CNNTD3\",\n        load_directory=Path(\"robot_nav/models/CNNTD3/checkpoint\"),\n        use_max_bound=False,\n        bound_weight=0.25,\n    ):\n        # Initialize the Actor network\n        self.device = device\n        self.actor = Actor(action_dim).to(self.device)\n        self.actor_target = Actor(action_dim).to(self.device)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = torch.optim.Adam(params=self.actor.parameters(), lr=lr)\n\n        # Initialize the Critic networks\n        self.critic = Critic(action_dim).to(self.device)\n        self.critic_target = Critic(action_dim).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = torch.optim.Adam(params=self.critic.parameters(), lr=lr)\n\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.state_dim = state_dim\n        self.writer = SummaryWriter(comment=model_name)\n        self.iter_count = 0\n        if load_model:\n            self.load(filename=model_name, directory=load_directory)\n        self.save_every = save_every\n        self.model_name = model_name\n        self.save_directory = save_directory\n        self.use_max_bound = use_max_bound\n        self.bound_weight = bound_weight\n\n    def get_action(self, obs, add_noise):\n        \"\"\"\n        Selects an action for a given observation.\n\n        Args:\n            obs (np.ndarray): The current observation/state.\n            add_noise (bool): Whether to add exploration noise to the action.\n\n        Returns:\n            np.ndarray: The selected action.\n        \"\"\"\n        if add_noise:\n            return (\n                self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n            ).clip(-self.max_action, self.max_action)\n        else:\n            return self.act(obs)\n\n    def act(self, state):\n        \"\"\"\n        Computes the deterministic action from the actor network for a given state.\n\n        Args:\n            state (np.ndarray): Input state.\n\n        Returns:\n            np.ndarray: Action predicted by the actor network.\n        \"\"\"\n        # Function to get the action from the actor\n        state = torch.Tensor(state).to(self.device)\n        return self.actor(state).cpu().data.numpy().flatten()\n\n    # training cycle\n    def train(\n        self,\n        replay_buffer,\n        iterations,\n        batch_size,\n        discount=0.99,\n        tau=0.005,\n        policy_noise=0.2,\n        noise_clip=0.5,\n        policy_freq=2,\n        max_lin_vel=0.5,\n        max_ang_vel=1,\n        goal_reward=100,\n        distance_norm=10,\n        time_step=0.3,\n    ):\n        \"\"\"\n        Trains the CNNTD3 agent using sampled batches from the replay buffer.\n\n        Args:\n            replay_buffer (ReplayBuffer): Buffer storing environment transitions.\n            iterations (int): Number of training iterations.\n            batch_size (int): Size of each training batch.\n            discount (float): Discount factor for future rewards.\n            tau (float): Soft update rate for target networks.\n            policy_noise (float): Std. dev. of noise added to target policy.\n            noise_clip (float): Maximum value for target policy noise.\n            policy_freq (int): Frequency of actor and target network updates.\n            max_lin_vel (float): Maximum linear velocity for bounding calculations.\n            max_ang_vel (float): Maximum angular velocity for bounding calculations.\n            goal_reward (float): Reward value for reaching the goal.\n            distance_norm (float): Normalization factor for distance in bounding.\n            time_step (float): Time delta between steps.\n        \"\"\"\n        av_Q = 0\n        max_Q = -inf\n        av_loss = 0\n        for it in range(iterations):\n            # sample a batch from the replay buffer\n            (\n                batch_states,\n                batch_actions,\n                batch_rewards,\n                batch_dones,\n                batch_next_states,\n            ) = replay_buffer.sample_batch(batch_size)\n            state = torch.Tensor(batch_states).to(self.device)\n            next_state = torch.Tensor(batch_next_states).to(self.device)\n            action = torch.Tensor(batch_actions).to(self.device)\n            reward = torch.Tensor(batch_rewards).to(self.device)\n            done = torch.Tensor(batch_dones).to(self.device)\n\n            # Obtain the estimated action from the next state by using the actor-target\n            next_action = self.actor_target(next_state)\n\n            # Add noise to the action\n            noise = (\n                torch.Tensor(batch_actions)\n                .data.normal_(0, policy_noise)\n                .to(self.device)\n            )\n            noise = noise.clamp(-noise_clip, noise_clip)\n            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\n            # Calculate the Q values from the critic-target network for the next state-action pair\n            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n\n            # Select the minimal Q value from the 2 calculated values\n            target_Q = torch.min(target_Q1, target_Q2)\n            av_Q += torch.mean(target_Q)\n            max_Q = max(max_Q, torch.max(target_Q))\n            # Calculate the final Q value from the target network parameters by using Bellman equation\n            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n\n            # Get the Q values of the basis networks with the current parameters\n            current_Q1, current_Q2 = self.critic(state, action)\n\n            # Calculate the loss between the current Q value and the target Q value\n            loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\n            if self.use_max_bound:\n                max_bound = get_max_bound(\n                    next_state,\n                    discount,\n                    max_ang_vel,\n                    max_lin_vel,\n                    time_step,\n                    distance_norm,\n                    goal_reward,\n                    reward,\n                    done,\n                    self.device,\n                )\n                max_excess_Q1 = F.relu(current_Q1 - max_bound)\n                max_excess_Q2 = F.relu(current_Q2 - max_bound)\n                max_bound_loss = (max_excess_Q1**2).mean() + (max_excess_Q2**2).mean()\n                # Add loss for Q values exceeding maximum possible upper bound\n                loss += self.bound_weight * max_bound_loss\n\n            # Perform the gradient descent\n            self.critic_optimizer.zero_grad()\n            loss.backward()\n            self.critic_optimizer.step()\n\n            if it % policy_freq == 0:\n                # Maximize the actor output value by performing gradient descent on negative Q values\n                # (essentially perform gradient ascent)\n                actor_grad, _ = self.critic(state, self.actor(state))\n                actor_grad = -actor_grad.mean()\n                self.actor_optimizer.zero_grad()\n                actor_grad.backward()\n                self.actor_optimizer.step()\n\n                # Use soft update to update the actor-target network parameters by\n                # infusing small amount of current parameters\n                for param, target_param in zip(\n                    self.actor.parameters(), self.actor_target.parameters()\n                ):\n                    target_param.data.copy_(\n                        tau * param.data + (1 - tau) * target_param.data\n                    )\n                # Use soft update to update the critic-target network parameters by infusing\n                # small amount of current parameters\n                for param, target_param in zip(\n                    self.critic.parameters(), self.critic_target.parameters()\n                ):\n                    target_param.data.copy_(\n                        tau * param.data + (1 - tau) * target_param.data\n                    )\n\n            av_loss += loss\n        self.iter_count += 1\n        # Write new values for tensorboard\n        self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n        self.writer.add_scalar(\"train/avg_Q\", av_Q / iterations, self.iter_count)\n        self.writer.add_scalar(\"train/max_Q\", max_Q, self.iter_count)\n        if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n            self.save(filename=self.model_name, directory=self.save_directory)\n\n    def save(self, filename, directory):\n        \"\"\"\n        Saves the current model parameters to the specified directory.\n\n        Args:\n            filename (str): Base filename for saved files.\n            directory (Path): Path to save the model files.\n        \"\"\"\n        Path(directory).mkdir(parents=True, exist_ok=True)\n        torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n        torch.save(\n            self.actor_target.state_dict(),\n            \"%s/%s_actor_target.pth\" % (directory, filename),\n        )\n        torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n        torch.save(\n            self.critic_target.state_dict(),\n            \"%s/%s_critic_target.pth\" % (directory, filename),\n        )\n\n    def load(self, filename, directory):\n        \"\"\"\n        Loads model parameters from the specified directory.\n\n        Args:\n            filename (str): Base filename for saved files.\n            directory (Path): Path to load the model files from.\n        \"\"\"\n        self.actor.load_state_dict(\n            torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n        )\n        self.actor_target.load_state_dict(\n            torch.load(\"%s/%s_actor_target.pth\" % (directory, filename))\n        )\n        self.critic.load_state_dict(\n            torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n        )\n        self.critic_target.load_state_dict(\n            torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n        )\n        print(f\"Loaded weights from: {directory}\")\n\n    def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n        \"\"\"\n        Prepares the environment's raw sensor data and navigation variables into\n        a format suitable for learning.\n\n        Args:\n            latest_scan (list or np.ndarray): Raw scan data (e.g., LiDAR).\n            distance (float): Distance to goal.\n            cos (float): Cosine of heading angle to goal.\n            sin (float): Sine of heading angle to goal.\n            collision (bool): Collision status (True if collided).\n            goal (bool): Goal reached status.\n            action (list or np.ndarray): Last action taken [lin_vel, ang_vel].\n\n        Returns:\n            tuple:\n                - state (list): Normalized and concatenated state vector.\n                - terminal (int): Terminal flag (1 if collision or goal, else 0).\n        \"\"\"\n        latest_scan = np.array(latest_scan)\n\n        inf_mask = np.isinf(latest_scan)\n        latest_scan[inf_mask] = 7.0\n        latest_scan /= 7\n\n        # Normalize to [0, 1] range\n        distance /= 10\n        lin_vel = action[0] * 2\n        ang_vel = (action[1] + 1) / 2\n        state = latest_scan.tolist() + [distance, cos, sin] + [lin_vel, ang_vel]\n\n        assert len(state) == self.state_dim\n        terminal = 1 if collision or goal else 0\n\n        return state, terminal\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.CNNTD3.act","title":"<code>act(state)</code>","text":"<p>Computes the deterministic action from the actor network for a given state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Input state.</p> required <p>Returns:</p> Type Description <p>np.ndarray: Action predicted by the actor network.</p> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>def act(self, state):\n    \"\"\"\n    Computes the deterministic action from the actor network for a given state.\n\n    Args:\n        state (np.ndarray): Input state.\n\n    Returns:\n        np.ndarray: Action predicted by the actor network.\n    \"\"\"\n    # Function to get the action from the actor\n    state = torch.Tensor(state).to(self.device)\n    return self.actor(state).cpu().data.numpy().flatten()\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.CNNTD3.get_action","title":"<code>get_action(obs, add_noise)</code>","text":"<p>Selects an action for a given observation.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>ndarray</code> <p>The current observation/state.</p> required <code>add_noise</code> <code>bool</code> <p>Whether to add exploration noise to the action.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The selected action.</p> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>def get_action(self, obs, add_noise):\n    \"\"\"\n    Selects an action for a given observation.\n\n    Args:\n        obs (np.ndarray): The current observation/state.\n        add_noise (bool): Whether to add exploration noise to the action.\n\n    Returns:\n        np.ndarray: The selected action.\n    \"\"\"\n    if add_noise:\n        return (\n            self.act(obs) + np.random.normal(0, 0.2, size=self.action_dim)\n        ).clip(-self.max_action, self.max_action)\n    else:\n        return self.act(obs)\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.CNNTD3.load","title":"<code>load(filename, directory)</code>","text":"<p>Loads model parameters from the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base filename for saved files.</p> required <code>directory</code> <code>Path</code> <p>Path to load the model files from.</p> required Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>def load(self, filename, directory):\n    \"\"\"\n    Loads model parameters from the specified directory.\n\n    Args:\n        filename (str): Base filename for saved files.\n        directory (Path): Path to load the model files from.\n    \"\"\"\n    self.actor.load_state_dict(\n        torch.load(\"%s/%s_actor.pth\" % (directory, filename))\n    )\n    self.actor_target.load_state_dict(\n        torch.load(\"%s/%s_actor_target.pth\" % (directory, filename))\n    )\n    self.critic.load_state_dict(\n        torch.load(\"%s/%s_critic.pth\" % (directory, filename))\n    )\n    self.critic_target.load_state_dict(\n        torch.load(\"%s/%s_critic_target.pth\" % (directory, filename))\n    )\n    print(f\"Loaded weights from: {directory}\")\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.CNNTD3.prepare_state","title":"<code>prepare_state(latest_scan, distance, cos, sin, collision, goal, action)</code>","text":"<p>Prepares the environment's raw sensor data and navigation variables into a format suitable for learning.</p> <p>Parameters:</p> Name Type Description Default <code>latest_scan</code> <code>list or ndarray</code> <p>Raw scan data (e.g., LiDAR).</p> required <code>distance</code> <code>float</code> <p>Distance to goal.</p> required <code>cos</code> <code>float</code> <p>Cosine of heading angle to goal.</p> required <code>sin</code> <code>float</code> <p>Sine of heading angle to goal.</p> required <code>collision</code> <code>bool</code> <p>Collision status (True if collided).</p> required <code>goal</code> <code>bool</code> <p>Goal reached status.</p> required <code>action</code> <code>list or ndarray</code> <p>Last action taken [lin_vel, ang_vel].</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>state (list): Normalized and concatenated state vector.</li> <li>terminal (int): Terminal flag (1 if collision or goal, else 0).</li> </ul> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>def prepare_state(self, latest_scan, distance, cos, sin, collision, goal, action):\n    \"\"\"\n    Prepares the environment's raw sensor data and navigation variables into\n    a format suitable for learning.\n\n    Args:\n        latest_scan (list or np.ndarray): Raw scan data (e.g., LiDAR).\n        distance (float): Distance to goal.\n        cos (float): Cosine of heading angle to goal.\n        sin (float): Sine of heading angle to goal.\n        collision (bool): Collision status (True if collided).\n        goal (bool): Goal reached status.\n        action (list or np.ndarray): Last action taken [lin_vel, ang_vel].\n\n    Returns:\n        tuple:\n            - state (list): Normalized and concatenated state vector.\n            - terminal (int): Terminal flag (1 if collision or goal, else 0).\n    \"\"\"\n    latest_scan = np.array(latest_scan)\n\n    inf_mask = np.isinf(latest_scan)\n    latest_scan[inf_mask] = 7.0\n    latest_scan /= 7\n\n    # Normalize to [0, 1] range\n    distance /= 10\n    lin_vel = action[0] * 2\n    ang_vel = (action[1] + 1) / 2\n    state = latest_scan.tolist() + [distance, cos, sin] + [lin_vel, ang_vel]\n\n    assert len(state) == self.state_dim\n    terminal = 1 if collision or goal else 0\n\n    return state, terminal\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.CNNTD3.save","title":"<code>save(filename, directory)</code>","text":"<p>Saves the current model parameters to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base filename for saved files.</p> required <code>directory</code> <code>Path</code> <p>Path to save the model files.</p> required Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>def save(self, filename, directory):\n    \"\"\"\n    Saves the current model parameters to the specified directory.\n\n    Args:\n        filename (str): Base filename for saved files.\n        directory (Path): Path to save the model files.\n    \"\"\"\n    Path(directory).mkdir(parents=True, exist_ok=True)\n    torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n    torch.save(\n        self.actor_target.state_dict(),\n        \"%s/%s_actor_target.pth\" % (directory, filename),\n    )\n    torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n    torch.save(\n        self.critic_target.state_dict(),\n        \"%s/%s_critic_target.pth\" % (directory, filename),\n    )\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.CNNTD3.train","title":"<code>train(replay_buffer, iterations, batch_size, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2, max_lin_vel=0.5, max_ang_vel=1, goal_reward=100, distance_norm=10, time_step=0.3)</code>","text":"<p>Trains the CNNTD3 agent using sampled batches from the replay buffer.</p> <p>Parameters:</p> Name Type Description Default <code>replay_buffer</code> <code>ReplayBuffer</code> <p>Buffer storing environment transitions.</p> required <code>iterations</code> <code>int</code> <p>Number of training iterations.</p> required <code>batch_size</code> <code>int</code> <p>Size of each training batch.</p> required <code>discount</code> <code>float</code> <p>Discount factor for future rewards.</p> <code>0.99</code> <code>tau</code> <code>float</code> <p>Soft update rate for target networks.</p> <code>0.005</code> <code>policy_noise</code> <code>float</code> <p>Std. dev. of noise added to target policy.</p> <code>0.2</code> <code>noise_clip</code> <code>float</code> <p>Maximum value for target policy noise.</p> <code>0.5</code> <code>policy_freq</code> <code>int</code> <p>Frequency of actor and target network updates.</p> <code>2</code> <code>max_lin_vel</code> <code>float</code> <p>Maximum linear velocity for bounding calculations.</p> <code>0.5</code> <code>max_ang_vel</code> <code>float</code> <p>Maximum angular velocity for bounding calculations.</p> <code>1</code> <code>goal_reward</code> <code>float</code> <p>Reward value for reaching the goal.</p> <code>100</code> <code>distance_norm</code> <code>float</code> <p>Normalization factor for distance in bounding.</p> <code>10</code> <code>time_step</code> <code>float</code> <p>Time delta between steps.</p> <code>0.3</code> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>def train(\n    self,\n    replay_buffer,\n    iterations,\n    batch_size,\n    discount=0.99,\n    tau=0.005,\n    policy_noise=0.2,\n    noise_clip=0.5,\n    policy_freq=2,\n    max_lin_vel=0.5,\n    max_ang_vel=1,\n    goal_reward=100,\n    distance_norm=10,\n    time_step=0.3,\n):\n    \"\"\"\n    Trains the CNNTD3 agent using sampled batches from the replay buffer.\n\n    Args:\n        replay_buffer (ReplayBuffer): Buffer storing environment transitions.\n        iterations (int): Number of training iterations.\n        batch_size (int): Size of each training batch.\n        discount (float): Discount factor for future rewards.\n        tau (float): Soft update rate for target networks.\n        policy_noise (float): Std. dev. of noise added to target policy.\n        noise_clip (float): Maximum value for target policy noise.\n        policy_freq (int): Frequency of actor and target network updates.\n        max_lin_vel (float): Maximum linear velocity for bounding calculations.\n        max_ang_vel (float): Maximum angular velocity for bounding calculations.\n        goal_reward (float): Reward value for reaching the goal.\n        distance_norm (float): Normalization factor for distance in bounding.\n        time_step (float): Time delta between steps.\n    \"\"\"\n    av_Q = 0\n    max_Q = -inf\n    av_loss = 0\n    for it in range(iterations):\n        # sample a batch from the replay buffer\n        (\n            batch_states,\n            batch_actions,\n            batch_rewards,\n            batch_dones,\n            batch_next_states,\n        ) = replay_buffer.sample_batch(batch_size)\n        state = torch.Tensor(batch_states).to(self.device)\n        next_state = torch.Tensor(batch_next_states).to(self.device)\n        action = torch.Tensor(batch_actions).to(self.device)\n        reward = torch.Tensor(batch_rewards).to(self.device)\n        done = torch.Tensor(batch_dones).to(self.device)\n\n        # Obtain the estimated action from the next state by using the actor-target\n        next_action = self.actor_target(next_state)\n\n        # Add noise to the action\n        noise = (\n            torch.Tensor(batch_actions)\n            .data.normal_(0, policy_noise)\n            .to(self.device)\n        )\n        noise = noise.clamp(-noise_clip, noise_clip)\n        next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\n        # Calculate the Q values from the critic-target network for the next state-action pair\n        target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n\n        # Select the minimal Q value from the 2 calculated values\n        target_Q = torch.min(target_Q1, target_Q2)\n        av_Q += torch.mean(target_Q)\n        max_Q = max(max_Q, torch.max(target_Q))\n        # Calculate the final Q value from the target network parameters by using Bellman equation\n        target_Q = reward + ((1 - done) * discount * target_Q).detach()\n\n        # Get the Q values of the basis networks with the current parameters\n        current_Q1, current_Q2 = self.critic(state, action)\n\n        # Calculate the loss between the current Q value and the target Q value\n        loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\n        if self.use_max_bound:\n            max_bound = get_max_bound(\n                next_state,\n                discount,\n                max_ang_vel,\n                max_lin_vel,\n                time_step,\n                distance_norm,\n                goal_reward,\n                reward,\n                done,\n                self.device,\n            )\n            max_excess_Q1 = F.relu(current_Q1 - max_bound)\n            max_excess_Q2 = F.relu(current_Q2 - max_bound)\n            max_bound_loss = (max_excess_Q1**2).mean() + (max_excess_Q2**2).mean()\n            # Add loss for Q values exceeding maximum possible upper bound\n            loss += self.bound_weight * max_bound_loss\n\n        # Perform the gradient descent\n        self.critic_optimizer.zero_grad()\n        loss.backward()\n        self.critic_optimizer.step()\n\n        if it % policy_freq == 0:\n            # Maximize the actor output value by performing gradient descent on negative Q values\n            # (essentially perform gradient ascent)\n            actor_grad, _ = self.critic(state, self.actor(state))\n            actor_grad = -actor_grad.mean()\n            self.actor_optimizer.zero_grad()\n            actor_grad.backward()\n            self.actor_optimizer.step()\n\n            # Use soft update to update the actor-target network parameters by\n            # infusing small amount of current parameters\n            for param, target_param in zip(\n                self.actor.parameters(), self.actor_target.parameters()\n            ):\n                target_param.data.copy_(\n                    tau * param.data + (1 - tau) * target_param.data\n                )\n            # Use soft update to update the critic-target network parameters by infusing\n            # small amount of current parameters\n            for param, target_param in zip(\n                self.critic.parameters(), self.critic_target.parameters()\n            ):\n                target_param.data.copy_(\n                    tau * param.data + (1 - tau) * target_param.data\n                )\n\n        av_loss += loss\n    self.iter_count += 1\n    # Write new values for tensorboard\n    self.writer.add_scalar(\"train/loss\", av_loss / iterations, self.iter_count)\n    self.writer.add_scalar(\"train/avg_Q\", av_Q / iterations, self.iter_count)\n    self.writer.add_scalar(\"train/max_Q\", max_Q, self.iter_count)\n    if self.save_every &gt; 0 and self.iter_count % self.save_every == 0:\n        self.save(filename=self.model_name, directory=self.save_directory)\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.Critic","title":"<code>Critic</code>","text":"<p>               Bases: <code>Module</code></p> <p>Critic network for the CNNTD3 agent.</p> <p>The Critic estimates Q-values for state-action pairs using two separate sub-networks (Q1 and Q2), as required by the TD3 algorithm. Each sub-network uses a combination of CNN-extracted features, embedded goal and previous action features, and the current action.</p> <p>Parameters:</p> Name Type Description Default <code>action_dim</code> <code>int</code> <p>The dimension of the action space.</p> required Architecture <ul> <li>Shared CNN layers process the laser scan input.</li> <li>Goal and previous action are embedded and concatenated.</li> <li>Each Q-network uses separate fully connected layers to produce scalar Q-values.</li> <li>Both Q-networks receive the full state and current action.</li> <li>Outputs two Q-value tensors (Q1, Q2) for TD3-style training and target smoothing.</li> </ul> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>class Critic(nn.Module):\n    \"\"\"\n    Critic network for the CNNTD3 agent.\n\n    The Critic estimates Q-values for state-action pairs using two separate sub-networks\n    (Q1 and Q2), as required by the TD3 algorithm. Each sub-network uses a combination of\n    CNN-extracted features, embedded goal and previous action features, and the current action.\n\n    Args:\n        action_dim (int): The dimension of the action space.\n\n    Architecture:\n        - Shared CNN layers process the laser scan input.\n        - Goal and previous action are embedded and concatenated.\n        - Each Q-network uses separate fully connected layers to produce scalar Q-values.\n        - Both Q-networks receive the full state and current action.\n        - Outputs two Q-value tensors (Q1, Q2) for TD3-style training and target smoothing.\n    \"\"\"\n\n    def __init__(self, action_dim):\n        super(Critic, self).__init__()\n        self.cnn1 = nn.Conv1d(1, 4, kernel_size=8, stride=4)\n        self.cnn2 = nn.Conv1d(4, 8, kernel_size=8, stride=4)\n        self.cnn3 = nn.Conv1d(8, 4, kernel_size=4, stride=2)\n\n        self.goal_embed = nn.Linear(3, 10)\n        self.action_embed = nn.Linear(2, 10)\n\n        self.layer_1 = nn.Linear(36, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2_s = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2_s.weight, nonlinearity=\"leaky_relu\")\n        self.layer_2_a = nn.Linear(action_dim, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_2_a.weight, nonlinearity=\"leaky_relu\")\n        self.layer_3 = nn.Linear(300, 1)\n        torch.nn.init.kaiming_uniform_(self.layer_3.weight, nonlinearity=\"leaky_relu\")\n\n        self.layer_4 = nn.Linear(36, 400)\n        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"leaky_relu\")\n        self.layer_5_s = nn.Linear(400, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_5_s.weight, nonlinearity=\"leaky_relu\")\n        self.layer_5_a = nn.Linear(action_dim, 300)\n        torch.nn.init.kaiming_uniform_(self.layer_5_a.weight, nonlinearity=\"leaky_relu\")\n        self.layer_6 = nn.Linear(300, 1)\n        torch.nn.init.kaiming_uniform_(self.layer_6.weight, nonlinearity=\"leaky_relu\")\n\n    def forward(self, s, action):\n        \"\"\"\n        Forward pass through both Q-networks of the Critic.\n\n        Args:\n            s (torch.Tensor): Input state tensor of shape (batch_size, state_dim).\n                              The last 5 elements are [distance, cos, sin, lin_vel, ang_vel].\n            action (torch.Tensor): Current action tensor of shape (batch_size, action_dim).\n\n        Returns:\n            tuple:\n                - q1 (torch.Tensor): First Q-value estimate (batch_size, 1).\n                - q2 (torch.Tensor): Second Q-value estimate (batch_size, 1).\n        \"\"\"\n        laser = s[:, :-5]\n        goal = s[:, -5:-2]\n        act = s[:, -2:]\n        laser = laser.unsqueeze(1)\n\n        l = F.leaky_relu(self.cnn1(laser))\n        l = F.leaky_relu(self.cnn2(l))\n        l = F.leaky_relu(self.cnn3(l))\n        l = l.flatten(start_dim=1)\n\n        g = F.leaky_relu(self.goal_embed(goal))\n\n        a = F.leaky_relu(self.action_embed(act))\n\n        s = torch.concat((l, g, a), dim=-1)\n\n        s1 = F.leaky_relu(self.layer_1(s))\n        self.layer_2_s(s1)\n        self.layer_2_a(action)\n        s11 = torch.mm(s1, self.layer_2_s.weight.data.t())\n        s12 = torch.mm(action, self.layer_2_a.weight.data.t())\n        s1 = F.leaky_relu(s11 + s12 + self.layer_2_a.bias.data)\n        q1 = self.layer_3(s1)\n\n        s2 = F.leaky_relu(self.layer_4(s))\n        self.layer_5_s(s2)\n        self.layer_5_a(action)\n        s21 = torch.mm(s2, self.layer_5_s.weight.data.t())\n        s22 = torch.mm(action, self.layer_5_a.weight.data.t())\n        s2 = F.leaky_relu(s21 + s22 + self.layer_5_a.bias.data)\n        q2 = self.layer_6(s2)\n        return q1, q2\n</code></pre>"},{"location":"api/models/cnntd3/#robot_nav.models.CNNTD3.CNNTD3.Critic.forward","title":"<code>forward(s, action)</code>","text":"<p>Forward pass through both Q-networks of the Critic.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_dim).               The last 5 elements are [distance, cos, sin, lin_vel, ang_vel].</p> required <code>action</code> <code>Tensor</code> <p>Current action tensor of shape (batch_size, action_dim).</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>q1 (torch.Tensor): First Q-value estimate (batch_size, 1).</li> <li>q2 (torch.Tensor): Second Q-value estimate (batch_size, 1).</li> </ul> Source code in <code>robot_nav/models/CNNTD3/CNNTD3.py</code> <pre><code>def forward(self, s, action):\n    \"\"\"\n    Forward pass through both Q-networks of the Critic.\n\n    Args:\n        s (torch.Tensor): Input state tensor of shape (batch_size, state_dim).\n                          The last 5 elements are [distance, cos, sin, lin_vel, ang_vel].\n        action (torch.Tensor): Current action tensor of shape (batch_size, action_dim).\n\n    Returns:\n        tuple:\n            - q1 (torch.Tensor): First Q-value estimate (batch_size, 1).\n            - q2 (torch.Tensor): Second Q-value estimate (batch_size, 1).\n    \"\"\"\n    laser = s[:, :-5]\n    goal = s[:, -5:-2]\n    act = s[:, -2:]\n    laser = laser.unsqueeze(1)\n\n    l = F.leaky_relu(self.cnn1(laser))\n    l = F.leaky_relu(self.cnn2(l))\n    l = F.leaky_relu(self.cnn3(l))\n    l = l.flatten(start_dim=1)\n\n    g = F.leaky_relu(self.goal_embed(goal))\n\n    a = F.leaky_relu(self.action_embed(act))\n\n    s = torch.concat((l, g, a), dim=-1)\n\n    s1 = F.leaky_relu(self.layer_1(s))\n    self.layer_2_s(s1)\n    self.layer_2_a(action)\n    s11 = torch.mm(s1, self.layer_2_s.weight.data.t())\n    s12 = torch.mm(action, self.layer_2_a.weight.data.t())\n    s1 = F.leaky_relu(s11 + s12 + self.layer_2_a.bias.data)\n    q1 = self.layer_3(s1)\n\n    s2 = F.leaky_relu(self.layer_4(s))\n    self.layer_5_s(s2)\n    self.layer_5_a(action)\n    s21 = torch.mm(s2, self.layer_5_s.weight.data.t())\n    s22 = torch.mm(action, self.layer_5_a.weight.data.t())\n    s2 = F.leaky_relu(s21 + s22 + self.layer_5_a.bias.data)\n    q2 = self.layer_6(s2)\n    return q1, q2\n</code></pre>"}]}